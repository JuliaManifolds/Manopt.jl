<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Do constrained optimization ¬∑ Manopt.jl</title><meta name="title" content="Do constrained optimization ¬∑ Manopt.jl"/><meta property="og:title" content="Do constrained optimization ¬∑ Manopt.jl"/><meta property="twitter:title" content="Do constrained optimization ¬∑ Manopt.jl"/><meta name="description" content="Documentation for Manopt.jl."/><meta property="og:description" content="Documentation for Manopt.jl."/><meta property="twitter:description" content="Documentation for Manopt.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/citations.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../Optimize/">üèîÔ∏è Get started: optimize.</a></li><li><a class="tocitem" href="../InplaceGradient/">Speedup using in-place computations</a></li><li><a class="tocitem" href="../AutomaticDifferentiation/">Use automatic differentiation</a></li><li><a class="tocitem" href="../EmbeddingObjectives/">Define objectives in the embedding</a></li><li><a class="tocitem" href="../CountAndCache/">Count and use a cache</a></li><li><a class="tocitem" href="../HowToDebug/">Print debug output</a></li><li><a class="tocitem" href="../HowToRecord/">Record values</a></li><li><a class="tocitem" href="../ImplementASolver/">Implement a solver</a></li><li><a class="tocitem" href="../ImplementOwnManifold/">Optimize on your own manifold</a></li><li class="is-active"><a class="tocitem" href>Do constrained optimization</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#A-first-augmented-Lagrangian-run"><span>A first augmented Lagrangian run</span></a></li><li><a class="tocitem" href="#A-faster-augmented-Lagrangian-run"><span>A faster augmented Lagrangian run</span></a></li><li><a class="tocitem" href="#Exact-penalty-method"><span>Exact penalty method</span></a></li><li><a class="tocitem" href="#Comparing-to-the-unconstrained-solver"><span>Comparing to the unconstrained solver</span></a></li><li><a class="tocitem" href="#Literature"><span>Literature</span></a></li></ul></li><li><a class="tocitem" href="../GeodesicRegression/">Do geodesic regression</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../../solvers/">Introduction</a></li><li><a class="tocitem" href="../../solvers/adaptive-regularization-with-cubics/">Adaptive Regularization with Cubics</a></li><li><a class="tocitem" href="../../solvers/alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/augmented_Lagrangian_method/">Augmented Lagrangian Method</a></li><li><a class="tocitem" href="../../solvers/ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../../solvers/conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../../solvers/convex_bundle_method/">Convex bundle method</a></li><li><a class="tocitem" href="../../solvers/cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../../solvers/difference_of_convex/">Difference of Convex</a></li><li><a class="tocitem" href="../../solvers/DouglasRachford/">Douglas‚ÄîRachford</a></li><li><a class="tocitem" href="../../solvers/exact_penalty_method/">Exact Penalty Method</a></li><li><a class="tocitem" href="../../solvers/FrankWolfe/">Frank-Wolfe</a></li><li><a class="tocitem" href="../../solvers/gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/LevenbergMarquardt/">Levenberg‚ÄìMarquardt</a></li><li><a class="tocitem" href="../../solvers/NelderMead/">Nelder‚ÄìMead</a></li><li><a class="tocitem" href="../../solvers/particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../../solvers/primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../../solvers/proximal_bundle_method/">Proximal bundle method</a></li><li><a class="tocitem" href="../../solvers/quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../../solvers/stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../../solvers/truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../../solvers/trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/objective/">Objective</a></li><li><a class="tocitem" href="../../plans/state/">Solver State</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../extensions/">Extensions</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../changelog/">Changelog</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">How to...</a></li><li class="is-active"><a href>Do constrained optimization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Do constrained optimization</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/Manopt.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/tutorials/ConstrainedOptimization.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="How-to-do-constrained-optimization"><a class="docs-heading-anchor" href="#How-to-do-constrained-optimization">How to do constrained optimization</a><a id="How-to-do-constrained-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-do-constrained-optimization" title="Permalink"></a></h1><p>Ronny Bergmann</p><p>This tutorial is a short introduction to using solvers for constraint optimisation in <a href="https://manoptjl.org"><code>Manopt.jl</code></a>.</p><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>A constraint optimisation problem is given by</p><p class="math-container">\[\tag{P}
\begin{align*}
\operatorname*{arg\,min}_{p‚àà\mathcal M} &amp; f(p)\\
\text{such that} &amp;\quad g(p) \leq 0\\
&amp;\quad h(p) = 0,\\
\end{align*}\]</p><p>where <span>$f: \mathcal M ‚Üí ‚Ñù$</span> is a cost function, and <span>$g: \mathcal M ‚Üí ‚Ñù^m$</span> and <span>$h: \mathcal M ‚Üí ‚Ñù^n$</span> are the inequality and equality constraints, respectively. The <span>$\leq$</span> and <span>$=$</span> in (P) are meant element-wise.</p><p>This can be seen as a balance between moving constraints into the geometry of a manifold <span>$\mathcal M$</span> and keeping some, since they can be handled well in algorithms, see [<a href="../../references/#BergmannHerzog:2019">BH19</a>], [<a href="../../references/#LiuBoumal:2019">LB19</a>] for details.</p><pre><code class="language-julia hljs">using Distributions, LinearAlgebra, Manifolds, Manopt, Random
Random.seed!(42);</code></pre><p>In this tutorial we want to look at different ways to specify the problem and its implications. We start with specifying an example problems to illustrate the different available forms.</p><p>We consider the problem of a Nonnegative PCA, cf.¬†Section 5.1.2 in [<a href="../../references/#LiuBoumal:2019">LB19</a>]</p><p>let <span>$v_0 ‚àà ‚Ñù^d$</span>, <span>$\lVert v_0 \rVert=1$</span> be given spike signal, that is a signal that is sparse with only <span>$s=\lfloor Œ¥d \rfloor$</span> nonzero entries.</p><p class="math-container">\[Z = \sqrt{œÉ} v_0v_0^{\mathrm{T}}+N,\]</p><p>where <span>$\sigma$</span> is a signal-to-noise ratio and <span>$N$</span> is a matrix with random entries, where the diagonal entries are distributed with zero mean and standard deviation <span>$1/d$</span> on the off-diagonals and <span>$2/d$</span> on the diagonal</p><pre><code class="language-julia hljs">d = 150; # dimension of v0
œÉ = 0.1^2; # SNR
Œ¥ = 0.1; sp = Int(floor(Œ¥ * d)); # Sparsity
S = sample(1:d, sp; replace=false);
v0 =  [i ‚àà S ? 1 / sqrt(sp) : 0.0 for i in 1:d];
N = rand(Normal(0, 1 / d), (d, d)); N[diagind(N, 0)] .= rand(Normal(0, 2 / d), d);
Z = Z = sqrt(œÉ) * v0 * transpose(v0) + N;</code></pre><p>In order to recover <span>$v_0$</span> we consider the constrained optimisation problem on the sphere <span>$\mathcal S^{d-1}$</span> given by</p><p class="math-container">\[\begin{align*}
\operatorname*{arg\,min}_{p‚àà\mathcal S^{d-1}} &amp; -p^{\mathrm{T}}Zp^{\mathrm{T}}\\
\text{such that} &amp;\quad p \geq 0\\
\end{align*}\]</p><p>or in the previous notation <span>$f(p) = -p^{\mathrm{T}}Zp^{\mathrm{T}}$</span> and <span>$g(p) = -p$</span>. We first initialize the manifold under consideration</p><pre><code class="language-julia hljs">M = Sphere(d - 1)</code></pre><pre><code class="nohighlight hljs">Sphere(149, ‚Ñù)</code></pre><h2 id="A-first-augmented-Lagrangian-run"><a class="docs-heading-anchor" href="#A-first-augmented-Lagrangian-run">A first augmented Lagrangian run</a><a id="A-first-augmented-Lagrangian-run-1"></a><a class="docs-heading-anchor-permalink" href="#A-first-augmented-Lagrangian-run" title="Permalink"></a></h2><p>We first defined <span>$f$</span> and <span>$g$</span> as usual functions</p><pre><code class="language-julia hljs">f(M, p) = -transpose(p) * Z * p;
g(M, p) = -p;</code></pre><p>since <span>$f$</span> is a functions defined in the embedding <span>$‚Ñù^d$</span> as well, we obtain its gradient by projection.</p><pre><code class="language-julia hljs">grad_f(M, p) = project(M, p, -transpose(Z) * p - Z * p);</code></pre><p>For the constraints this is a little more involved, since each function <span>$g_i = g(p)_i = p_i$</span> has to return its own gradient. These are again in the embedding just <span>$\operatorname{grad} g_i(p) = -e_i$</span> the <span>$i$</span> th unit vector. We can project these again onto the tangent space at <span>$p$</span>:</p><pre><code class="language-julia hljs">grad_g(M, p) = project.(
    Ref(M), Ref(p), [[i == j ? -1.0 : 0.0 for j in 1:d] for i in 1:d]
);</code></pre><p>We further start in a random point:</p><pre><code class="language-julia hljs">p0 = rand(M);</code></pre><p>Let‚Äôs check a few things for the initial point</p><pre><code class="language-julia hljs">f(M, p0)</code></pre><pre><code class="nohighlight hljs">0.0057476048331242344</code></pre><p>How much the function g is positive</p><pre><code class="language-julia hljs">maximum(g(M, p0))</code></pre><pre><code class="nohighlight hljs">0.17885478285466855</code></pre><p>Now as a first method we can just call the <a href="https://manoptjl.org/stable/solvers/augmented_Lagrangian_method/">Augmented Lagrangian Method</a> with a simple call:</p><pre><code class="language-julia hljs">@time v1 = augmented_Lagrangian_method(
    M, f, grad_f, p0; g=g, grad_g=grad_g,
    debug=[:Iteration, :Cost, :Stop, &quot; | &quot;, (:Change, &quot;Œîp : %1.5e&quot;), 20, &quot;\n&quot;],
    stopping_criterion = StopAfterIteration(300) | (
        StopWhenSmallerOrEqual(:œµ, 1e-5) &amp; StopWhenChangeLess(1e-8)
    )
);</code></pre><pre><code class="nohighlight hljs">Initial f(x): 0.005748 | 
# 20    f(x): -0.123843 | Œîp : 1.00030e+00
# 40    f(x): -0.123843 | Œîp : 4.46945e-04
# 60    f(x): -0.123843 | Œîp : 3.87541e-04
The value of the variable (œµ) is smaller than or equal to its threshold (1.0e-5).
The algorithm performed a step with a change (7.466076176399011e-6) less than 9.120108393559073e-6.
  5.039556 seconds (9.67 M allocations: 3.925 GiB, 9.21% gc time, 82.75% compilation time)</code></pre><p>Now we have both a lower function value and the point is nearly within the constraints, ‚Ä¶ up to numerical inaccuracies</p><pre><code class="language-julia hljs">f(M, v1)</code></pre><pre><code class="nohighlight hljs">-0.1238387017550424</code></pre><pre><code class="language-julia hljs">maximum( g(M, v1) )</code></pre><pre><code class="nohighlight hljs">-7.042061792236086e-12</code></pre><h2 id="A-faster-augmented-Lagrangian-run"><a class="docs-heading-anchor" href="#A-faster-augmented-Lagrangian-run">A faster augmented Lagrangian run</a><a id="A-faster-augmented-Lagrangian-run-1"></a><a class="docs-heading-anchor-permalink" href="#A-faster-augmented-Lagrangian-run" title="Permalink"></a></h2><p>Now this is a little slow, so we can modify two things:</p><ol><li>Gradients should be evaluated in place, so for example</li></ol><pre><code class="language-julia hljs">grad_f!(M, X, p) = project!(M, X, p, -transpose(Z) * p - Z * p);</code></pre><ol><li>The constraints are currently always evaluated all together, since the function <code>grad_g</code> always returns a vector of gradients.  We first change the constraints function into a vector of functions.  We further change the gradient <em>both</em> into a vector of gradient functions <span>$\operatorname{grad} g_i, i=1,\ldots,d$</span>, <em>as well as</em> gradients that are computed in place.</li></ol><pre><code class="language-julia hljs">g2 = [(M, p) -&gt; -p[i] for i in 1:d];
grad_g2! = [
    (M, X, p) -&gt; project!(M, X, p, [i == j ? -1.0 : 0.0 for j in 1:d]) for i in 1:d
];</code></pre><p>We obtain</p><pre><code class="language-julia hljs">@time v2 = augmented_Lagrangian_method(
        M, f, grad_f!, p0; g=g2, grad_g=grad_g2!, evaluation=InplaceEvaluation(),
        debug=[:Iteration, :Cost, :Stop, &quot; | &quot;, (:Change, &quot;Œîp : %1.5e&quot;), 20, &quot;\n&quot;],
        stopping_criterion = StopAfterIteration(300) | (
          StopWhenSmallerOrEqual(:œµ, 1e-5) &amp; StopWhenChangeLess(1e-8)
        )
    );</code></pre><pre><code class="nohighlight hljs">Initial f(x): 0.005748 | 
# 20    f(x): -0.123843 | Œîp : 1.00019e+00
# 40    f(x): -0.123843 | Œîp : 6.77478e-04
# 60    f(x): -0.123843 | Œîp : 7.70563e-05
The value of the variable (œµ) is smaller than or equal to its threshold (1.0e-5).
The algorithm performed a step with a change (3.4787074961680085e-6) less than 6.456542290346536e-6.
  1.948764 seconds (3.39 M allocations: 1.804 GiB, 2.88% gc time, 72.91% compilation time)</code></pre><p>As a technical remark: note that (by default) the change to <a href="https://manoptjl.org/stable/plans/problem/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a>s affects both the constrained solver as well as the inner solver of the subproblem in each iteration.</p><pre><code class="language-julia hljs">f(M, v2)</code></pre><pre><code class="nohighlight hljs">-0.12384008804682478</code></pre><pre><code class="language-julia hljs">maximum(g(M, v2))</code></pre><pre><code class="nohighlight hljs">5.039908187343563e-13</code></pre><p>These are the very similar to the previous values but the solver took much less time and less memory allocations.</p><h2 id="Exact-penalty-method"><a class="docs-heading-anchor" href="#Exact-penalty-method">Exact penalty method</a><a id="Exact-penalty-method-1"></a><a class="docs-heading-anchor-permalink" href="#Exact-penalty-method" title="Permalink"></a></h2><p>As a second solver, we have the <a href="https://manoptjl.org/stable/solvers/exact_penalty_method/">Exact Penalty Method</a>, which currently is available with two smoothing variants, which make an inner solver for smooth optimization, that is by default again [quasi Newton] possible: <a href="https://manoptjl.org/stable/solvers/exact_penalty_method/#Manopt.LogarithmicSumOfExponentials"><code>LogarithmicSumOfExponentials</code></a> and <a href="https://manoptjl.org/stable/solvers/exact_penalty_method/#Manopt.LinearQuadraticHuber"><code>LinearQuadraticHuber</code></a>. We compare both here as well. The first smoothing technique is the default, so we can just call</p><pre><code class="language-julia hljs">@time v3 = exact_penalty_method(
    M, f, grad_f!, p0; g=g2, grad_g=grad_g2!, evaluation=InplaceEvaluation(),
    debug=[:Iteration, :Cost, :Stop, &quot; | &quot;, :Change, 50, &quot;\n&quot;],
);</code></pre><pre><code class="nohighlight hljs">Initial f(x): 0.005748 | 
# 50    f(x): -0.123071 | Last Change: 0.981298
# 100   f(x): -0.123840 | Last Change: 0.014479
The value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).
The algorithm performed a step with a change (2.3197576444804602e-7) less than 1.0e-6.
  1.460613 seconds (3.80 M allocations: 1.801 GiB, 3.95% gc time, 79.90% compilation time)</code></pre><p>We obtain a similar cost value as for the Augmented Lagrangian Solver above, but here the constraint is actually fulfilled and not just numerically ‚Äúon the boundary‚Äù.</p><pre><code class="language-julia hljs">f(M, v3)</code></pre><pre><code class="nohighlight hljs">-0.1238403951818009</code></pre><pre><code class="language-julia hljs">maximum(g(M, v3))</code></pre><pre><code class="nohighlight hljs">-3.5740153559377975e-6</code></pre><p>The second smoothing technique is often beneficial, when we have a lot of constraints (in the above mentioned vectorial manner), since we can avoid several gradient evaluations for the constraint functions here. This leads to a faster iteration time.</p><pre><code class="language-julia hljs">@time v4 = exact_penalty_method(
    M, f, grad_f!, p0; g=g2, grad_g=grad_g2!,
    evaluation=InplaceEvaluation(),
    smoothing=LinearQuadraticHuber(),
    debug=[:Iteration, :Cost, :Stop, &quot; | &quot;, :Change, 50, &quot;\n&quot;],
);</code></pre><pre><code class="nohighlight hljs">Initial f(x): 0.005748 | 
# 50    f(x): -0.123845 | Last Change: 0.009211
# 100   f(x): -0.123843 | Last Change: 0.000348
The value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).
The algorithm performed a step with a change (8.969129273937203e-7) less than 1.0e-6.
  1.211480 seconds (2.00 M allocations: 525.950 MiB, 3.28% gc time, 87.62% compilation time)</code></pre><p>For the result we see the same behaviour as for the other smoothing.</p><pre><code class="language-julia hljs">f(M, v4)</code></pre><pre><code class="nohighlight hljs">-0.12384258526239394</code></pre><pre><code class="language-julia hljs">maximum(g(M, v4))</code></pre><pre><code class="nohighlight hljs">2.7300637844646115e-8</code></pre><h2 id="Comparing-to-the-unconstrained-solver"><a class="docs-heading-anchor" href="#Comparing-to-the-unconstrained-solver">Comparing to the unconstrained solver</a><a id="Comparing-to-the-unconstrained-solver-1"></a><a class="docs-heading-anchor-permalink" href="#Comparing-to-the-unconstrained-solver" title="Permalink"></a></h2><p>We can compare this to the <em>global</em> optimum on the sphere, which is the unconstrained optimisation problem, where we can just use Quasi Newton.</p><p>Note that this is much faster, since every iteration of the algorithms above does a quasi-Newton call as well.</p><pre><code class="language-julia hljs">@time w1 = quasi_Newton(
    M, f, grad_f!, p0; evaluation=InplaceEvaluation()
);</code></pre><pre><code class="nohighlight hljs">  0.627805 seconds (678.63 k allocations: 61.910 MiB, 1.90% gc time, 98.43% compilation time)</code></pre><pre><code class="language-julia hljs">f(M, w1)</code></pre><pre><code class="nohighlight hljs">-0.1402190180976313</code></pre><p>But for sure here the constraints here are not fulfilled and we have quite positive entries in <span>$g(w_1)$</span></p><pre><code class="language-julia hljs">maximum(g(M, w1))</code></pre><pre><code class="nohighlight hljs">0.11762276192957026</code></pre><h2 id="Literature"><a class="docs-heading-anchor" href="#Literature">Literature</a><a id="Literature-1"></a><a class="docs-heading-anchor-permalink" href="#Literature" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[BH19]</dt><dd><div>R.¬†Bergmann and R.¬†Herzog. <em>Intrinsic formulation of KKT conditions and constraint qualifications on smooth manifolds</em>. <a href="https://doi.org/10.1137/18M1181602">SIAM¬†Journal¬†on¬†Optimization <strong>29</strong>, 2423‚Äì2444</a> (2019), <a href="https://arxiv.org/abs/1804.06214">arXiv:1804.06214</a>.</div></dd><dt>[LB19]</dt><dd><div>C.¬†Liu and N.¬†Boumal. <em>Simple algorithms for optimization on Riemannian manifolds with constraints</em>. <a href="https://doi.org/10.1007/s00245-019-09564-3">Applied¬†Mathematics¬†&amp;¬†Optimization</a> (2019), <a href="https://arxiv.org/abs/1091.10000">arXiv:1091.10000</a>.</div></dd></dl></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ImplementOwnManifold/">¬´ Optimize on your own manifold</a><a class="docs-footer-nextpage" href="../GeodesicRegression/">Do geodesic regression ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Friday 15 March 2024 21:32">Friday 15 March 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
