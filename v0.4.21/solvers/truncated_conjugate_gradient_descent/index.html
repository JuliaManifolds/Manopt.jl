<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Steihaug-Toint TCG Method · Manopt.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../../tutorials/Optimize!/">Get started: Optimize!</a></li><li><a class="tocitem" href="../../tutorials/InplaceGradient/">Speedup using Inplace computations</a></li><li><a class="tocitem" href="../../tutorials/AutomaticDifferentiation/">Use Automatic Differentiation</a></li><li><a class="tocitem" href="../../tutorials/CountAndCache/">Count and use a Cache</a></li><li><a class="tocitem" href="../../tutorials/HowToRecord/">Record values</a></li><li><a class="tocitem" href="../../tutorials/ConstrainedOptimization/">Do Contrained Optimization</a></li><li><a class="tocitem" href="../../tutorials/GeodesicRegression/">Do Geodesic Regression</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../augmented_Lagrangian_method/">Augmented Lagrangian Method</a></li><li><a class="tocitem" href="../ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../difference_of_convex/">Difference of Convex</a></li><li><a class="tocitem" href="../DouglasRachford/">Douglas–Rachford</a></li><li><a class="tocitem" href="../exact_penalty_method/">Exact Penalty Method</a></li><li><a class="tocitem" href="../FrankWolfe/">Frank-Wolfe</a></li><li><a class="tocitem" href="../gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../LevenbergMarquardt/">Levenberg–Marquardt</a></li><li><a class="tocitem" href="../NelderMead/">Nelder–Mead</a></li><li><a class="tocitem" href="../particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../subgradient/">Subgradient method</a></li><li class="is-active"><a class="tocitem" href>Steihaug-Toint TCG Method</a><ul class="internal"><li><a class="tocitem" href="#Initialization"><span>Initialization</span></a></li><li><a class="tocitem" href="#Iteration"><span>Iteration</span></a></li><li><a class="tocitem" href="#Result"><span>Result</span></a></li><li><a class="tocitem" href="#Remarks"><span>Remarks</span></a></li><li><a class="tocitem" href="#Interface"><span>Interface</span></a></li><li><a class="tocitem" href="#State"><span>State</span></a></li><li><a class="tocitem" href="#Stopping-Criteria"><span>Stopping Criteria</span></a></li><li><a class="tocitem" href="#Literature"><span>Literature</span></a></li></ul></li><li><a class="tocitem" href="../trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/objective/">Objective</a></li><li><a class="tocitem" href="../../plans/state/">Solver State</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../../functions/">Introduction</a></li><li><a class="tocitem" href="../../functions/bezier/">Bézier curves</a></li><li><a class="tocitem" href="../../functions/costs/">Cost functions</a></li><li><a class="tocitem" href="../../functions/differentials/">Differentials</a></li><li><a class="tocitem" href="../../functions/adjointdifferentials/">Adjoint Differentials</a></li><li><a class="tocitem" href="../../functions/gradients/">Gradients</a></li><li><a class="tocitem" href="../../functions/proximal_maps/">Proximal Maps</a></li><li><a class="tocitem" href="../../functions/manifold/">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/data/">Data</a></li><li><a class="tocitem" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../extensions/">Extensions</a></li><li><a class="tocitem" href="../../list/">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href>Steihaug-Toint TCG Method</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Steihaug-Toint TCG Method</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/solvers/truncated_conjugate_gradient_descent.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="tCG"><a class="docs-heading-anchor" href="#tCG">Steihaug-Toint Truncated Conjugate-Gradient Method</a><a id="tCG-1"></a><a class="docs-heading-anchor-permalink" href="#tCG" title="Permalink"></a></h1><p>The aim is to solve the trust-region subproblem</p><p class="math-container">\[\operatorname*{arg\,min}_{η  ∈  T_{x}\mathcal{M}} m_{x}(η) = F(x) +
⟨\operatorname{grad}F(x), η⟩_{x} + \frac{1}{2} ⟨
\mathcal{H}[η], η⟩_{x}\]</p><p class="math-container">\[\text{s.t.} \; ⟨η, η⟩_{x} \leq {Δ}^2\]</p><p>on a manifold by using the Steihaug-Toint truncated conjugate-gradient method, abbreviated tCG-method. All terms involving the trust-region radius use an inner product w.r.t. the preconditioner; this is because the iterates grow in length w.r.t. the preconditioner, guaranteeing that we do not re-enter the trust-region.</p><h2 id="Initialization"><a class="docs-heading-anchor" href="#Initialization">Initialization</a><a id="Initialization-1"></a><a class="docs-heading-anchor-permalink" href="#Initialization" title="Permalink"></a></h2><p>Initialize <span>$η_0 = η$</span> if using randomized approach and <span>$η$</span> the zero tangent vector otherwise, <span>$r_0 = \operatorname{grad}F(x)$</span>, <span>$z_0 = \operatorname{P}(r_0)$</span>, <span>$δ_0 = z_0$</span> and <span>$k=0$</span></p><h2 id="Iteration"><a class="docs-heading-anchor" href="#Iteration">Iteration</a><a id="Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Iteration" title="Permalink"></a></h2><p>Repeat until a convergence criterion is reached</p><ol><li>Set <span>$α =\frac{⟨r_k, z_k⟩_x}{⟨δ_k, \mathcal{H}[δ_k]⟩_x}$</span> and  <span>$⟨η_k, η_k⟩_{x}^* = ⟨η_k, \operatorname{P}(η_k)⟩_x +  2α ⟨η_k, \operatorname{P}(δ_k)⟩_{x} +  {α}^2  ⟨δ_k, \operatorname{P}(δ_k)⟩_{x}$</span>.</li><li>If <span>$⟨δ_k, \mathcal{H}[δ_k]⟩_x ≤ 0$</span> or <span>$⟨η_k, η_k⟩_x^* ≥ Δ^2$</span>  return <span>$η_{k+1} = η_k + τ δ_k$</span> and stop.</li><li>Set <span>$η_{k}^*= η_k + α δ_k$</span>, if  <span>$⟨η_k, η_k⟩_{x} + \frac{1}{2} ⟨η_k,  \operatorname{Hess}[F] (η_k)_{x}⟩_{x} ≤ ⟨η_k^*,  η_k^*⟩_{x} + \frac{1}{2} ⟨η_k^*,  \operatorname{Hess}[F] (η_k)_ {x}⟩_{x}$</span>  set <span>$η_{k+1} = η_k$</span> else set <span>$η_{k+1} = η_{k}^*$</span>.</li><li>Set <span>$r_{k+1} = r_k + α \mathcal{H}[δ_k]$</span>,   <span>$z_{k+1} = \operatorname{P}(r_{k+1})$</span>,   <span>$β = \frac{⟨r_{k+1},  z_{k+1}⟩_{x}}{⟨r_k, z_k ⟩_{x}}$</span> and <span>$δ_{k+1} = -z_{k+1} + β δ_k$</span>.</li><li>Set <span>$k=k+1$</span>.</li></ol><h2 id="Result"><a class="docs-heading-anchor" href="#Result">Result</a><a id="Result-1"></a><a class="docs-heading-anchor-permalink" href="#Result" title="Permalink"></a></h2><p>The result is given by the last computed <span>$η_k$</span>.</p><h2 id="Remarks"><a class="docs-heading-anchor" href="#Remarks">Remarks</a><a id="Remarks-1"></a><a class="docs-heading-anchor-permalink" href="#Remarks" title="Permalink"></a></h2><p>The <span>$\operatorname{P}(⋅)$</span> denotes the symmetric, positive deﬁnite preconditioner. It is required if a randomized approach is used i.e. using a random tangent vector <span>$η_0$</span> as the initial vector. The idea behind it is to avoid saddle points. Preconditioning is simply a rescaling of the variables and thus a redefinition of the shape of the trust region. Ideally <span>$\operatorname{P}(⋅)$</span> is a cheap, positive approximation of the inverse of the Hessian of <span>$F$</span> at <span>$x$</span>. On default, the preconditioner is just the identity.</p><p>To step number 2: obtain <span>$τ$</span> from the positive root of <span>$\left\lVert η_k + τ δ_k \right\rVert_{\operatorname{P}, x} = Δ$</span> what becomes after the conversion of the equation to</p><p class="math-container">\[ τ = \frac{-⟨η_k, \operatorname{P}(δ_k)⟩_{x} +
 \sqrt{⟨η_k, \operatorname{P}(δ_k)⟩_{x}^{2} +
 ⟨δ_k, \operatorname{P}(δ_k)⟩_{x} ( Δ^2 -
 ⟨η_k, \operatorname{P}(η_k)⟩_{x})}}
 {⟨δ_k, \operatorname{P}(δ_k)⟩_{x}}.\]</p><p>It can occur that <span>$⟨δ_k, \operatorname{Hess}[F] (δ_k)_{x}⟩_{x} = κ ≤ 0$</span> at iteration <span>$k$</span>. In this case, the model is not strictly convex, and the stepsize <span>$α =\frac{⟨r_k, z_k⟩_{x}} {κ}$</span> computed in step 1. does not give a reduction in the model function <span>$m_x(⋅)$</span>. Indeed, <span>$m_x(⋅)$</span> is unbounded from below along the line <span>$η_k + α δ_k$</span>. If our aim is to minimize the model within the trust-region, it makes far more sense to reduce <span>$m_x(⋅)$</span> along <span>$η_k + α δ_k$</span> as much as we can while staying within the trust-region, and this means moving to the trust-region boundary along this line. Thus, when <span>$κ ≤ 0$</span> at iteration k, we replace <span>$α = \frac{⟨r_k, z_k⟩_{x}}{κ}$</span> with <span>$τ$</span> described as above. The other possibility is that <span>$η_{k+1}$</span> would lie outside the trust-region at iteration k (i.e. <span>$⟨η_k, η_k⟩_{x}^{* } ≥ {Δ}^2$</span> that can be identified with the norm of <span>$η_{k+1}$</span>). In particular, when <span>$\operatorname{Hess}[F] (⋅)_{x}$</span> is positive deﬁnite and <span>$η_{k+1}$</span> lies outside the trust region, the solution to the trust-region problem must lie on the trust-region boundary. Thus, there is no reason to continue with the conjugate gradient iteration, as it stands, as subsequent iterates will move further outside the trust-region boundary. A sensible strategy, just as in the case considered above, is to move to the trust-region boundary by finding <span>$τ$</span>.</p><p>Although it is virtually impossible in practice to know how many iterations are necessary to provide a good estimate <span>$η_{k}$</span> of the trust-region subproblem, the method stops after a certain number of iterations, which is realised by <a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a>. In order to increase the convergence rate of the underlying trust-region method, see <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a>, a typical stopping criterion is to stop as soon as an iteration <span>$k$</span> is reached for which</p><p class="math-container">\[  \Vert r_k \Vert_x \leqq \Vert r_0 \Vert_x \min \left( \Vert r_0 \Vert^{θ}_x, κ \right)\]</p><p>holds, where <span>$0 &lt; κ &lt; 1$</span> and <span>$θ &gt; 0$</span> are chosen in advance. This is realized in this method by <a href="#Manopt.StopWhenResidualIsReducedByFactorOrPower"><code>StopWhenResidualIsReducedByFactorOrPower</code></a>. It can be shown shown that under appropriate conditions the iterates <span>$x_k$</span> of the underlying trust-region method converge to nondegenerate critical points with an order of convergence of at least <span>$\min \left( θ + 1, 2 \right)$</span>, see [<a href="#AbsilMahonySepulchre2008">Absil, Mahony, Sepulchre, 2008</a>]. The method also aborts if the curvature of the model is negative, i.e. if <span>$\langle \delta_k, \mathcal{H}[δ_k] \rangle_x \leqq 0$</span>, which is realised by <a href="#Manopt.StopWhenCurvatureIsNegative"><code>StopWhenCurvatureIsNegative</code></a>. If the next possible approximate solution <span>$η_{k}^{*}$</span> calculated in iteration <span>$k$</span> lies outside the trust region, i.e. if <span>$\lVert η_{k}^{*} \rVert_x \geq Δ$</span>, then the method aborts, which is realised by <a href="#Manopt.StopWhenTrustRegionIsExceeded"><code>StopWhenTrustRegionIsExceeded</code></a>. Furthermore, the method aborts if the new model value evaluated at <span>$η_{k}^{*}$</span> is greater than the previous model value evaluated at <span>$η_{k}$</span>, which is realised by <a href="#Manopt.StopWhenModelIncreased"><code>StopWhenModelIncreased</code></a>.</p><h2 id="Interface"><a class="docs-heading-anchor" href="#Interface">Interface</a><a id="Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.truncated_conjugate_gradient_descent" href="#Manopt.truncated_conjugate_gradient_descent"><code>Manopt.truncated_conjugate_gradient_descent</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">truncated_conjugate_gradient_descent(M, f, grad_f, p; kwargs...)
truncated_conjugate_gradient_descent(M, f, grad_f, p, X; kwargs...)
truncated_conjugate_gradient_descent(M, f, grad_f, Hess_f; kwargs...)
truncated_conjugate_gradient_descent(M, f, grad_f, Hess_f, p; kwargs...)
truncated_conjugate_gradient_descent(M, f, grad_f, Hess_f, p, X; kwargs...)
truncated_conjugate_gradient_descent(M, mho::ManifoldHessianObjective, p, X; kwargs...)</code></pre><p>solve the trust-region subproblem</p><p class="math-container">\[\operatorname*{arg\,min}_{η ∈ T_pM}
m_p(η) \quad\text{where}
m_p(η) = f(p) + ⟨\operatorname{grad} f(p),η⟩_x + \frac{1}{2}⟨\operatorname{Hess} f(p)[η],η⟩_x,\]</p><p class="math-container">\[\text{such that}\quad ⟨η,η⟩_x ≤ Δ^2\]</p><p>on a manifold M by using the Steihaug-Toint truncated conjugate-gradient method, abbreviated tCG-method. For a description of the algorithm and theorems offering convergence guarantees, see the reference:</p><ul><li>P.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: <a href="https://doi.org/10.1007/s10208-005-0179-9">10.1007/s10208-005-0179-9</a></li><li>A. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: <a href="https://doi.org/10.1137/1.9780898719857">10.1137/1.9780898719857</a></li></ul><p><strong>Input</strong></p><p>See signatures above, you can leave out only the Hessian, the vector, the point and the vector, or all 3.</p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>f</code> – a cost function <span>$F: \mathcal M → ℝ$</span> to minimize</li><li><code>grad_f</code> – the gradient <span>$\operatorname{grad}f: \mathcal M → T\mathcal M$</span> of <code>F</code></li><li><code>Hess_f</code> – (optional, cf. <a href="../trust_regions/#Manopt.ApproxHessianFiniteDifference"><code>ApproxHessianFiniteDifference</code></a>) the hessian <span>$\operatorname{Hess}f: T_p\mathcal M → T_p\mathcal M$</span>, <span>$X ↦ \operatorname{Hess}F(p)[X] = ∇_X\operatorname{grad}f(p)$</span></li><li><code>p</code> – a point on the manifold <span>$p ∈ \mathcal M$</span></li><li><code>X</code> – an update tangential vector <span>$X ∈ T_p\mathcal M$</span></li></ul><p><strong>Optional</strong></p><ul><li><code>evaluation</code> – (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient and hessian work by  allocation (default) or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> in place</li><li><code>preconditioner</code> – a preconditioner for the hessian H</li><li><code>θ</code> – (<code>1.0</code>) 1+θ is the superlinear convergence target rate. The method aborts   if the residual is less than or equal to the initial residual to the power of 1+θ.</li><li><code>κ</code> – (<code>0.1</code>) the linear convergence target rate. The method aborts if the   residual is less than or equal to κ times the initial residual.</li><li><code>randomize</code> – set to true if the trust-region solve is to be initiated with a   random tangent vector. If set to true, no preconditioner will be   used. This option is set to true in some scenarios to escape saddle   points, but is otherwise seldom activated.</li><li><code>trust_region_radius</code> – (<code>injectivity_radius(M)/4</code>) a trust-region radius</li><li><code>project!</code> : (<code>copyto!</code>) specify a projection operation for tangent vectors   for numerical stability. A function <code>(M, Y, p, X) -&gt; ...</code> working in place of <code>Y</code>.   per default, no projection is perfomed, set it to <code>project!</code> to activate projection.</li><li><code>stopping_criterion</code> – (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>| [</code>StopWhenResidualIsReducedByFactorOrPower<code>](@ref)</code> | &#39;<a href="#Manopt.StopWhenCurvatureIsNegative"><code>StopWhenCurvatureIsNegative</code></a><code>|</code><a href="#Manopt.StopWhenTrustRegionIsExceeded"><code>StopWhenTrustRegionIsExceeded</code></a> )   a functor inheriting from <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> indicating when to stop,   where for the default, the maximal number of iterations is set to the dimension of the   manifold, the power factor is <code>θ</code>, the reduction factor is <code>κ</code>.</li></ul><p>and the ones that are passed to <a href="../../plans/state/#Manopt.decorate_state!"><code>decorate_state!</code></a> for decorators.</p><p><strong>Output</strong></p><p>the obtained (approximate) minimizer <span>$\eta^*$</span>, see <a href="../#Manopt.get_solver_return"><code>get_solver_return</code></a> for details</p><p><strong>see also</strong></p><p><a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b3c826e0ec086873ea43a06a40a1c15567c902f0/src/solvers/truncated_conjugate_gradient_descent.jl#L331-L404">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.truncated_conjugate_gradient_descent!" href="#Manopt.truncated_conjugate_gradient_descent!"><code>Manopt.truncated_conjugate_gradient_descent!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">truncated_conjugate_gradient_descent!(M, f, grad_f, Hess_f, p, X; kwargs...)
truncated_conjugate_gradient_descent!(M, f, grad_f, p, X; kwargs...)</code></pre><p>solve the trust-region subproblem in place of <code>X</code> (and <code>p</code>).</p><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>f</code> – a cost function <span>$F: \mathcal M → ℝ$</span> to minimize</li><li><code>grad_f</code> – the gradient <span>$\operatorname{grad}f: \mathcal M → T\mathcal M$</span> of <code>f</code></li><li><code>Hess_f</code> – the hessian <span>$\operatorname{Hess}f(x): T_p\mathcal M → T_p\mathcal M$</span>, <span>$X ↦ \operatorname{Hess}f(p)[X]$</span></li><li><code>p</code> – a point on the manifold <span>$p ∈ \mathcal M$</span></li><li><code>X</code> – an update tangential vector <span>$X ∈ T_x\mathcal M$</span></li></ul><p>For more details and all optional arguments, see <a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b3c826e0ec086873ea43a06a40a1c15567c902f0/src/solvers/truncated_conjugate_gradient_descent.jl#L534-L549">source</a></section></article><h2 id="State"><a class="docs-heading-anchor" href="#State">State</a><a id="State-1"></a><a class="docs-heading-anchor-permalink" href="#State" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.TruncatedConjugateGradientState" href="#Manopt.TruncatedConjugateGradientState"><code>Manopt.TruncatedConjugateGradientState</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TruncatedConjugateGradientState &lt;: AbstractHessianSolverState</code></pre><p>describe the Steihaug-Toint truncated conjugate-gradient method, with</p><p><strong>Fields</strong></p><p>a default value is given in brackets if a parameter can be left out in initialization.</p><ul><li><code>x</code> : a point, where the trust-region subproblem needs   to be solved</li><li><code>η</code> : a tangent vector (called update vector), which solves the   trust-region subproblem after successful calculation by the algorithm</li><li><code>stop</code> : a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a>.</li><li><code>gradient</code> : the gradient at the current iterate</li><li><code>δ</code> : search direction</li><li><code>trust_region_radius</code> : (<code>injectivity_radius(M)/4</code>) the trust-region radius</li><li><code>residual</code> : the gradient</li><li><code>randomize</code> : indicates if the trust-region solve and so the algorithm is to be       initiated with a random tangent vector. If set to true, no       preconditioner will be used. This option is set to true in some       scenarios to escape saddle points, but is otherwise seldom activated.</li><li><code>project!</code> : (<code>copyto!</code>) specify a projection operation for tangent vectors   for numerical stability. A function <code>(M, Y, p, X) -&gt; ...</code> working in place of <code>Y</code>.   per default, no projection is perfomed, set it to <code>project!</code> to activate projection.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">TruncatedConjugateGradientState(M, p=rand(M), η=zero_vector(M,p);
    trust_region_radius=injectivity_radius(M)/4,
    randomize=false,
    θ=1.0,
    κ=0.1,
    project!=copyto!,
)

and a slightly involved `stopping_criterion`</code></pre><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b3c826e0ec086873ea43a06a40a1c15567c902f0/src/solvers/truncated_conjugate_gradient_descent.jl#L1-L41">source</a></section></article><h2 id="Stopping-Criteria"><a class="docs-heading-anchor" href="#Stopping-Criteria">Stopping Criteria</a><a id="Stopping-Criteria-1"></a><a class="docs-heading-anchor-permalink" href="#Stopping-Criteria" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenResidualIsReducedByFactorOrPower" href="#Manopt.StopWhenResidualIsReducedByFactorOrPower"><code>Manopt.StopWhenResidualIsReducedByFactorOrPower</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopWhenResidualIsReducedByFactorOrPower &lt;: StoppingCriterion</code></pre><p>A functor for testing if the norm of residual at the current iterate is reduced either by a power of 1+θ or by a factor κ compared to the norm of the initial residual, i.e. <span>$\Vert r_k \Vert_x \leqq \Vert r_0 \Vert_{x} \
\min \left( \kappa, \Vert r_0 \Vert_{x}^{\theta} \right)$</span>.</p><p><strong>Fields</strong></p><ul><li><code>κ</code> – the reduction factor</li><li><code>θ</code> – part of the reduction power</li><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="../../plans/stopping_criteria/#Manopt.get_reason-Tuple{AbstractManoptSolverState}"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopWhenResidualIsReducedByFactorOrPower(; κ=0.1, θ=1.0)</code></pre><p>initialize the StopWhenResidualIsReducedByFactorOrPower functor to indicate to stop after the norm of the current residual is lesser than either the norm of the initial residual to the power of 1+θ or the norm of the initial residual times κ.</p><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b3c826e0ec086873ea43a06a40a1c15567c902f0/src/solvers/truncated_conjugate_gradient_descent.jl#L113-L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenTrustRegionIsExceeded" href="#Manopt.StopWhenTrustRegionIsExceeded"><code>Manopt.StopWhenTrustRegionIsExceeded</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopWhenTrustRegionIsExceeded &lt;: StoppingCriterion</code></pre><p>A functor for testing if the norm of the next iterate in the  Steihaug-Toint tcg mehtod is larger than the trust-region radius, i.e. <span>$\Vert η_{k}^{*} \Vert_x ≧ trust_region_radius$</span>. terminate the algorithm when the trust region has been left.</p><p><strong>Fields</strong></p><ul><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="../../plans/stopping_criteria/#Manopt.get_reason-Tuple{AbstractManoptSolverState}"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopWhenTrustRegionIsExceeded()</code></pre><p>initialize the StopWhenTrustRegionIsExceeded functor to indicate to stop after the norm of the next iterate is greater than the trust-region radius.</p><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b3c826e0ec086873ea43a06a40a1c15567c902f0/src/solvers/truncated_conjugate_gradient_descent.jl#L190-L211">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenCurvatureIsNegative" href="#Manopt.StopWhenCurvatureIsNegative"><code>Manopt.StopWhenCurvatureIsNegative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopWhenCurvatureIsNegative &lt;: StoppingCriterion</code></pre><p>A functor for testing if the curvature of the model is negative, i.e. <span>$\langle \delta_k, \operatorname{Hess}[F](\delta_k)\rangle_x \leqq 0$</span>. In this case, the model is not strictly convex, and the stepsize as computed does not give a reduction of the model.</p><p><strong>Fields</strong></p><ul><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="../../plans/stopping_criteria/#Manopt.get_reason-Tuple{AbstractManoptSolverState}"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopWhenCurvatureIsNegative()</code></pre><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b3c826e0ec086873ea43a06a40a1c15567c902f0/src/solvers/truncated_conjugate_gradient_descent.jl#L239-L258">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenModelIncreased" href="#Manopt.StopWhenModelIncreased"><code>Manopt.StopWhenModelIncreased</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopWhenModelIncreased &lt;: StoppingCriterion</code></pre><p>A functor for testing if the curvature of the model value increased.</p><p><strong>Fields</strong></p><ul><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="../../plans/stopping_criteria/#Manopt.get_reason-Tuple{AbstractManoptSolverState}"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopWhenModelIncreased()</code></pre><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b3c826e0ec086873ea43a06a40a1c15567c902f0/src/solvers/truncated_conjugate_gradient_descent.jl#L287-L303">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.update_stopping_criterion!-Tuple{StopWhenResidualIsReducedByFactorOrPower, Val{:ResidualPower}, Any}" href="#Manopt.update_stopping_criterion!-Tuple{StopWhenResidualIsReducedByFactorOrPower, Val{:ResidualPower}, Any}"><code>Manopt.update_stopping_criterion!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">update_stopping_criterion!(c::StopWhenResidualIsReducedByFactorOrPower, :ResidualPower, v)</code></pre><p>Update the residual Power <code>θ</code>  to <code>v</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b3c826e0ec086873ea43a06a40a1c15567c902f0/src/solvers/truncated_conjugate_gradient_descent.jl#L168-L171">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.update_stopping_criterion!-Tuple{StopWhenResidualIsReducedByFactorOrPower, Val{:ResidualFactor}, Any}" href="#Manopt.update_stopping_criterion!-Tuple{StopWhenResidualIsReducedByFactorOrPower, Val{:ResidualFactor}, Any}"><code>Manopt.update_stopping_criterion!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">update_stopping_criterion!(c::StopWhenResidualIsReducedByFactorOrPower, :ResidualFactor, v)</code></pre><p>Update the residual Factor <code>κ</code> to <code>v</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b3c826e0ec086873ea43a06a40a1c15567c902f0/src/solvers/truncated_conjugate_gradient_descent.jl#L179-L182">source</a></section></article><h2 id="Literature"><a class="docs-heading-anchor" href="#Literature">Literature</a><a id="Literature-1"></a><a class="docs-heading-anchor-permalink" href="#Literature" title="Permalink"></a></h2><ul>
<li id="AbsilMahonySepulchre2008">[<a>Absil, Mahony, Sepulchre, 2008</a>]
  Absil, Pierre-Antoine and Mahony, Robert and Sepulchre, Rodolphe:
  <emph> Optimization Algorithms on Matrix Manifolds </emph>
  Mathematics of Computation - Math. Comput., Volume 78.
  doi: <a href="https://doi.org/10.1515/9781400830244">10.1515/9781400830244</a>,
</li>
</ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../subgradient/">« Subgradient method</a><a class="docs-footer-nextpage" href="../trust_regions/">Trust-Regions Solver »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Monday 22 May 2023 09:32">Monday 22 May 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
