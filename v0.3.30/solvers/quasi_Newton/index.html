<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quasi-Newton · Manopt.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../../tutorials/Optimize!/">Get started: Optimize!</a></li><li><a class="tocitem" href="../../tutorials/AutomaticDifferentiation/">Use AD in Manopt</a></li><li><a class="tocitem" href="../../tutorials/HowToRecord/">Record values</a></li><li><a class="tocitem" href="../../tutorials/GeodesicRegression/">Do Geodesic regression</a></li><li><a class="tocitem" href="../../tutorials/Bezier/">Use Bezier Curves</a></li><li><a class="tocitem" href="../../tutorials/SecondOrderDifference/">Compute a second order difference</a></li><li><a class="tocitem" href="../../tutorials/StochasticGradientDescent/">Do stochastic gradient descent</a></li><li><a class="tocitem" href="../../tutorials/Benchmark/">speed up! using <code>gradF!</code></a></li><li><a class="tocitem" href="../../tutorials/JacobiFields/">Illustrate Jacobi Fields</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../DouglasRachford/">Douglas–Rachford</a></li><li><a class="tocitem" href="../gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../NelderMead/">Nelder–Mead</a></li><li><a class="tocitem" href="../particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li class="is-active"><a class="tocitem" href>Quasi-Newton</a><ul class="internal"><li><a class="tocitem" href="#Background"><span>Background</span></a></li><li><a class="tocitem" href="#Direction-Updates"><span>Direction Updates</span></a></li><li><a class="tocitem" href="#Hessian-Update-Rules"><span>Hessian Update Rules</span></a></li><li><a class="tocitem" href="#Options"><span>Options</span></a></li><li><a class="tocitem" href="#Literature"><span>Literature</span></a></li></ul></li><li><a class="tocitem" href="../stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/robustPCA/">Robust PCA</a></li><li><a class="tocitem" href="../../examples/smallestEigenvalue/">Rayleigh quotient</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/options/">Options</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../../functions/">Introduction</a></li><li><a class="tocitem" href="../../functions/bezier/">Bézier curves</a></li><li><a class="tocitem" href="../../functions/costs/">Cost functions</a></li><li><a class="tocitem" href="../../functions/differentials/">Differentials</a></li><li><a class="tocitem" href="../../functions/adjointdifferentials/">Adjoint Differentials</a></li><li><a class="tocitem" href="../../functions/gradients/">Gradients</a></li><li><a class="tocitem" href="../../functions/Jacobi_fields/">Jacobi Fields</a></li><li><a class="tocitem" href="../../functions/proximal_maps/">Proximal Maps</a></li><li><a class="tocitem" href="../../functions/manifold/">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/data/">Data</a></li><li><a class="tocitem" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../list/">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href>Quasi-Newton</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Quasi-Newton</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/solvers/quasi_Newton.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="quasiNewton"><a class="docs-heading-anchor" href="#quasiNewton">Riemannian quasi-Newton methods</a><a id="quasiNewton-1"></a><a class="docs-heading-anchor-permalink" href="#quasiNewton" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Manopt.quasi_Newton" href="#Manopt.quasi_Newton"><code>Manopt.quasi_Newton</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">quasi_Newton(M, F, gradF, x)</code></pre><p>Perform a quasi Newton iteration for <code>F</code> on the manifold <code>M</code> starting in the point <code>x</code> using a retraction <span>$R$</span> and a vector transport <span>$T$</span></p><p>The <span>$k$</span>th iteration consists of</p><ol><li>Compute the search direction <span>$η_k = -\mathcal{B}_k [\operatorname{grad}f (x_k)]$</span> or solve <span>$\mathcal{H}_k [η_k] = -\operatorname{grad}f (x_k)]$</span>.</li><li>Determine a suitable stepsize <span>$α_k$</span> along the curve <span>$\gamma(α) = R_{x_k}(α η_k)$</span> e.g. by using <a href="../../plans/stepsize/#Manopt.WolfePowellLinesearch"><code>WolfePowellLinesearch</code></a>.</li><li>Compute <span>$x_{k+1} = R_{x_k}(α_k η_k)$</span>.</li><li>Define <span>$s_k = T_{x_k, α_k η_k}(α_k η_k)$</span> and <span>$y_k = \operatorname{grad}f(x_{k+1}) - T_{x_k, α_k η_k}(\operatorname{grad}f(x_k))$</span>.</li><li>Compute the new approximate Hessian <span>$H_{k+1}$</span> or its inverse <span>$B_k$</span>.</li></ol><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal{M}$</span>.</li><li><code>F</code> – a cost function <span>$F : \mathcal{M} →ℝ$</span> to minimize.</li><li><code>gradF</code>– the gradient <span>$\operatorname{grad}F : \mathcal{M} →T_x\mathcal M$</span> of <span>$F$</span>.</li><li><code>x</code> – an initial value <span>$x ∈ \mathcal{M}$</span>.</li></ul><p><strong>Optional</strong></p><ul><li><code>basis</code> – (<code>DefaultOrthonormalBasis()</code>) basis within the tangent space(s) to represent the Hessian (inverse).</li><li><code>cautious_update</code> – (<code>false</code>) – whether or not to use a <a href="#Manopt.QuasiNewtonCautiousDirectionUpdate"><code>QuasiNewtonCautiousDirectionUpdate</code></a></li><li><code>cautious_function</code> – (<code>(x) -&gt; x*10^(-4)</code>) – a monotone increasing function that is zero at 0 and strictly increasing at 0 for the cautious update.</li><li><code>direction_update</code> – (<a href="#Manopt.InverseBFGS"><code>InverseBFGS</code></a><code>()</code>) the update rule to use.</li><li><code>evaluation</code> – (<a href="../../plans/problem/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient works by  allocation (default) form <code>gradF(M, x)</code> or <a href="../../plans/problem/#Manopt.MutatingEvaluation"><code>MutatingEvaluation</code></a> in place, i.e.  is of the form <code>gradF!(M, X, x)</code>.</li><li><code>initial_operator</code> – (<code>Matrix{Float64}(I,n,n)</code>) initial matrix to use die the approximation, where <code>n=manifold_dimension(M)</code>, see also <code>scale_initial_operator</code>.</li><li><code>memory_size</code> – (<code>20</code>) limited memory, number of <span>$s_k, y_k$</span> to store. Set to a negative value to use a full memory representation</li><li><code>retraction_method</code> – (<code>default_retraction_method(M)</code>) a retraction method to use, by default the exponential map.</li><li><code>scale_initial_operator</code> - (<code>true</code>) scale initial operator with <span>$\frac{⟨s_k,y_k⟩_{x_k}}{\lVert y_k\rVert_{x_k}}$</span> in the computation</li><li><code>stabilize</code> – (<code>true</code>) stabilize the method numerically by projecting computed (Newton-) directions to the tangent space to reduce numerical errors</li><li><code>stepsize</code> – (<a href="../../plans/stepsize/#Manopt.WolfePowellLinesearch"><code>WolfePowellLinesearch</code></a><code>(retraction_method, vector_transport_method)</code>) specify a <a href="../../plans/stepsize/#Manopt.Stepsize"><code>Stepsize</code></a>.</li><li><code>stopping_criterion</code> - (<code>StopWhenAny(StopAfterIteration(max(1000, memory_size)), StopWhenGradientNormLess(10^(-6))</code>) specify a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a></li><li><code>vector_transport_method</code> – (<code>default_vector_transport_method(M)</code>) a vector transport to use.</li><li><code>return_options</code> – (<code>false</code>) – specify whether to return just the result <code>x</code> (default) or the complete <a href="../../plans/options/#Manopt.Options"><code>Options</code></a>, e.g. to access recorded values. if activated, the extended result, i.e. the</li></ul><p><strong>Output</strong></p><ul><li><code>x_opt</code> – the resulting (approximately critical) point of the quasi–Newton method</li></ul><p>OR</p><ul><li><code>options</code> – the options returned by the solver (see <code>return_options</code>)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/solvers/quasi_Newton.jl#L1-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.quasi_Newton!" href="#Manopt.quasi_Newton!"><code>Manopt.quasi_Newton!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">quasi_Newton!(M, F, gradF, x; options...)</code></pre><p>Perform a quasi Newton iteration for <code>F</code> on the manifold <code>M</code> starting in the point <code>x</code> using a retraction <span>$R$</span> and a vector transport <span>$T$</span>.</p><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal{M}$</span>.</li><li><code>F</code> – a cost function <span>$F: \mathcal{M} →ℝ$</span> to minimize.</li><li><code>gradF</code>– the gradient <span>$\operatorname{grad}F : \mathcal{M} → T_x\mathcal M$</span> of <span>$F$</span>.</li><li><code>x</code> – an initial value <span>$x ∈ \mathcal{M}$</span>.</li></ul><p>For all optional parameters, see <a href="#Manopt.quasi_Newton"><code>quasi_Newton</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/solvers/quasi_Newton.jl#L57-L70">source</a></section></article><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><p>The aim is to minimize a real-valued function on a Riemannian manifold, i.e.</p><p class="math-container">\[\min f(x), \quad x ∈ \mathcal{M}.\]</p><p>Riemannian quasi-Newtonian methods are as generalizations of their Euclidean counterparts Riemannian line search methods. These methods determine a search direction <span>$η_k ∈ T_{x_k} \mathcal{M}$</span> at the current iterate <span>$x_k$</span> and a suitable stepsize <span>$α_k$</span> along <span>$\gamma(α) = R_{x_k}(α η_k)$</span>, where <span>$R: T \mathcal{M} →\mathcal{M}$</span> is a retraction. The next iterate is obtained by</p><p class="math-container">\[x_{k+1} = R_{x_k}(α_k η_k).\]</p><p>In quasi-Newton methods, the search direction is given by</p><p class="math-container">\[η_k = -{\mathcal{H}_k}^{-1}[\operatorname{grad}f (x_k)] = -\mathcal{B}_k [\operatorname{grad} (x_k)],\]</p><p>where <span>$\mathcal{H}_k : T_{x_k} \mathcal{M} →T_{x_k} \mathcal{M}$</span> is a positive definite self-adjoint operator, which approximates the action of the Hessian <span>$\operatorname{Hess} f (x_k)[⋅]$</span> and <span>$\mathcal{B}_k = {\mathcal{H}_k}^{-1}$</span>. The idea of quasi-Newton methods is instead of creating a complete new approximation of the Hessian operator <span>$\operatorname{Hess} f(x_{k+1})$</span> or its inverse at every iteration, the previous operator <span>$\mathcal{H}_k$</span> or <span>$\mathcal{B}_k$</span> is updated by a convenient formula using the obtained information about the curvature of the objective function during the iteration. The resulting operator <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> acts on the tangent space <span>$T_{x_{k+1}} \mathcal{M}$</span> of the freshly computed iterate <span>$x_{k+1}$</span>. In order to get a well-defined method, the following requirements are placed on the new operator <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> that is created by an update. Since the Hessian <span>$\operatorname{Hess} f(x_{k+1})$</span> is a self-adjoint operator on the tangent space <span>$T_{x_{k+1}} \mathcal{M}$</span>, and <span>$\mathcal{H}_{k+1}$</span> approximates it, we require that <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> is also self-adjoint on <span>$T_{x_{k+1}} \mathcal{M}$</span>. In order to achieve a steady descent, we want <span>$η_k$</span> to be a descent direction in each iteration. Therefore we require, that <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> is a positive definite operator on <span>$T_{x_{k+1}} \mathcal{M}$</span>. In order to get information about the curvature of the objective function into the new operator <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span>, we require that it satisfies a form of a Riemannian quasi-Newton equation:</p><p class="math-container">\[\mathcal{H}_{k+1} [T_{x_k \rightarrow x_{k+1}}({R_{x_k}}^{-1}(x_{k+1}))] = \operatorname{grad}(x_{k+1}) - T_{x_k \rightarrow x_{k+1}}(\operatorname{grad}f(x_k))\]</p><p>or</p><p class="math-container">\[\mathcal{B}_{k+1} [\operatorname{grad}f(x_{k+1}) - T_{x_k \rightarrow x_{k+1}}(\operatorname{grad}f(x_k))] = T_{x_k \rightarrow x_{k+1}}({R_{x_k}}^{-1}(x_{k+1}))\]</p><p>where <span>$T_{x_k \rightarrow x_{k+1}} : T_{x_k} \mathcal{M} →T_{x_{k+1}} \mathcal{M}$</span> and the chosen retraction <span>$R$</span> is the associated retraction of <span>$T$</span>. We note that, of course, not all updates in all situations will meet these conditions in every iteration. For specific quasi-Newton updates, the fulfilment of the Riemannian curvature condition, which requires that</p><p class="math-container">\[g_{x_{k+1}}(s_k, y_k) &gt; 0\]</p><p>holds, is a requirement for the inheritance of the self-adjointness and positive definiteness of the <span>$\mathcal{H}_k$</span> or <span>$\mathcal{B}_k$</span> to the operator <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span>. Unfortunately, the fulfillment of the Riemannian curvature condition is not given by a step size <span>$\alpha_k &gt; 0$</span> that satisfies the generalised Wolfe conditions. However, in order to create a positive definite operator <span>$\mathcal{H}_{k+1}$</span> or <span>$\mathcal{B}_{k+1}$</span> in each iteration, in <sup class="footnote-reference"><a id="citeref-HuangGallivanAbsil2015" href="#footnote-HuangGallivanAbsil2015">[HuangGallivanAbsil2015]</a></sup> the so-called locking condition was introduced, which requires that the isometric vector transport <span>$T^S$</span>, which is used in the update formula, and its associate retraction <span>$R$</span> fulfill</p><p class="math-container">\[T^{S}{x, ξ_x}(ξ_x) = β T^{R}{x, ξ_x}(ξ_x), \quad β = \frac{\lVert ξ_x \rVert_x}{\lVert T^{R}{x, ξ_x}(ξ_x) \rVert_{R_{x}(ξ_x)}},\]</p><p>where <span>$T^R$</span> is the vector transport by differentiated retraction. With the requirement that the isometric vector transport <span>$T^S$</span> and its associated retraction <span>$R$</span> satisfies the locking condition and using the tangent vector</p><p class="math-container">\[y_k = {β_k}^{-1} \operatorname{grad}f(x_{k+1}) - T^{S}{x_k, α_k η_k}(\operatorname{grad}f(x_k)),\]</p><p>where</p><p class="math-container">\[β_k = \frac{\lVert α_k η_k \rVert_{x_k}}{\lVert T^{R}{x_k, α_k η_k}(α_k η_k) \rVert_{x_{k+1}}},\]</p><p>in the update, it can be shown that choosing a stepsize <span>$α_k &gt; 0$</span> that satisfies the Riemannian Wolfe conditions leads to the fulfilment of the Riemannian curvature condition, which in turn implies that the operator generated by the updates is positive definite. In the following we denote the specific operators in matrix notation and hence use <span>$H_k$</span> and <span>$B_k$</span>, respectively.</p><h2 id="Direction-Updates"><a class="docs-heading-anchor" href="#Direction-Updates">Direction Updates</a><a id="Direction-Updates-1"></a><a class="docs-heading-anchor-permalink" href="#Direction-Updates" title="Permalink"></a></h2><p>In general there are different ways to compute a fixed <a href="#Manopt.AbstractQuasiNewtonUpdateRule"><code>AbstractQuasiNewtonUpdateRule</code></a>. In general these are represented by</p><article class="docstring"><header><a class="docstring-binding" id="Manopt.AbstractQuasiNewtonDirectionUpdate" href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>Manopt.AbstractQuasiNewtonDirectionUpdate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractQuasiNewtonDirectionUpdate</code></pre><p>An abstract representation of an Quasi Newton Update rule to determine the next direction given current <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a>.</p><p>All subtypes should be functors, i.e. one should be able to call them as <code>H(M,x,d)</code> to compute a new direction update.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L1-L8">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.QuasiNewtonMatrixDirectionUpdate" href="#Manopt.QuasiNewtonMatrixDirectionUpdate"><code>Manopt.QuasiNewtonMatrixDirectionUpdate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuasiNewtonMatrixDirectionUpdate &lt;: AbstractQuasiNewtonDirectionUpdate</code></pre><p>These <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a>s represent any quasi-Newton update rule, where the operator is stored as a matrix. A distinction is made between the update of the approximation of the Hessian, <span>$H_k \mapsto H_{k+1}$</span>, and the update of the approximation of the Hessian inverse, <span>$B_k \mapsto B_{k+1}$</span>. For the first case, the coordinates of the search direction <span>$η_k$</span> with respect to a basis <span>$\{b_i\}^{n}_{i=1}$</span> are determined by solving a linear system of equations, i.e.</p><p class="math-container">\[\text{Solve} \quad \hat{η_k} = - H_k \widehat{\operatorname{grad}f(x_k)}\]</p><p>where <span>$H_k$</span> is the matrix representing the operator with respect to the basis <span>$\{b_i\}^{n}_{i=1}$</span> and <span>$\widehat{\operatorname{grad}f(x_k)}$</span> represents the coordinates of the gradient of the objective function <span>$f$</span> in <span>$x_k$</span> with respect to the basis <span>$\{b_i\}^{n}_{i=1}$</span>. If a method is chosen where Hessian inverse is approximated, the coordinates of the search direction <span>$η_k$</span> with respect to a basis <span>$\{b_i\}^{n}_{i=1}$</span> are obtained simply by matrix-vector multiplication, i.e.</p><p class="math-container">\[\hat{η_k} = - B_k \widehat{\operatorname{grad}f(x_k)}\]</p><p>where <span>$B_k$</span> is the matrix representing the operator with respect to the basis <span>$\{b_i\}^{n}_{i=1}$</span> and <span>$\widehat{\operatorname{grad}f(x_k)}$</span> as above. In the end, the search direction <span>$η_k$</span> is generated from the coordinates <span>$\hat{eta_k}$</span> and the vectors of the basis <span>$\{b_i\}^{n}_{i=1}$</span> in both variants. The <a href="#Manopt.AbstractQuasiNewtonUpdateRule"><code>AbstractQuasiNewtonUpdateRule</code></a> indicates which quasi-Newton update rule is used. In all of them, the Euclidean update formula is used to generate the matrix <span>$H_{k+1}$</span> and <span>$B_{k+1}$</span>, and the basis <span>$\{b_i\}^{n}_{i=1}$</span> is transported into the upcoming tangent space <span>$T_{x_{k+1}} \mathcal{M}$</span>, preferably with an isometric vector transport, or generated there.</p><p><strong>Fields</strong></p><ul><li><code>basis</code> – the basis.</li><li><code>matrix</code> – the matrix which represents the approximating operator.</li><li><code>scale</code> – indicates whether the initial matrix (= identity matrix) should be scaled before the first update.</li><li><code>update</code> – a <a href="#Manopt.AbstractQuasiNewtonUpdateRule"><code>AbstractQuasiNewtonUpdateRule</code></a>.</li><li><code>vector_transport_method</code> – an <code>AbstractVectorTransportMethod</code></li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">QuasiNewtonMatrixDirectionUpdate(M::AbstractMatrix, update, basis, matrix;
scale=true, vector_transport_method=default_vector_transport_method(M))</code></pre><p>Generate the Update rule with defaults from a manifold and the names corresponding to the fields above.</p><p><strong>See also</strong></p><p><a href="#Manopt.QuasiNewtonLimitedMemoryDirectionUpdate"><code>QuasiNewtonLimitedMemoryDirectionUpdate</code></a> <a href="#Manopt.QuasiNewtonCautiousDirectionUpdate"><code>QuasiNewtonCautiousDirectionUpdate</code></a> <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L392-L428">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.QuasiNewtonLimitedMemoryDirectionUpdate" href="#Manopt.QuasiNewtonLimitedMemoryDirectionUpdate"><code>Manopt.QuasiNewtonLimitedMemoryDirectionUpdate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuasiNewtonLimitedMemoryDirectionUpdate &lt;: AbstractQuasiNewtonDirectionUpdate</code></pre><p>This <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> represents the limited-memory Riemanian BFGS update, where the approximating  operator is represented by <span>$m$</span> stored pairs of tangent vectors <span>$\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m}^{k-1}$</span> in the <span>$k$</span>-th iteration. For the calculation of the search direction <span>$η_k$</span>, the generalisation of the two-loop recursion is used (see <sup class="footnote-reference"><a id="citeref-HuangGallivanAbsil2015" href="#footnote-HuangGallivanAbsil2015">[HuangGallivanAbsil2015]</a></sup>), since it only requires inner products and linear combinations of tangent vectors in <span>$T_{x_k} \mathcal{M}$</span>. For that the stored pairs of tangent vectors <span>$\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m}^{k-1}$</span>, the gradient <span>$\operatorname{grad}f(x_k)$</span> of the objective function <span>$f$</span> in <span>$x_k$</span> and the positive definite self-adjoint operator</p><p class="math-container">\[\mathcal{B}^{(0)}_k[⋅] = \frac{g_{x_k}(s_{k-1}, y_{k-1})}{g_{x_k}(y_{k-1}, y_{k-1})} \; \mathrm{id}_{T_{x_k} \mathcal{M}}[⋅]\]</p><p>are used. The two-loop recursion can be understood as that the <a href="#Manopt.InverseBFGS"><code>InverseBFGS</code></a> update is executed <span>$m$</span> times in a row on <span>$\mathcal{B}^{(0)}_k[⋅]$</span> using the tangent vectors <span>$\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m}^{k-1}$</span>, and in the same time the resulting operator <span>$\mathcal{B}^{LRBFGS}_k [⋅]$</span> is directly applied on <span>$\operatorname{grad}f(x_k)$</span>. When updating there are two cases: if there is still free memory, i.e. <span>$k &lt; m$</span>, the previously stored vector pairs <span>$\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m}^{k-1}$</span> have to be transported into the upcoming tangent space <span>$T_{x_{k+1}} \mathcal{M}$</span>; if there is no free memory, the oldest pair <span>$\{ \widetilde{s}_{k−m}, \widetilde{y}_{k−m}\}$</span> has to be discarded and then all the remaining vector pairs <span>$\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m+1}^{k-1}$</span> are transported into the tangent space <span>$T_{x_{k+1}} \mathcal{M}$</span>. After that we calculate and store <span>$s_k = \widetilde{s}_k = T^{S}_{x_k, α_k η_k}(α_k η_k)$</span> and <span>$y_k = \widetilde{y}_k$</span>. This process ensures that new information about the objective function is always included and the old, probably no longer relevant, information is discarded.</p><p><strong>Fields</strong></p><ul><li><code>memory_s</code> – the set of the stored (and transported) search directions times step size <span>$\{ \widetilde{s}_i\}_{i=k-m}^{k-1}$</span>.</li><li><code>memory_y</code> – set of the stored gradient differences <span>$\{ \widetilde{y}_i\}_{i=k-m}^{k-1}$</span>.</li><li><code>ξ</code> – a variable used in the two-loop recursion.</li><li><code>ρ</code> – a variable used in the two-loop recursion.</li><li><code>scale</code> –</li><li><code>vector_transport_method</code> – a <code>AbstractVectorTransportMethod</code></li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">QuasiNewtonLimitedMemoryDirectionUpdate(
    M::AbstractManifold,
    x,
    update::AbstractQuasiNewtonUpdateRule,
    memory_size;
    initial_vector=zero_vector(M,x),
    scale=1.0
    project=true
    )</code></pre><p><strong>See also</strong></p><p><a href="#Manopt.InverseBFGS"><code>InverseBFGS</code></a> <a href="#Manopt.QuasiNewtonCautiousDirectionUpdate"><code>QuasiNewtonCautiousDirectionUpdate</code></a> <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L474-L515">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.QuasiNewtonCautiousDirectionUpdate" href="#Manopt.QuasiNewtonCautiousDirectionUpdate"><code>Manopt.QuasiNewtonCautiousDirectionUpdate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuasiNewtonCautiousDirectionUpdate &lt;: AbstractQuasiNewtonDirectionUpdate</code></pre><p>These <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a>s represent any quasi-Newton update rule, which are based on the idea of a so-called cautious update. The search direction is calculated as given in <a href="#Manopt.QuasiNewtonMatrixDirectionUpdate"><code>QuasiNewtonMatrixDirectionUpdate</code></a> or [<code>LimitedMemoryQuasiNewctionDirectionUpdate</code>]. But the update given in <a href="#Manopt.QuasiNewtonMatrixDirectionUpdate"><code>QuasiNewtonMatrixDirectionUpdate</code></a> or [<code>LimitedMemoryQuasiNewctionDirectionUpdate</code>] is only executed if</p><p class="math-container">\[\frac{g_{x_{k+1}}(y_k,s_k)}{\lVert s_k \rVert^{2}_{x_{k+1}}} \geq \theta(\lVert \operatorname{grad}f(x_k) \rVert_{x_k}),\]</p><p>is satisfied, where <span>$\theta$</span> is a monotone increasing function satisfying <span>$\theta(0) = 0$</span> and <span>$\theta$</span> is strictly increasing at <span>$0$</span>. If this is not the case, the corresponding update will be skipped, which means that for <a href="#Manopt.QuasiNewtonMatrixDirectionUpdate"><code>QuasiNewtonMatrixDirectionUpdate</code></a> the matrix <span>$H_k$</span> or <span>$B_k$</span> is not updated, but the basis <span>$\{b_i\}^{n}_{i=1}$</span> is nevertheless transported into the upcoming tangent space <span>$T_{x_{k+1}} \mathcal{M}$</span>, and for [<code>LimitedMemoryQuasiNewctionDirectionUpdate</code>] neither the oldest vector pair <span>$\{ \widetilde{s}_{k−m}, \widetilde{y}_{k−m}\}$</span> is discarded nor the newest vector pair <span>$\{ \widetilde{s}_{k}, \widetilde{y}_{k}\}$</span> is added into storage, but all stored vector pairs <span>$\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m}^{k-1}$</span> are transported into the tangent space <span>$T_{x_{k+1}} \mathcal{M}$</span>. If <a href="#Manopt.InverseBFGS"><code>InverseBFGS</code></a> or <a href="#Manopt.InverseBFGS"><code>InverseBFGS</code></a> is chosen as update, then the resulting method follows the method of <sup class="footnote-reference"><a id="citeref-HuangAbsilGallivan2018" href="#footnote-HuangAbsilGallivan2018">[HuangAbsilGallivan2018]</a></sup>, taking into account that the corresponding step size is chosen.</p><p><strong>Fields</strong></p><ul><li><code>update</code> – an <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a></li><li><code>θ</code> – a monotone increasing function satisfying <span>$θ(0) = 0$</span> and <span>$θ$</span> is strictly increasing at <span>$0$</span>.</li></ul><p><strong>See also</strong></p><p><a href="#Manopt.QuasiNewtonMatrixDirectionUpdate"><code>QuasiNewtonMatrixDirectionUpdate</code></a> <a href="#Manopt.QuasiNewtonLimitedMemoryDirectionUpdate"><code>QuasiNewtonLimitedMemoryDirectionUpdate</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L564-L589">source</a></section></article><h2 id="Hessian-Update-Rules"><a class="docs-heading-anchor" href="#Hessian-Update-Rules">Hessian Update Rules</a><a id="Hessian-Update-Rules-1"></a><a class="docs-heading-anchor-permalink" href="#Hessian-Update-Rules" title="Permalink"></a></h2><p>Using</p><article class="docstring"><header><a class="docstring-binding" id="Manopt.update_hessian!" href="#Manopt.update_hessian!"><code>Manopt.update_hessian!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">update_hessian!(d, p, o, x_old, iter)</code></pre><p>update the hessian wihtin the <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a> <code>o</code> given a <a href="../../plans/problem/#Manopt.Problem"><code>Problem</code></a> <code>p</code> as well as the an <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> <code>d</code> and the last iterate <code>x_old</code>. Note that the current (<code>iter</code>th) iterate is already stored in <code>o.x</code>.</p><p>See also <a href="#Manopt.AbstractQuasiNewtonUpdateRule"><code>AbstractQuasiNewtonUpdateRule</code></a> for the different rules that are available within <code>d</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/solvers/quasi_Newton.jl#L182-L191">source</a></section></article><p>the following update formulae for either <span>$H_{k+1}$</span> or <span>$B_{k+1}$</span> are available.</p><article class="docstring"><header><a class="docstring-binding" id="Manopt.AbstractQuasiNewtonUpdateRule" href="#Manopt.AbstractQuasiNewtonUpdateRule"><code>Manopt.AbstractQuasiNewtonUpdateRule</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AbstractQuasiNewtonUpdateRule</code></pre><p>Specify a type for the different <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a>s.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L11-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.BFGS" href="#Manopt.BFGS"><code>Manopt.BFGS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BFGS &lt;: AbstractQuasiNewtonUpdateRule</code></pre><p>indicates in <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> that the Riemanian BFGS update is used in the Riemannian quasi-Newton method.</p><p>We denote by <span>$\widetilde{H}_k^\mathrm{BFGS}$</span> the operator concatenated with a vector transport and its inverse before and after to act on <span>$x_{k+1} = R_{x_k}(α_k η_k)$</span>. Then the update formula reads</p><p class="math-container">\[H^\mathrm{BFGS}_{k+1} = \widetilde{H}^\mathrm{BFGS}_k  + \frac{y_k y^{\mathrm{T}}_k }{s^{\mathrm{T}}_k y_k} - \frac{\widetilde{H}^\mathrm{BFGS}_k s_k s^{\mathrm{T}}_k \widetilde{H}^\mathrm{BFGS}_k }{s^{\mathrm{T}}_k \widetilde{H}^\mathrm{BFGS}_k s_k}\]</p><p>where <span>$s_k$</span> and <span>$y_k$</span> are the coordinate vectors with respect to the current basis (from <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a>) of</p><p class="math-container">\[T^{S}_{x_k, α_k η_k}(α_k η_k) \quad\text{and}\quad
\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, α_k η_k}(\operatorname{grad}f(x_k)) ∈ T_{x_{k+1}} \mathcal{M},\]</p><p>respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L18-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.DFP" href="#Manopt.DFP"><code>Manopt.DFP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DFP &lt;: AbstractQuasiNewtonUpdateRule</code></pre><p>indicates in an <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> that the Riemanian DFP update is used in the Riemannian quasi-Newton method.</p><p>We denote by <span>$\widetilde{H}_k^\mathrm{DFP}$</span> the operator concatenated with a vector transport and its inverse before and after to act on <span>$x_{k+1} = R_{x_k}(α_k η_k)$</span>. Then the update formula reads</p><p class="math-container">\[H^\mathrm{DFP}_{k+1} = \Bigl(
  \mathrm{id}_{T_{x_{k+1}} \mathcal{M}} - \frac{y_k s^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k}
\Bigr)
\widetilde{H}^\mathrm{DFP}_k
\Bigl(
  \mathrm{id}_{T_{x_{k+1}} \mathcal{M}} - \frac{s_k y^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k}
\Bigr) + \frac{y_k y^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k}\]</p><p>where <span>$s_k$</span> and <span>$y_k$</span> are the coordinate vectors with respect to the current basis (from <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a>) of</p><p class="math-container">\[T^{S}_{x_k, α_k η_k}(α_k η_k) \quad\text{and}\quad
\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, α_k η_k}(\operatorname{grad}f(x_k)) ∈ T_{x_{k+1}} \mathcal{M},\]</p><p>respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L70-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.Broyden" href="#Manopt.Broyden"><code>Manopt.Broyden</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Broyden &lt;: AbstractQuasiNewtonUpdateRule</code></pre><p>indicates in <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> that the Riemanian Broyden update is used in the Riemannian quasi-Newton method, which is as a convex combination of <a href="#Manopt.BFGS"><code>BFGS</code></a> and <a href="#Manopt.DFP"><code>DFP</code></a>.</p><p>We denote by <span>$\widetilde{H}_k^\mathrm{Br}$</span> the operator concatenated with a vector transport and its inverse before and after to act on <span>$x_{k+1} = R_{x_k}(α_k η_k)$</span>. Then the update formula reads</p><p class="math-container">\[H^\mathrm{Br}_{k+1} = \widetilde{H}^\mathrm{Br}_k
  - \frac{\widetilde{H}^\mathrm{Br}_k s_k s^{\mathrm{T}}_k \widetilde{H}^\mathrm{Br}_k}{s^{\mathrm{T}}_k \widetilde{H}^\mathrm{Br}_k s_k} + \frac{y_k y^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k}
  + φ_k s^{\mathrm{T}}_k \widetilde{H}^\mathrm{Br}_k s_k
  \Bigl(
        \frac{y_k}{s^{\mathrm{T}}_k y_k} - \frac{\widetilde{H}^\mathrm{Br}_k s_k}{s^{\mathrm{T}}_k \widetilde{H}^\mathrm{Br}_k s_k}
  \Bigr)
  \Bigl(
        \frac{y_k}{s^{\mathrm{T}}_k y_k} - \frac{\widetilde{H}^\mathrm{Br}_k s_k}{s^{\mathrm{T}}_k \widetilde{H}^\mathrm{Br}_k s_k}
  \Bigr)^{\mathrm{T}}\]</p><p>where <span>$s_k$</span> and <span>$y_k$</span> are the coordinate vectors with respect to the current basis (from <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a>) of</p><p class="math-container">\[T^{S}_{x_k, α_k η_k}(α_k η_k) \quad\text{and}\quad
\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, α_k η_k}(\operatorname{grad}f(x_k)) ∈ T_{x_{k+1}} \mathcal{M},\]</p><p>respectively, and <span>$φ_k$</span> is the Broyden factor which is <code>:constant</code> by default but can also be set to <code>:Davidon</code>.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">Broyden(φ, update_rule::Symbol = :constant)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L209-L240">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.SR1" href="#Manopt.SR1"><code>Manopt.SR1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SR1 &lt;: AbstractQuasiNewtonUpdateRule</code></pre><p>indicates in <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> that the Riemanian SR1 update is used in the Riemannian quasi-Newton method.</p><p>We denote by <span>$\widetilde{H}_k^\mathrm{SR1}$</span> the operator concatenated with a vector transport and its inverse before and after to act on <span>$x_{k+1} = R_{x_k}(α_k η_k)$</span>. Then the update formula reads</p><p class="math-container">\[H^\mathrm{SR1}_{k+1} = \widetilde{H}^\mathrm{SR1}_k
+ \frac{
  (y_k - \widetilde{H}^\mathrm{SR1}_k s_k) (y_k - \widetilde{H}^\mathrm{SR1}_k s_k)^{\mathrm{T}}
}{
(y_k - \widetilde{H}^\mathrm{SR1}_k s_k)^{\mathrm{T}} s_k
}\]</p><p>where <span>$s_k$</span> and <span>$y_k$</span> are the coordinate vectors with respect to the current basis (from <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a>) of</p><p class="math-container">\[T^{S}_{x_k, α_k η_k}(α_k η_k) \quad\text{and}\quad
\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, α_k η_k}(\operatorname{grad}f(x_k)) ∈ T_{x_{k+1}} \mathcal{M},\]</p><p>respectively.</p><p>This method can be stabilized by only performing the update if denominator is larger than <span>$r\lVert s_k\rVert_{x_{k+1}}\lVert y_k - \widetilde{H}^\mathrm{SR1}_k s_k \rVert_{x_{k+1}}$</span> for some <span>$r&gt;0$</span>. For more details, see Section 6.2 in <sup class="footnote-reference"><a id="citeref-NocedalWright2006" href="#footnote-NocedalWright2006">[NocedalWright2006]</a></sup></p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">SR1(r::Float64=-1.0)</code></pre><p>Generate the <code>SR1</code> update, which by default does not include the check (since the default sets <span>$t&lt;0$</span>`)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L124-L162">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.InverseBFGS" href="#Manopt.InverseBFGS"><code>Manopt.InverseBFGS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InverseBFGS &lt;: AbstractQuasiNewtonUpdateRule</code></pre><p>indicates in <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> that the inverse Riemanian BFGS update is used in the Riemannian quasi-Newton method.</p><p>We denote by <span>$\widetilde{B}_k^\mathrm{BFGS}$</span> the operator concatenated with a vector transport and its inverse before and after to act on <span>$x_{k+1} = R_{x_k}(α_k η_k)$</span>. Then the update formula reads</p><p class="math-container">\[B^\mathrm{BFGS}_{k+1}  = \Bigl(
  \mathrm{id}_{T_{x_{k+1}} \mathcal{M}} - \frac{s_k y^{\mathrm{T}}_k }{s^{\mathrm{T}}_k y_k}
\Bigr)
\widetilde{B}^\mathrm{BFGS}_k
\Bigl(
  \mathrm{id}_{T_{x_{k+1}} \mathcal{M}} - \frac{y_k s^{\mathrm{T}}_k }{s^{\mathrm{T}}_k y_k}
\Bigr) + \frac{s_k s^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k}\]</p><p>where <span>$s_k$</span> and <span>$y_k$</span> are the coordinate vectors with respect to the current basis (from <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a>) of</p><p class="math-container">\[T^{S}_{x_k, α_k η_k}(α_k η_k) \quad\text{and}\quad
\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, α_k η_k}(\operatorname{grad}f(x_k)) ∈ T_{x_{k+1}} \mathcal{M},\]</p><p>respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L41-L67">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.InverseDFP" href="#Manopt.InverseDFP"><code>Manopt.InverseDFP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InverseDFP &lt;: AbstractQuasiNewtonUpdateRule</code></pre><p>indicates in <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> that the inverse Riemanian DFP update is used in the Riemannian quasi-Newton method.</p><p>We denote by <span>$\widetilde{B}_k^\mathrm{DFP}$</span> the operator concatenated with a vector transport and its inverse before and after to act on <span>$x_{k+1} = R_{x_k}(α_k η_k)$</span>. Then the update formula reads</p><p class="math-container">\[B^\mathrm{DFP}_{k+1} = \widetilde{B}^\mathrm{DFP}_k
+ \frac{s_k s^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k}
- \frac{\widetilde{B}^\mathrm{DFP}_k y_k y^{\mathrm{T}}_k \widetilde{B}^\mathrm{DFP}_k}{y^{\mathrm{T}}_k \widetilde{B}^\mathrm{DFP}_k y_k}\]</p><p>where <span>$s_k$</span> and <span>$y_k$</span> are the coordinate vectors with respect to the current basis (from <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a>) of</p><p class="math-container">\[T^{S}_{x_k, α_k η_k}(α_k η_k) \quad\text{and}\quad
\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, α_k η_k}(\operatorname{grad}f(x_k)) ∈ T_{x_{k+1}} \mathcal{M},\]</p><p>respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L99-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.InverseBroyden" href="#Manopt.InverseBroyden"><code>Manopt.InverseBroyden</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InverseBroyden &lt;: AbstractQuasiNewtonUpdateRule</code></pre><p>Indicates in <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> that the Riemanian Broyden update is used in the Riemannian quasi-Newton method, which is as a convex combination of <a href="#Manopt.InverseBFGS"><code>InverseBFGS</code></a> and <a href="#Manopt.InverseDFP"><code>InverseDFP</code></a>.</p><p>We denote by <span>$\widetilde{H}_k^\mathrm{Br}$</span> the operator concatenated with a vector transport and its inverse before and after to act on <span>$x_{k+1} = R_{x_k}(α_k η_k)$</span>. Then the update formula reads</p><p class="math-container">\[B^\mathrm{Br}_{k+1} = \widetilde{B}^\mathrm{Br}_k
 - \frac{\widetilde{B}^\mathrm{Br}_k y_k y^{\mathrm{T}}_k \widetilde{B}^\mathrm{Br}_k}{y^{\mathrm{T}}_k \widetilde{B}^\mathrm{Br}_k y_k}
   + \frac{s_k s^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k}
 + φ_k y^{\mathrm{T}}_k \widetilde{B}^\mathrm{Br}_k y_k
 \Bigl(
     \frac{s_k}{s^{\mathrm{T}}_k y_k} - \frac{\widetilde{B}^\mathrm{Br}_k y_k}{y^{\mathrm{T}}_k \widetilde{B}^\mathrm{Br}_k y_k}
    \Bigr) \Bigl(
        \frac{s_k}{s^{\mathrm{T}}_k y_k} - \frac{\widetilde{B}^\mathrm{Br}_k y_k}{y^{\mathrm{T}}_k \widetilde{B}^\mathrm{Br}_k y_k}
 \Bigr)^{\mathrm{T}}\]</p><p>where <span>$s_k$</span> and <span>$y_k$</span> are the coordinate vectors with respect to the current basis (from <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a>) of</p><p class="math-container">\[T^{S}_{x_k, α_k η_k}(α_k η_k) \quad\text{and}\quad
\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, α_k η_k}(\operatorname{grad}f(x_k)) ∈ T_{x_{k+1}} \mathcal{M},\]</p><p>respectively, and <span>$φ_k$</span> is the Broyden factor which is <code>:constant</code> by default but can also be set to <code>:Davidon</code>.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">InverseBroyden(φ, update_rule::Symbol = :constant)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L247-L281">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.InverseSR1" href="#Manopt.InverseSR1"><code>Manopt.InverseSR1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InverseSR1 &lt;: AbstractQuasiNewtonUpdateRule</code></pre><p>indicates in <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> that the inverse Riemanian SR1 update is used in the Riemannian quasi-Newton method.</p><p>We denote by <span>$\widetilde{B}_k^\mathrm{SR1}$</span> the operator concatenated with a vector transport and its inverse before and after to act on <span>$x_{k+1} = R_{x_k}(α_k η_k)$</span>. Then the update formula reads</p><p class="math-container">\[B^\mathrm{SR1}_{k+1} = \widetilde{B}^\mathrm{SR1}_k
+ \frac{
  (s_k - \widetilde{B}^\mathrm{SR1}_k y_k) (s_k - \widetilde{B}^\mathrm{SR1}_k y_k)^{\mathrm{T}}
}{
  (s_k - \widetilde{B}^\mathrm{SR1}_k y_k)^{\mathrm{T}} y_k
}\]</p><p>where <span>$s_k$</span> and <span>$y_k$</span> are the coordinate vectors with respect to the current basis (from <a href="#Manopt.QuasiNewtonOptions"><code>QuasiNewtonOptions</code></a>) of</p><p class="math-container">\[T^{S}_{x_k, α_k η_k}(α_k η_k) \quad\text{and}\quad
\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, α_k η_k}(\operatorname{grad}f(x_k)) ∈ T_{x_{k+1}} \mathcal{M},\]</p><p>respectively.</p><p>This method can be stabilized by only performing the update if denominator is larger than <span>$r\lVert y_k\rVert_{x_{k+1}}\lVert s_k - \widetilde{H}^\mathrm{SR1}_k y_k \rVert_{x_{k+1}}$</span> for some <span>$r&gt;0$</span>. For more details, see Section 6.2 in <sup class="footnote-reference"><a id="citeref-NocedalWright2006" href="#footnote-NocedalWright2006">[NocedalWright2006]</a></sup>.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">InverseSR1(r::Float64=-1.0)</code></pre><p>Generate the <code>InverseSR1</code> update, which by default does not include the check, since the default sets <span>$t&lt;0$</span>`.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L168-L203">source</a></section></article><h2 id="Options"><a class="docs-heading-anchor" href="#Options">Options</a><a id="Options-1"></a><a class="docs-heading-anchor-permalink" href="#Options" title="Permalink"></a></h2><p>The quasi Newton algorithm is based on a <a href="../../plans/problem/#Manopt.GradientProblem"><code>GradientProblem</code></a>.</p><article class="docstring"><header><a class="docstring-binding" id="Manopt.QuasiNewtonOptions" href="#Manopt.QuasiNewtonOptions"><code>Manopt.QuasiNewtonOptions</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuasiNewtonOptions &lt;: Options</code></pre><p>These Quasi Newton <a href="../../plans/options/#Manopt.Options"><code>Options</code></a> represent any quasi-Newton based method and can be used with any update rule for the direction.</p><p><strong>Fields</strong></p><ul><li><code>x</code> – the current iterate, a point on a manifold</li><li><code>gradient</code> – the current gradient</li><li><code>sk</code> – the current step</li><li><code>yk</code> the current gradient difference</li><li><code>direction_update</code> - an <a href="#Manopt.AbstractQuasiNewtonDirectionUpdate"><code>AbstractQuasiNewtonDirectionUpdate</code></a> rule.</li><li><code>retraction_method</code> – an <code>AbstractRetractionMethod</code></li><li><code>stop</code> – a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a></li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">QuasiNewtonOptions(
    M::AbstractManifold,
    x;
    initial_vector=zero_vector(M,x),
    direction_update=InverseBFGS(),
    stopping_criterion=StopAfterIteration(1000) | StopWhenGradientNormLess(1e-6),
    retraction_method::RM=default_retraction_method(M),
    vector_transport_method::VTM=default_vector_transport_method(M),
    stepsize=WolfePowellLinesearch(M; retraction_method, vector_transport_method, linesearch_stopsize=1e-12,
)</code></pre><p><strong>See also</strong></p><p><a href="../../plans/problem/#Manopt.GradientProblem"><code>GradientProblem</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/quasi_newton_plan.jl#L288-L318">source</a></section></article><h2 id="Literature"><a class="docs-heading-anchor" href="#Literature">Literature</a><a id="Literature-1"></a><a class="docs-heading-anchor-permalink" href="#Literature" title="Permalink"></a></h2><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-HuangGallivanAbsil2015"><a class="tag is-link" href="#citeref-HuangGallivanAbsil2015">HuangGallivanAbsil2015</a><blockquote><p>Huang, Wen and Gallivan, K. A. and Absil, P.-A., A Broyden Class of Quasi-Newton Methods for Riemannian Optimization, SIAM J. Optim., 25 (2015), pp. 1660-1685. doi: <a href="https://doi.org/10.1137/140955483">10.1137/140955483</a></p></blockquote></li><li class="footnote" id="footnote-HuangAbsilGallivan2018"><a class="tag is-link" href="#citeref-HuangAbsilGallivan2018">HuangAbsilGallivan2018</a><blockquote><p>Huang, Wen and Absil, P.-A and Gallivan, Kyle, A Riemannian BFGS Method Without Differentiated Retraction for Nonconvex Optimization Problems, SIAM J. Optim., 28 (2018), pp. 470-495. doi: <a href="https://doi.org/10.1137/17M1127582">10.1137/17M1127582</a></p></blockquote></li><li class="footnote" id="footnote-NocedalWright2006"><a class="tag is-link" href="#citeref-NocedalWright2006">NocedalWright2006</a><blockquote><p>Nocedal, J., Wright, S.: Numerical Optimization, Second Edition, Springer, 2006. doi: <a href="https://doi.org/10.1007/978-0-387-40065-5">10.1007/978-0-387-40065-5</a></p></blockquote></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../primal_dual_semismooth_Newton/">« Primal-dual Riemannian semismooth Newton</a><a class="docs-footer-nextpage" href="../stochastic_gradient_descent/">Stochastic Gradient Descent »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.21 on <span class="colophon-date" title="Wednesday 13 July 2022 11:59">Wednesday 13 July 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
