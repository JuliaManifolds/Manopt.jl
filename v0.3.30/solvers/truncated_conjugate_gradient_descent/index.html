<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Steihaug-Toint TCG Method · Manopt.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../../tutorials/Optimize!/">Get started: Optimize!</a></li><li><a class="tocitem" href="../../tutorials/AutomaticDifferentiation/">Use AD in Manopt</a></li><li><a class="tocitem" href="../../tutorials/HowToRecord/">Record values</a></li><li><a class="tocitem" href="../../tutorials/GeodesicRegression/">Do Geodesic regression</a></li><li><a class="tocitem" href="../../tutorials/Bezier/">Use Bezier Curves</a></li><li><a class="tocitem" href="../../tutorials/SecondOrderDifference/">Compute a second order difference</a></li><li><a class="tocitem" href="../../tutorials/StochasticGradientDescent/">Do stochastic gradient descent</a></li><li><a class="tocitem" href="../../tutorials/Benchmark/">speed up! using <code>gradF!</code></a></li><li><a class="tocitem" href="../../tutorials/JacobiFields/">Illustrate Jacobi Fields</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../DouglasRachford/">Douglas–Rachford</a></li><li><a class="tocitem" href="../gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../NelderMead/">Nelder–Mead</a></li><li><a class="tocitem" href="../particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../subgradient/">Subgradient method</a></li><li class="is-active"><a class="tocitem" href>Steihaug-Toint TCG Method</a><ul class="internal"><li><a class="tocitem" href="#Initialization"><span>Initialization</span></a></li><li><a class="tocitem" href="#Iteration"><span>Iteration</span></a></li><li><a class="tocitem" href="#Result"><span>Result</span></a></li><li><a class="tocitem" href="#Remarks"><span>Remarks</span></a></li><li><a class="tocitem" href="#Interface"><span>Interface</span></a></li><li><a class="tocitem" href="#Options"><span>Options</span></a></li><li><a class="tocitem" href="#Additional-Stopping-Criteria"><span>Additional Stopping Criteria</span></a></li></ul></li><li><a class="tocitem" href="../trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/robustPCA/">Robust PCA</a></li><li><a class="tocitem" href="../../examples/smallestEigenvalue/">Rayleigh quotient</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/options/">Options</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../../functions/">Introduction</a></li><li><a class="tocitem" href="../../functions/bezier/">Bézier curves</a></li><li><a class="tocitem" href="../../functions/costs/">Cost functions</a></li><li><a class="tocitem" href="../../functions/differentials/">Differentials</a></li><li><a class="tocitem" href="../../functions/adjointdifferentials/">Adjoint Differentials</a></li><li><a class="tocitem" href="../../functions/gradients/">Gradients</a></li><li><a class="tocitem" href="../../functions/Jacobi_fields/">Jacobi Fields</a></li><li><a class="tocitem" href="../../functions/proximal_maps/">Proximal Maps</a></li><li><a class="tocitem" href="../../functions/manifold/">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/data/">Data</a></li><li><a class="tocitem" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../list/">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href>Steihaug-Toint TCG Method</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Steihaug-Toint TCG Method</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/solvers/truncated_conjugate_gradient_descent.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="tCG"><a class="docs-heading-anchor" href="#tCG">Steihaug-Toint Truncated Conjugate-Gradient Method</a><a id="tCG-1"></a><a class="docs-heading-anchor-permalink" href="#tCG" title="Permalink"></a></h1><p>The aim is to solve the trust-region subproblem</p><p class="math-container">\[\operatorname*{arg\,min}_{η  ∈  T_{x}\mathcal{M}} m_{x}(η) = F(x) +
⟨\operatorname{grad}F(x), η⟩_{x} + \frac{1}{2} ⟨
\operatorname{Hess}F(x)[η], η⟩_{x}\]</p><p class="math-container">\[\text{s.t.} \; ⟨η, η⟩_{x} \leq {Δ}^2\]</p><p>on a manifold by using the Steihaug-Toint truncated conjugate-gradient method. All terms involving the trust-region radius use an inner product w.r.t. the preconditioner; this is because the iterates grow in length w.r.t. the preconditioner, guaranteeing that we do not re-enter the trust-region.</p><h2 id="Initialization"><a class="docs-heading-anchor" href="#Initialization">Initialization</a><a id="Initialization-1"></a><a class="docs-heading-anchor-permalink" href="#Initialization" title="Permalink"></a></h2><p>Initialize <span>$η_0 = η$</span> if using randomized approach and <span>$η$</span> the zero tangent vector otherwise, <span>$r_0 = \operatorname{grad}F(x)$</span>, <span>$z_0 = \operatorname{P}(r_0)$</span>, <span>$δ_0 = z_0$</span> and <span>$k=0$</span></p><h2 id="Iteration"><a class="docs-heading-anchor" href="#Iteration">Iteration</a><a id="Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Iteration" title="Permalink"></a></h2><p>Repeat until a convergence criterion is reached</p><ol><li>Set <span>$κ = ⟨δ_k, \operatorname{Hess}F(x)[δ_k]⟩_x$</span>,  <span>$α =\frac{⟨r_k, z_k⟩_x}{κ}$</span> and  <span>$⟨η_k, η_k⟩_{x}^* = ⟨η_k, \operatorname{P}(η_k)⟩_x +  2α ⟨η_k, \operatorname{P}(δ_k)⟩_{x} +  {α}^2  ⟨δ_k, \operatorname{P}(δ_k)⟩_{x}$</span>.</li><li>If <span>$κ ≤ 0$</span> or <span>$⟨η_k, η_k⟩_x^* ≥ Δ^2$</span>  return <span>$η_{k+1} = η_k + τ δ_k$</span> and stop.</li><li>Set <span>$η_{k}^*= η_k + α δ_k$</span>, if  <span>$⟨η_k, η_k⟩_{x} + \frac{1}{2} ⟨η_k,  \operatorname{Hess}[F] (η_k)_{x}⟩_{x} ≤ ⟨η_k^*,  η_k^*⟩_{x} + \frac{1}{2} ⟨η_k^*,  \operatorname{Hess}[F] (η_k)_ {x}⟩_{x}$</span>  set <span>$η_{k+1} = η_k$</span> else set <span>$η_{k+1} = η_{k}^*$</span>.</li><li>Set <span>$r_{k+1} = r_k + α \operatorname{Hess}[F](δ_k)_x$</span>,   <span>$z_{k+1} = \operatorname{P}(r_{k+1})$</span>,   <span>$β = \frac{⟨r_{k+1},  z_{k+1}⟩_{x}}{⟨r_k, z_k ⟩_{x}}$</span> and <span>$δ_{k+1} = -z_{k+1} + β δ_k$</span>.</li><li>Set <span>$k=k+1$</span>.</li></ol><h2 id="Result"><a class="docs-heading-anchor" href="#Result">Result</a><a id="Result-1"></a><a class="docs-heading-anchor-permalink" href="#Result" title="Permalink"></a></h2><p>The result is given by the last computed <span>$η_k$</span>.</p><h2 id="Remarks"><a class="docs-heading-anchor" href="#Remarks">Remarks</a><a id="Remarks-1"></a><a class="docs-heading-anchor-permalink" href="#Remarks" title="Permalink"></a></h2><p>The <span>$\operatorname{P}(⋅)$</span> denotes the symmetric, positive deﬁnite preconditioner. It is required if a randomized approach is used i.e. using a random tangent vector <span>$η_0$</span> as initial vector. The idea behind it is to avoid saddle points. Preconditioning is simply a rescaling of the variables and thus a redeﬁnition of the shape of the trust region. Ideally <span>$\operatorname{P}(⋅)$</span> is a cheap, positive approximation of the inverse of the Hessian of <span>$F$</span> at <span>$x$</span>. On default, the preconditioner is just the identity.</p><p>To step number 2: Obtain <span>$τ$</span> from the positive root of <span>$\left\lVert η_k + τ δ_k \right\rVert_{\operatorname{P}, x} = Δ$</span> what becomes after the conversion of the equation to</p><p class="math-container">\[ τ = \frac{-⟨η_k, \operatorname{P}(δ_k)⟩_{x} +
 \sqrt{⟨η_k, \operatorname{P}(δ_k)⟩_{x}^{2} +
 ⟨δ_k, \operatorname{P}(δ_k)⟩_{x} ( Δ^2 -
 ⟨η_k, \operatorname{P}(η_k)⟩_{x})}}
 {⟨δ_k, \operatorname{P}(δ_k)⟩_{x}}.\]</p><p>It can occur that <span>$⟨δ_k, \operatorname{Hess}[F] (δ_k)_{x}⟩_{x} = κ ≤ 0$</span> at iteration <span>$k$</span>. In this case, the model is not strictly convex, and the stepsize <span>$α =\frac{⟨r_k, z_k⟩_{x}} {κ}$</span> computed in step 1. does not give a reduction in the modelfunction <span>$m_x(⋅)$</span>. Indeed, <span>$m_x(⋅)$</span> is unbounded from below along the line <span>$η_k + α δ_k$</span>. If our aim is to minimize the model within the trust-region, it makes far more sense to reduce <span>$m_x(⋅)$</span> along <span>$η_k + α δ_k$</span> as much as we can while staying within the trust-region, and this means moving to the trust-region boundary along this line. Thus when <span>$κ ≤ 0$</span> at iteration k, we replace <span>$α = \frac{⟨r_k, z_k⟩_{x}}{κ}$</span> with <span>$τ$</span> described as above. The other possibility is that <span>$η_{k+1}$</span> would lie outside the trust-region at iteration k (i.e. <span>$⟨η_k, η_k⟩_{x}^{* } ≥ {Δ}^2$</span> what can be identified with the norm of <span>$η_{k+1}$</span>). In particular, when <span>$\operatorname{Hess}[F] (⋅)_{x}$</span> is positive deﬁnite and <span>$η_{k+1}$</span> lies outside the trust region, the solution to the trust-region problem must lie on the trust-region boundary. Thus, there is no reason to continue with the conjugate gradient iteration, as it stands, as subsequent iterates will move further outside the trust-region boundary. A sensible strategy, just as in the case considered above, is to move to the trust-region boundary by ﬁnding <span>$τ$</span>.</p><h2 id="Interface"><a class="docs-heading-anchor" href="#Interface">Interface</a><a id="Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.truncated_conjugate_gradient_descent" href="#Manopt.truncated_conjugate_gradient_descent"><code>Manopt.truncated_conjugate_gradient_descent</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">truncated_conjugate_gradient_descent(M, F, gradF, x, η, HessF, trust_region_radius)</code></pre><p>solve the trust-region subproblem</p><p class="math-container">\[\operatorname*{arg\,min}_{η ∈ T_xM}
m_x(η) \quad\text{where}
m_x(η) = F(x) + ⟨\operatorname{grad}F(x),η⟩_x + \frac{1}{2}⟨\operatorname{Hess}F(x)[η],η⟩_x,\]</p><p class="math-container">\[\text{such that}\quad ⟨η,η⟩_x ≤ Δ^2\]</p><p>with the <a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>. For a description of the algorithm and theorems offering convergence guarantees, see the reference:</p><ul><li>P.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: <a href="https://doi.org/10.1007/s10208-005-0179-9">10.1007/s10208-005-0179-9</a></li><li>A. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: <a href="https://doi.org/10.1137/1.9780898719857">10.1137/1.9780898719857</a></li></ul><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>F</code> – a cost function <span>$F: \mathcal M → ℝ$</span> to minimize</li><li><code>gradF</code> – the gradient <span>$\operatorname{grad}F: \mathcal M → T\mathcal M$</span> of <code>F</code></li><li><code>x</code> – a point on the manifold <span>$x ∈ \mathcal M$</span></li><li><code>η</code> – an update tangential vector <span>$η ∈ T_x\mathcal M$</span></li><li><code>HessF</code> – the hessian <span>$\operatorname{Hess}F(x): T_x\mathcal M → T_x\mathcal M$</span>, <span>$X ↦ \operatoname{Hess}F(x)[X] = ∇_ξ\operatorname{grad}f(x)$</span></li></ul><p><strong>Optional</strong></p><ul><li><code>evaluation</code> – (<a href="../../plans/problem/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient and hessian work by  allocation (default) or <a href="../../plans/problem/#Manopt.MutatingEvaluation"><code>MutatingEvaluation</code></a> in place</li><li><code>preconditioner</code> – a preconditioner for the hessian H</li><li><code>θ</code> – (<code>1.0</code>) 1+θ is the superlinear convergence target rate. The algorithm will   terminate early if the residual was reduced by a power of 1+theta.</li><li><code>κ</code> – (<code>0.1</code>) the linear convergence target rate: algorithm will terminate   early if the residual was reduced by a factor of kappa.</li><li><code>randomize</code> – set to true if the trust-region solve is to be initiated with a   random tangent vector. If set to true, no preconditioner will be   used. This option is set to true in some scenarios to escape saddle   points, but is otherwise seldom activated.</li><li><code>trust_region_radius</code> – (<code>injectivity_radius(M)/4</code>) a trust-region radius</li><li><code>project_vector!</code> : (<code>copyto!</code>) specify a projection operation for tangent vectors   for numerical stability. A function <code>(M, Y, p, X) -&gt; ...</code> working in place of <code>Y</code>.   per default, no projection is perfomed, set it to <code>project!</code> to activate projection.</li><li><code>stopping_criterion</code> – (<a href="../../plans/stopping_criteria/#Manopt.StopWhenAny"><code>StopWhenAny</code></a>, <a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a>,   <a href="#Manopt.StopIfResidualIsReducedByFactor"><code>StopIfResidualIsReducedByFactor</code></a>, <a href="#Manopt.StopIfResidualIsReducedByPower"><code>StopIfResidualIsReducedByPower</code></a>,   <a href="#Manopt.StopWhenCurvatureIsNegative"><code>StopWhenCurvatureIsNegative</code></a>, <a href="#Manopt.StopWhenTrustRegionIsExceeded"><code>StopWhenTrustRegionIsExceeded</code></a> )   a functor inheriting from <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> indicating when to stop,   where for the default, the maximal number of iterations is set to the dimension of the   manifold, the power factor is <code>θ</code>, the reduction factor is <code>κ</code>.   .</li><li><code>return_options</code> – (<code>false</code>) – if activated, the extended result, i.e. the   complete <a href="../../plans/options/#Manopt.Options"><code>Options</code></a> re returned. This can be used to access recorded values.   If set to false (default) just the optimal value <code>x_opt</code> is returned</li></ul><p>and the ones that are passed to <a href="../../plans/options/#Manopt.decorate_options"><code>decorate_options</code></a> for decorators.</p><p><strong>Output</strong></p><ul><li><code>η</code> – an approximate solution of the trust-region subproblem in <span>$T_{x}\mathcal M$</span>.</li></ul><p>OR</p><ul><li><code>options</code> - the options returned by the solver (see <code>return_options</code>)</li></ul><p><strong>see also</strong></p><p><a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/solvers/truncated_conjugate_gradient_descent.jl#L1-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.truncated_conjugate_gradient_descent!" href="#Manopt.truncated_conjugate_gradient_descent!"><code>Manopt.truncated_conjugate_gradient_descent!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">truncated_conjugate_gradient_descent!(M, F, gradF, x, η, HessF, trust_region_radius; kwargs...)</code></pre><p>solve the trust-region subproblem in place of <code>x</code>.</p><p><strong>Input</strong></p><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>F</code> – a cost function <span>$F: \mathcal M → ℝ$</span> to minimize</li><li><code>gradF</code> – the gradient <span>$\operatorname{grad}F: \mathcal M → T\mathcal M$</span> of <code>F</code></li><li><code>HessF</code> – the hessian <span>$\operatorname{Hess}F(x): T_x\mathcal M → T_x\mathcal M$</span>, <span>$X ↦ \operatoname{Hess}F(x)[X] = ∇_ξ\operatorname{grad}f(x)$</span></li><li><code>x</code> – a point on the manifold <span>$x ∈ \mathcal M$</span></li><li><code>η</code> – an update tangential vector <span>$η ∈ T_x\mathcal M$</span></li></ul><p>For more details and all optional arguments, see <a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/solvers/truncated_conjugate_gradient_descent.jl#L83-L98">source</a></section></article><h2 id="Options"><a class="docs-heading-anchor" href="#Options">Options</a><a id="Options-1"></a><a class="docs-heading-anchor-permalink" href="#Options" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.TruncatedConjugateGradientOptions" href="#Manopt.TruncatedConjugateGradientOptions"><code>Manopt.TruncatedConjugateGradientOptions</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TruncatedConjugateGradientOptions &lt;: AbstractHessianOptions</code></pre><p>describe the Steihaug-Toint truncated conjugate-gradient method, with</p><p><strong>Fields</strong></p><p>a default value is given in brackets if a parameter can be left out in initialization.</p><ul><li><code>x</code> : a point, where the trust-region subproblem needs   to be solved</li><li><code>η</code> : a tangent vector (called update vector), which solves the   trust-region subproblem after successful calculation by the algorithm</li><li><code>stop</code> : a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a>.</li><li><code>gradient</code> : the gradient at the current iterate</li><li><code>δ</code> : search direction</li><li><code>trust_region_radius</code> : (<code>injectivity_radius(M)/4</code>) the trust-region radius</li><li><code>residual</code> : the gradient</li><li><code>randomize</code> : indicates if the trust-region solve and so the algorithm is to be       initiated with a random tangent vector. If set to true, no       preconditioner will be used. This option is set to true in some       scenarios to escape saddle points, but is otherwise seldom activated.</li><li><code>project_vector!</code> : (<code>copyto!</code>) specify a projection operation for tangent vectors   for numerical stability. A function <code>(M, Y, p, X) -&gt; ...</code> working in place of <code>Y</code>.   per default, no projection is perfomed, set it to <code>project!</code> to activate projection.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">TruncatedConjugateGradientOptions(M, p, x, η;
    trust_region_radius=injectivity_radius(M)/4,
    randomize=false,
    θ=1.0,
    κ=0.1,
    project_vector! = copyto!,
)

and a slightly involved `stopping_criterion`</code></pre><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/hessian_plan.jl#L46-L85">source</a></section></article><h2 id="Additional-Stopping-Criteria"><a class="docs-heading-anchor" href="#Additional-Stopping-Criteria">Additional Stopping Criteria</a><a id="Additional-Stopping-Criteria-1"></a><a class="docs-heading-anchor-permalink" href="#Additional-Stopping-Criteria" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopIfResidualIsReducedByPower" href="#Manopt.StopIfResidualIsReducedByPower"><code>Manopt.StopIfResidualIsReducedByPower</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopIfResidualIsReducedByPower &lt;: StoppingCriterion</code></pre><p>A functor for testing if the norm of residual at the current iterate is reduced by a power of 1+θ compared to the norm of the initial residual, i.e. <span>$\Vert r_k \Vert_x \leqq  \Vert r_0 \Vert_{x}^{1+\theta}$</span>. In this case the algorithm reached superlinear convergence.</p><p><strong>Fields</strong></p><ul><li><code>θ</code> – part of the reduction power</li><li><code>initialResidualNorm</code> - stores the norm of the residual at the initial vector   <span>$η$</span> of the Steihaug-Toint tcg mehtod <a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a></li><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="../../plans/stopping_criteria/#Manopt.get_reason-Tuple{Options}"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopIfResidualIsReducedByPower(iRN, θ)</code></pre><p>initialize the StopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN to the power of 1+θ.</p><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/hessian_plan.jl#L481-L506">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopIfResidualIsReducedByFactor" href="#Manopt.StopIfResidualIsReducedByFactor"><code>Manopt.StopIfResidualIsReducedByFactor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopIfResidualIsReducedByFactor &lt;: StoppingCriterion</code></pre><p>A functor for testing if the norm of residual at the current iterate is reduced by a factor compared to the norm of the initial residual, i.e. <span>$\Vert r_k \Vert_x \leqq κ \Vert r_0 \Vert_x$</span>. In this case the algorithm reached linear convergence.</p><p><strong>Fields</strong></p><ul><li><code>κ</code> – the reduction factor</li><li><code>initialResidualNorm</code> - stores the norm of the residual at the initial vector   <span>$η$</span> of the Steihaug-Toint tcg mehtod <a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a></li><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be reached, see <a href="../../plans/stopping_criteria/#Manopt.get_reason-Tuple{Options}"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopIfResidualIsReducedByFactor(iRN, κ)</code></pre><p>initialize the StopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN times κ.</p><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/hessian_plan.jl#L440-L465">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenTrustRegionIsExceeded" href="#Manopt.StopWhenTrustRegionIsExceeded"><code>Manopt.StopWhenTrustRegionIsExceeded</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopWhenTrustRegionIsExceeded &lt;: StoppingCriterion</code></pre><p>A functor for testing if the norm of the next iterate in the  Steihaug-Toint tcg mehtod is larger than the trust-region radius, i.e. <span>$\Vert η_{k}^{*} \Vert_x ≧ trust_region_radius$</span>. terminate the algorithm when the trust region has been left.</p><p><strong>Fields</strong></p><ul><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="../../plans/stopping_criteria/#Manopt.get_reason-Tuple{Options}"><code>get_reason</code></a>.</li><li><code>storage</code> – stores the necessary parameters <code>η, δ, residual</code> to check the   criterion.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopWhenTrustRegionIsExceeded([a])</code></pre><p>initialize the StopWhenTrustRegionIsExceeded functor to indicate to stop after the norm of the next iterate is greater than the trust-region radius using the <a href="../../plans/options/#Manopt.StoreOptionsAction"><code>StoreOptionsAction</code></a> <code>a</code>, which is initialized to store <code>:η, :δ, :residual</code> by default.</p><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/hessian_plan.jl#L532-L556">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenCurvatureIsNegative" href="#Manopt.StopWhenCurvatureIsNegative"><code>Manopt.StopWhenCurvatureIsNegative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopWhenCurvatureIsNegative &lt;: StoppingCriterion</code></pre><p>A functor for testing if the curvature of the model is negative, i.e. <span>$\langle \delta_k, \operatorname{Hess}[F](\delta_k)\rangle_x \leqq 0$</span>. In this case, the model is not strictly convex, and the stepsize as computed does not give a reduction of the model.</p><p><strong>Fields</strong></p><ul><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="../../plans/stopping_criteria/#Manopt.get_reason-Tuple{Options}"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopWhenCurvatureIsNegative()</code></pre><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/hessian_plan.jl#L571-L589">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenModelIncreased" href="#Manopt.StopWhenModelIncreased"><code>Manopt.StopWhenModelIncreased</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopWhenModelIncreased &lt;: StoppingCriterion</code></pre><p>A functor for testing if the curvature of the model value increased.</p><p><strong>Fields</strong></p><ul><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="../../plans/stopping_criteria/#Manopt.get_reason-Tuple{Options}"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopWhenModelIncreased()</code></pre><p><strong>See also</strong></p><p><a href="#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/hessian_plan.jl#L604-L619">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.update_stopping_criterion!-Tuple{StopIfResidualIsReducedByPower, Val{:ResidualPower}, Any}" href="#Manopt.update_stopping_criterion!-Tuple{StopIfResidualIsReducedByPower, Val{:ResidualPower}, Any}"><code>Manopt.update_stopping_criterion!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">update_stopping_criterion!(c::StopIfResidualIsReducedByPower, :ResidualPower, v)</code></pre><p>Update the residual Power <span>$θ$</span> time period after which an algorithm shall stop.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/f6849a63442135f9ea69180ecead3e332079c9a8/src/plans/hessian_plan.jl#L521-L525">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../subgradient/">« Subgradient method</a><a class="docs-footer-nextpage" href="../trust_regions/">Trust-Regions Solver »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.21 on <span class="colophon-date" title="Wednesday 13 July 2022 11:59">Wednesday 13 July 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
