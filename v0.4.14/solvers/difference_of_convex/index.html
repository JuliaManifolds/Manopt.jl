<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Difference of Convex · Manopt.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../../tutorials/Optimize!/">Get started: Optimize!</a></li><li><a class="tocitem" href="../../tutorials/InplaceGradient/">Speedup using Inplace computations</a></li><li><a class="tocitem" href="../../tutorials/AutomaticDifferentiation/">Use Automatic Differentiation</a></li><li><a class="tocitem" href="../../tutorials/HowToRecord/">Record values</a></li><li><a class="tocitem" href="../../tutorials/ConstrainedOptimization/">Do Contrained Optimization</a></li><li><a class="tocitem" href="../../tutorials/GeodesicRegression/">Do Geodesic Regression</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../augmented_Lagrangian_method/">Augmented Lagrangian Method</a></li><li><a class="tocitem" href="../ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../cyclic_proximal_point/">Cyclic Proximal Point</a></li><li class="is-active"><a class="tocitem" href>Difference of Convex</a><ul class="internal"><li><a class="tocitem" href="#DCASolver"><span>Difference of Convex Algorithm</span></a></li><li><a class="tocitem" href="#DCPPASolver"><span>Difference of Convex Proximal Point</span></a></li><li><a class="tocitem" href="#Manopt-Solver-States"><span>Manopt Solver States</span></a></li><li><a class="tocitem" href="#The-difference-of-convex-objective"><span>The difference of convex objective</span></a></li><li><a class="tocitem" href="#Further-helper-functions"><span>Further helper functions</span></a></li></ul></li><li><a class="tocitem" href="../DouglasRachford/">Douglas–Rachford</a></li><li><a class="tocitem" href="../exact_penalty_method/">Exact Penalty Method</a></li><li><a class="tocitem" href="../FrankWolfe/">Frank-Wolfe</a></li><li><a class="tocitem" href="../gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../LevenbergMarquardt/">Levenberg–Marquardt</a></li><li><a class="tocitem" href="../NelderMead/">Nelder–Mead</a></li><li><a class="tocitem" href="../particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/objective/">Objective</a></li><li><a class="tocitem" href="../../plans/state/">Solver State</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../../functions/">Introduction</a></li><li><a class="tocitem" href="../../functions/bezier/">Bézier curves</a></li><li><a class="tocitem" href="../../functions/costs/">Cost functions</a></li><li><a class="tocitem" href="../../functions/differentials/">Differentials</a></li><li><a class="tocitem" href="../../functions/adjointdifferentials/">Adjoint Differentials</a></li><li><a class="tocitem" href="../../functions/gradients/">Gradients</a></li><li><a class="tocitem" href="../../functions/proximal_maps/">Proximal Maps</a></li><li><a class="tocitem" href="../../functions/manifold/">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/data/">Data</a></li><li><a class="tocitem" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../extensions/">Extensions</a></li><li><a class="tocitem" href="../../list/">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href>Difference of Convex</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Difference of Convex</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/solvers/difference_of_convex.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="DifferenceOfConvexSolvers"><a class="docs-heading-anchor" href="#DifferenceOfConvexSolvers">Difference of Convex</a><a id="DifferenceOfConvexSolvers-1"></a><a class="docs-heading-anchor-permalink" href="#DifferenceOfConvexSolvers" title="Permalink"></a></h1><h2 id="DCASolver"><a class="docs-heading-anchor" href="#DCASolver">Difference of Convex Algorithm</a><a id="DCASolver-1"></a><a class="docs-heading-anchor-permalink" href="#DCASolver" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.difference_of_convex_algorithm" href="#Manopt.difference_of_convex_algorithm"><code>Manopt.difference_of_convex_algorithm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">difference_of_convex_algorithm(M, f, g, ∂h, p=rand(M); kwargs...)</code></pre><p>Compute the difference of convex algorithm<sup class="footnote-reference"><a id="citeref-FerreiraSantosSouza2021" href="#footnote-FerreiraSantosSouza2021">[FerreiraSantosSouza2021]</a></sup> to minimize</p><p class="math-container">\[    \operatorname*{arg\,min}_{p∈\mathcal M}\  g(p) - h(p)\]</p><p>where you need to provide <span>$f(p) = g(p) - h(p)$</span>, <span>$g$</span> and the subdifferential <span>$∂h$</span> of <span>$h$</span>.</p><p>This algorithm performs the following steps given a start point <code>p</code>= <span>$p^{(0)}$</span>. Then repeat for <span>$k=0,1,\ldots$</span></p><ol><li>Take <span>$X^{(k)}  ∈ ∂h(p^{(k)})$</span></li><li>Set the next iterate to the solution of the subproblem</li></ol><p class="math-container">\[  p^{(k+1)} \in \operatorname*{argmin}_{q\in \mathcal M} g(q) - ⟨X^{(k)}, \log_{p^{(k)}}q⟩\]</p><p>until the <code>stopping_criterion</code> is fulfilled.</p><p><strong>Optional parameters</strong></p><ul><li><code>evaluation</code>          – (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient works by allocation (default) form <code>grad_f(M, p)</code> or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> form <code>grad_f!(M, X, x)</code></li><li><code>gradient</code>            – (<code>nothing</code>) specify <span>$\operatorname{grad} f$</span>, for debug / analysis or enhancing <code>stopping_criterion=</code></li><li><code>grad_g</code>              – (<code>nothing</code>) specify the gradient of <code>g</code>. If specified, a subsolver is automatically set up.</li><li><code>initial_vector</code>      - (<code>zero_vector(M, p)</code>) initialise the inner tangent vecor to store the subgradient result.</li><li><code>stopping_criterion</code>  – (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(200) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenChangeLess"><code>StopWhenChangeLess</code></a><code>(1e-8)</code>) a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> for the algorithm – includes a <a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(1e-8)</code>, when a <code>gradient</code> is provided.</li></ul><p>While there are several parameters for a sub solver, the easiest is to provide the function <code>grad_g=</code>, such that together with the mandatory function <code>g</code> a default cost and gradient can be generated and passed to a default subsolver. Hence the easiest example call looks like</p><pre><code class="nohighlight hljs">difference_of_convex_algorithm(M, f, g, grad_h, p; grad_g=grad_g)</code></pre><p><strong>Optional parameters for the sub problem</strong></p><ul><li><code>sub_cost</code>              - (<a href="#Manopt.LinearizedDCCost"><code>LinearizedDCCost</code></a><code>(g, p, initial_vector)</code>) a cost to be used within the default <code>sub_problem</code> Use this if you have a more efficient version than the default that is built using <code>g</code> from above.</li><li><code>sub_grad</code>              - (<a href="#Manopt.LinearizedDCGrad"><code>LinearizedDCGrad</code></a><code>(grad_g, p, initial_vector; evaluation=evaluation)</code> gradient to be used within the default <code>sub_problem</code>. This is generated by default when <code>grad_g</code> is provided. You can specify your own by overwriting this keyword.</li><li><code>sub_hess</code>              – (a fininte difference approximation by default) specify a Hessian  of the subproblem, which the default solver, see <code>sub_state</code> needs</li><li><code>sub_kwargs</code>            - (<code>[]</code>) pass keyword arguments to the <code>sub_state</code>, in form of a <code>Dict(:kwname=&gt;value)</code>, unless you set the <code>sub_state</code> directly.</li><li><code>sub_objective</code>         - (a gradient or hessian objetive based on the last 3 keywords) provide the objective used within <code>sub_problem</code> (if that is not specified by the user)</li><li><code>sub_problem</code>           - (<a href="../../plans/problem/#Manopt.DefaultManoptProblem"><code>DefaultManoptProblem</code></a><code>(M, sub_objective)</code> specify a manopt problem for the sub-solver runs. You can also provide a function for a closed form solution. Then <code>evaluation=</code> is taken into account for the form of this function.</li><li><code>sub_state</code>             - (<a href="../trust_regions/#Manopt.TrustRegionsState"><code>TrustRegionsState</code></a> by default, requires <code>sub_hessian</code> to be provided; decorated with <code>sub_kwargs</code>). Choose the solver by specifying a solver state to solve the <code>sub_problem</code> if the <code>sub_problem</code> if a function (i.e. a closed form solution), this is set to <code>evaluation</code> and can be changed to the evalution type of the closed form solution accordingly.</li><li><code>sub_stopping_criterion</code> - (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(300) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenStepsizeLess"><code>StopWhenStepsizeLess</code></a><code>(1e-9) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(1e-9)</code>) a stopping crierion used withing the default <code>sub_state=</code></li><li><code>sub_stepsize</code>           - (<a href="../../plans/stepsize/#Manopt.ArmijoLinesearch"><code>ArmijoLinesearch</code></a><code>(M)</code>) specify a step size used within the <code>sub_state</code></li></ul><p>...all others are passed on to decorate the inner <a href="#Manopt.DifferenceOfConvexState"><code>DifferenceOfConvexState</code></a>.</p><p><strong>Output</strong></p><p>the obtained (approximate) minimizer <span>$p^*$</span>, see <a href="../#Manopt.get_solver_return"><code>get_solver_return</code></a> for details</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/solvers/difference_of_convex_algorithm.jl#L112-L186">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.difference_of_convex_algorithm!" href="#Manopt.difference_of_convex_algorithm!"><code>Manopt.difference_of_convex_algorithm!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">difference_of_convex_algorithm!(M, f, g, ∂h, p; kwargs...)</code></pre><p>Run the difference of convex algorithm and perform the steps in place of <code>p</code>. See <a href="#Manopt.difference_of_convex_algorithm"><code>difference_of_convex_algorithm</code></a> for more details.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/solvers/difference_of_convex_algorithm.jl#L192-L197">source</a></section></article><h2 id="DCPPASolver"><a class="docs-heading-anchor" href="#DCPPASolver">Difference of Convex Proximal Point</a><a id="DCPPASolver-1"></a><a class="docs-heading-anchor-permalink" href="#DCPPASolver" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.difference_of_convex_proximal_point" href="#Manopt.difference_of_convex_proximal_point"><code>Manopt.difference_of_convex_proximal_point</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">difference_of_convex_proximal_point(M, prox_g, grad_h, p; kwargs...)
difference_of_convex_proximal_point(M, grad_h, p; kwargs...)</code></pre><p>Compute the difference of convex proximal point algorithm <sup class="footnote-reference"><a id="citeref-SouzaOliveira2015" href="#footnote-SouzaOliveira2015">[SouzaOliveira2015]</a></sup> to minimize</p><p class="math-container">\[    \operatorname*{arg\,min}_{p∈\mathcal M} g(p) - h(p)\]</p><p>where you have to provide the (sub) gradient <span>$∂h$</span> of <span>$h$</span> and either</p><ul><li>the proximal map <span>$\operatorname{prox}_{\lambda g}$</span> of <code>g</code> as a function <code>prox_g(M, λ, p)</code> or  <code>prox_g(M, q, λ, p)</code></li><li>the functions <code>g</code> and <code>grad_g</code> to compute the proximal map using a sub solver</li><li>your own sub-solver, see optional keywods below</li></ul><p>This algorithm performs the following steps given a start point <code>p</code>= <span>$p^{(0)}$</span>. Then repeat for <span>$k=0,1,\ldots$</span></p><ol><li><span>$X^{(k)}  ∈ \operatorname{grad} h(p^{(k)})$</span></li><li><span>$q^{(k)} = \operatorname{retr}_{p^{(k)}}(λ_kX^{(k)})$</span></li><li><span>$r^{(k)} = \operatorname{prox}_{λ_kg}(q^{(k)})$</span></li><li><span>$X^{(k)} = \operatorname{retr}^{-1}_{p^{(k)}}(r^{(k)})$</span></li><li>Compute a stepsize <span>$s_k$</span> and</li><li>set <span>$p^{(k+1)} = \operatorname{retr}_{p^{(k)}}(s_kX^{(k)})$</span>.</li></ol><p>until the <code>stopping_criterion</code> is fulfilled. See <sup class="footnote-reference"><a id="citeref-AlmeidaNetoOliveiraSouza2020" href="#footnote-AlmeidaNetoOliveiraSouza2020">[AlmeidaNetoOliveiraSouza2020]</a></sup> for more details on the modified variant, where we slightly changed step 4-6, sine here we get the classical proximal point method for DC functions for <span>$s_k = 1$</span> and we can employ linesearches similar to other solvers.</p><p><strong>Optional parameters</strong></p><ul><li><code>λ</code>                         – ( <code>i -&gt; 1/2</code> ) a function returning the sequence of prox parameters λi</li><li><code>evaluation</code>                – (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient works by allocation (default) form <code>gradF(M, x)</code> or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> in place, i.e. is of the form <code>gradF!(M, X, x)</code>.</li><li><code>cost</code>                      - (<code>nothing</code>) provide the cost <code>f</code>, e.g. for debug reasonscost to be used within the default <code>sub_problem</code>. Use this if you have a more efficient version than using <code>g</code> from above.</li><li><code>gradient</code>                  – (<code>nothing</code>) specify <span>$\operatorname{grad} f$</span>, for debug / analysis  or enhancing the <code>stopping_criterion</code></li><li><code>g</code>                         – (<code>nothing</code>) specify the function <code>g</code>.</li><li><code>grad_g</code>                    – (<code>nothing</code>) specify the gradient of <code>g</code>. If both <code>g</code>and <code>grad_g</code> are specified, a subsolver is automatically set up.</li><li><code>inverse_retraction_method</code> - (<code>default_inverse_retraction_method(M)</code>) an inverse retraction method to use (see step 4).</li><li><code>retraction_method</code>         – (<code>default_retraction_method(M)</code>) a retraction to use (see step 2)</li><li><code>stepsize</code>                  – (<a href="../../plans/stepsize/#Manopt.ConstantStepsize"><code>ConstantStepsize</code></a><code>(M)</code>) specify a <a href="../../plans/stepsize/#Manopt.Stepsize"><code>Stepsize</code></a> to run the modified algorithm (experimental.) functor.</li><li><code>stopping_criterion</code> (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(200) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenChangeLess"><code>StopWhenChangeLess</code></a><code>(1e-8)</code>) a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> for the algorithm – includes a <a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(1e-8)</code>, when a <code>gradient</code> is provided.</li></ul><p>While there are several parameters for a sub solver, the easiest is to provide the function <code>g</code> and <code>grad_g</code>, such that together with the mandatory function <code>g</code> a default cost and gradient can be generated and passed to a default subsolver. Hence the easiest example call looks like</p><pre><code class="nohighlight hljs">difference_of_convex_proximal_point(M, grad_h, p0; g=g, grad_g=grad_g)</code></pre><p><strong>Optional parameters for the sub problem</strong></p><ul><li><code>sub_cost</code>               – (<a href="#Manopt.ProximalDCCost"><code>ProximalDCCost</code></a><code>(g, copy(M, p), λ(1))</code>) cost to be used within the default <code>sub_problem</code> that is initialized as soon as <code>g</code> is provided.</li><li><code>sub_grad</code>               – (<a href="#Manopt.ProximalDCGrad"><code>ProximalDCGrad</code></a><code>(grad_g, copy(M, p), λ(1); evaluation=evaluation)</code> gradient to be used within the default <code>sub_problem</code>, that is initialized as soon as <code>grad_g</code> is provided. This is generated by default when <code>grad_g</code> is provided. You can specify your own by overwriting this keyword.</li><li><code>sub_hess</code>               – (a fininte difference approximation by default) specify a Hessian of the subproblem, which the default solver, see <code>sub_state</code> needs</li><li><code>sub_kwargs</code>             – (<code>[]</code>) pass keyword arguments to the <code>sub_state</code>, in form of a <code>Dict(:kwname=&gt;value)</code>, unless you set the <code>sub_state</code> directly.</li><li><code>sub_objective</code>          – (a gradient or hessian objetive based on the last 3 keywords) provide the objective used within <code>sub_problem</code> (if that is not specified by the user)</li><li><code>sub_problem</code>            – (<a href="../../plans/problem/#Manopt.DefaultManoptProblem"><code>DefaultManoptProblem</code></a><code>(M, sub_objective)</code> specify a manopt problem for the sub-solver runs. You can also provide a function for a closed form solution. Then <code>evaluation=</code> is taken into account for the form of this function.</li><li><code>sub_state</code>              – (<a href="../trust_regions/#Manopt.TrustRegionsState"><code>TrustRegionsState</code></a> – requires the <code>sub_hessian to be provided,  decorated with</code>sub<em>kwargs<code>) choose the solver by specifying a solver state to solve the</code>sub</em>problem`</li><li><code>sub_stopping_criterion</code> - (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(300) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenStepsizeLess"><code>StopWhenStepsizeLess</code></a><code>(1e-9) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(1e-9)</code>) a stopping crierion used withing the default <code>sub_state=</code></li></ul><p>...all others are passed on to decorate the inner <a href="#Manopt.DifferenceOfConvexProximalState"><code>DifferenceOfConvexProximalState</code></a>.</p><p><strong>Output</strong></p><p>the obtained (approximate) minimizer <span>$p^*$</span>, see <a href="../#Manopt.get_solver_return"><code>get_solver_return</code></a> for details</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/solvers/difference-of-convex-proximal-point.jl#L120-L213">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.difference_of_convex_proximal_point!" href="#Manopt.difference_of_convex_proximal_point!"><code>Manopt.difference_of_convex_proximal_point!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">difference_of_convex_proximal_point!(M, prox_g, grad_h, p; cost=nothing, kwargs...)</code></pre><p>Compute the difference of convex algorithm to minimize</p><p class="math-container">\[    \operatorname*{arg\,min}_{p∈\mathcal M} g(p) - h(p)\]</p><p>where you have to provide the proximal map of <code>g</code> and the gradient of <code>h</code>.</p><p>The compuation is done inplace of <code>p</code>.</p><p>For all further details, especially the keyword arguments, see <a href="#Manopt.difference_of_convex_proximal_point"><code>difference_of_convex_proximal_point</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/solvers/difference-of-convex-proximal-point.jl#L224-L238">source</a></section></article><h2 id="Manopt-Solver-States"><a class="docs-heading-anchor" href="#Manopt-Solver-States">Manopt Solver States</a><a id="Manopt-Solver-States-1"></a><a class="docs-heading-anchor-permalink" href="#Manopt-Solver-States" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.DifferenceOfConvexState" href="#Manopt.DifferenceOfConvexState"><code>Manopt.DifferenceOfConvexState</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferenceOfConvexState{Pr,St,P,T,SC&lt;:StoppingCriterion} &lt;:
           AbstractManoptSolverState</code></pre><p>A struct to store the current state of the [<code>difference_of_convex_algorithm</code>])(@ref). It comes in two forms, depending on the realisation of the <code>subproblem</code>.</p><p><strong>Fields</strong></p><ul><li><code>p</code> – the current iterate, i.e. a point on the manifold</li><li><code>X</code> – the current subgradient, i.e. a tangent vector to <code>p</code>.</li><li><code>sub_problem</code> – problem for the subsolver</li><li><code>sub_state</code> – state of the subproblem</li><li><code>stop</code> – a functor inheriting from <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> indicating when to stop.</li></ul><p>For the sub task, we need a method to solve</p><p class="math-container">\[    \operatorname*{argmin}_{q∈\mathcal M}\ g(p) - ⟨X, \log_p q⟩\]</p><p>besides a problem and options, one can also provide a function and an <a href="../../plans/objective/#Manopt.AbstractEvaluationType"><code>AbstractEvaluationType</code></a>, respectively, to indicate a closed form solution for the sub task.</p><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">DifferenceOfConvexState(M, p, sub_problem, sub_state; kwargs...)
DifferenceOfConvexState(M, p, sub_solver; evaluation=InplaceEvaluation(), kwargs...)</code></pre><p>Generate the state either using a solver from Manopt, given by an <a href="../../plans/problem/#Manopt.AbstractManoptProblem"><code>AbstractManoptProblem</code></a> <code>sub_problem</code> and an <a href="../../plans/state/#Manopt.AbstractManoptSolverState"><code>AbstractManoptSolverState</code></a> <code>sub_state</code>, or a closed form solution <code>sub_solver</code> for the sub-problem, where by default its <a href="../../plans/objective/#Manopt.AbstractEvaluationType"><code>AbstractEvaluationType</code></a> <code>evaluation</code> is in-place, i.e. the function is of the form <code>(M, p, X) -&gt; q</code> or <code>(M, q, p, X) -&gt; q</code>, such that the current iterate <code>p</code> and the subgradient <code>X</code> of <code>h</code> can be passed to that function and the result if <code>q</code>.</p><p><strong>Further keyword Arguments</strong></p><ul><li><code>initial_vector=zero_vector</code> (<code>zero_vectoir(M,p)</code>) how to initialize the inner gradient tangent vector</li><li><code>stopping_criterion</code> – <a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(200)</code> a stopping criterion</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/solvers/difference_of_convex_algorithm.jl#L2-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.DifferenceOfConvexProximalState" href="#Manopt.DifferenceOfConvexProximalState"><code>Manopt.DifferenceOfConvexProximalState</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferenceOfConvexProximalState{Type} &lt;: Options</code></pre><p>A struct to store the current state of the algorithm as well as the form. It comes in two forms, depending on the realisation of the <code>subproblem</code>.</p><p><strong>Fields</strong></p><ul><li><code>inverse_retraction_method</code> – (<code>default_inverse_retraction_method(M)</code>) an inverse retraction method to use within Frank Wolfe.</li><li><code>retraction_method</code> – (<code>default_retraction_method(M)</code>) a type of retraction</li><li><code>p</code>, <code>q</code>, <code>r</code>  – the current iterate, the gradient step and the prox, respetively their type is set by intializing <code>p</code></li><li><code>stepsize</code> – (<a href="../../plans/stepsize/#Manopt.ConstantStepsize"><code>ConstantStepsize</code></a><code>(1.0)</code>) a <a href="../../plans/stepsize/#Manopt.Stepsize"><code>Stepsize</code></a> function to run the modified algorithm (experimental)</li><li><code>stop</code> – (<a href="../../plans/stopping_criteria/#Manopt.StopWhenChangeLess"><code>StopWhenChangeLess</code></a><code>(1e-8)</code>) a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a></li><li><code>X</code>, <code>Y</code> – (<code>zero_vector(M,p)</code>) the current gradient and descent direction, respectively their common type is set by the keyword <code>X</code></li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">DifferenceOfConvexProximalState(M, p; kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>X</code>, <code>retraction_method</code>, <code>inverse_retraction_method</code>, <code>stepsize</code> for the fields above</li><li><code>stoppping_criterion</code> for the <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/solvers/difference-of-convex-proximal-point.jl#L2-L20">source</a></section></article><h2 id="The-difference-of-convex-objective"><a class="docs-heading-anchor" href="#The-difference-of-convex-objective">The difference of convex objective</a><a id="The-difference-of-convex-objective-1"></a><a class="docs-heading-anchor-permalink" href="#The-difference-of-convex-objective" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.ManifoldDifferenceOfConvexObjective" href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>Manopt.ManifoldDifferenceOfConvexObjective</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ManifoldDifferenceOfConvexObjective{E} &lt;: AbstractManifoldCostObjective{E}</code></pre><p>Specify an objetive for a <a href="#Manopt.difference_of_convex_algorithm"><code>difference_of_convex_algorithm</code></a>.</p><p>The objective <span>$f: \mathcal M \to ℝ$</span> is given as</p><p class="math-container">\[    f(p) = g(p) - h(p)\]</p><p>where both <span>$g$</span> and <span>$h$</span> are convex, lsc. and proper. Furthermore we assume that the subdifferential <span>$∂h$</span> of <span>$h$</span> is given.</p><p><strong>Fields</strong></p><ul><li><code>cost</code> – an implementation of <span>$f(p) = g(p)-h(p)$</span> as a function <code>f(M,p)</code>.</li><li><code>∂h!!</code> – a deterministic version of <span>$∂h: \mathcal M → T\mathcal M$</span>, i.e. calling <code>∂h(M, p)</code> returns a subgradient of <span>$h$</span> at <code>p</code> and if there is more than one, it returns a deterministic choice.</li></ul><p>Note that the subdifferential might be given in two possible signatures</p><ul><li><code>∂h(M,p)</code> which does an <a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a></li><li><code>∂h!(M, X, p)</code> which does an <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> in place of <code>X</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/plans/difference_of_convex_plan.jl#L1-L25">source</a></section></article><p>as well as for the corresponding sub problem</p><article class="docstring"><header><a class="docstring-binding" id="Manopt.LinearizedDCCost" href="#Manopt.LinearizedDCCost"><code>Manopt.LinearizedDCCost</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LinearizedDCCost</code></pre><p>A functor <code>(M,q) → ℝ</code> to represent the inner problem of a <a href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>ManifoldDifferenceOfConvexObjective</code></a>, i.e. a cost function of the form</p><p class="math-container">\[    F_{p_k,X_k}(p) = g(p) - ⟨X_k, \log_{p_k}p⟩\]</p><p>for a point <code>p_k</code> and a tangent vector <code>X_k</code> at <code>p_k</code> (e.g. outer iterates) that are stored within this functor as well.</p><p><strong>Fields</strong></p><ul><li><code>g</code> a function</li><li><code>pk</code> a point on a manifold</li><li><code>Xk</code> a tangent vector at <code>pk</code></li></ul><p>Both interims values can be set using <code>set_manopt_parameter!(::LinearizedDCCost, ::Val{:p}, p)</code> and <code>set_manopt_parameter!(::LinearizedDCCost, ::Val{:X}, X)</code>, respectively.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">LinearizedDCCost(g, p, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/plans/difference_of_convex_plan.jl#L84-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.LinearizedDCGrad" href="#Manopt.LinearizedDCGrad"><code>Manopt.LinearizedDCGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LinearizedDCGrad</code></pre><p>A functor <code>(M,X,p) → ℝ</code> to represent the gradient of the inner problem of a <a href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>ManifoldDifferenceOfConvexObjective</code></a>, i.e. for a cost function of the form</p><p class="math-container">\[    F_{p_k,X_k}(p) = g(p) - ⟨X_k, \log_{p_k}p⟩\]</p><p>its gradient is given by using <span>$F=F_1(F_2(p))$</span>, where <span>$F_1(X) = ⟨X_k,X⟩$</span> and <span>$F_2(p) = \log_{p_k}p$</span> and the chain rule as well as the adjoint differential of the logarithmic map with respect to its argument for <span>$D^*F_2(p)$</span></p><p class="math-container">\[    \operatorname{grad} F(q) = \operatorname{grad} f(q) - DF_2^*(q)[X]\]</p><p>for a point <code>pk</code> and a tangent vector <code>Xk</code> at <code>pk</code> (the outer iterates) that are stored within this functor as well</p><p><strong>Fields</strong></p><ul><li><code>grad_g!!</code> the gradient of <span>$g$</span> (see also <a href="#Manopt.LinearizedDCCost"><code>LinearizedDCCost</code></a>)</li><li><code>pk</code> a point on a manifold</li><li><code>Xk</code> a tangent vector at <code>pk</code></li></ul><p>Both interims values can be set using <code>set_manopt_parameter!(::LinearizedDCGrad, ::Val{:p}, p)</code> and <code>set_manopt_parameter!(::LinearizedDCGrad, ::Val{:X}, X)</code>, respectively.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">LinearizedDCGrad(grad_g, p, X; evaluation=AllocatingEvaluation())</code></pre><p>Where you specify whether <code>grad_g</code> is <a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a> or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a>, while this function still provides <em>both</em> signatures.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/plans/difference_of_convex_plan.jl#L125-L159">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.ManifoldDifferenceOfConvexProximalObjective" href="#Manopt.ManifoldDifferenceOfConvexProximalObjective"><code>Manopt.ManifoldDifferenceOfConvexProximalObjective</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ManifoldDifferenceOfConvexProximalObjective{E} &lt;: Problem</code></pre><p>Specify an objective <a href="#Manopt.difference_of_convex_proximal_point"><code>difference_of_convex_proximal_point</code></a> algorithm. The problem is of the form</p><p class="math-container">\[    \operatorname*{argmin}_{p\in \mathcal M} g(p) - h(p)\]</p><p>where both <span>$g$</span> and <span>$h$</span> are convex, lsc. and proper.</p><p><strong>Fields</strong></p><ul><li><code>cost</code> – (<code>nothing</code>) implementation of <span>$f(p) = g(p)-h(p)$</span> (optional)</li><li><code>gradient</code> - the gradient of the cost</li><li><code>grad_h!!</code> – a function <span>$\operatorname{grad}h: \mathcal M → T\mathcal M$</span>,</li></ul><p>Note that both the gradients miht be given in two possible signatures as allocating or Inplace.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">ManifoldDifferenceOfConvexProximalObjective(gradh; cost=norhting, gradient=nothing)</code></pre><p>an note that neither cost nor gradient are required for the algorithm, just for eventual debug or stopping criteria.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/plans/difference_of_convex_plan.jl#L202-L228">source</a></section></article><p>as well as for the corresponding sub problems</p><article class="docstring"><header><a class="docstring-binding" id="Manopt.ProximalDCCost" href="#Manopt.ProximalDCCost"><code>Manopt.ProximalDCCost</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ProximalDCCost</code></pre><p>A functor <code>(M, p) → ℝ</code> to represent the inner cost function of a <a href="#Manopt.ManifoldDifferenceOfConvexProximalObjective"><code>ManifoldDifferenceOfConvexProximalObjective</code></a>, i.e. the cost function of the proximal map of <code>g</code>.</p><p class="math-container">\[    F_{p_k}(p) = \frac{1}{2λ}d_{\mathcal M}(p_k,p)^2 + g(p)\]</p><p>for a point <code>pk</code> and a proximal parameter <span>$λ$</span>.</p><p><strong>Fields</strong></p><ul><li><code>g</code>  - a function</li><li><code>pk</code> - a point on a manifold</li><li><code>λ</code>  - the prox parameter</li></ul><p>Both interims values can be set using <code>set_manopt_parameter!(::ProximalDCCost, ::Val{:p}, p)</code> and <code>set_manopt_parameter!(::ProximalDCCost, ::Val{:λ}, λ)</code>, respectively.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">ProximalDCCost(g, p, λ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/plans/difference_of_convex_plan.jl#L290-L315">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.ProximalDCGrad" href="#Manopt.ProximalDCGrad"><code>Manopt.ProximalDCGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ProximalDCGrad</code></pre><p>A functor <code>(M,X,p) → ℝ</code> to represent the gradient of the inner cost function of a <a href="#Manopt.ManifoldDifferenceOfConvexProximalObjective"><code>ManifoldDifferenceOfConvexProximalObjective</code></a>, i.e. the gradient function of the proximal map cost function of <code>g</code>, i.e. of</p><p class="math-container">\[    F_{p_k}(p) = \frac{1}{2λ}d_{\mathcal M}(p_k,p)^2 + g(p)\]</p><p>which reads</p><p class="math-container">\[    \operatorname{grad} F_{p_k}(p) = \operatorname{grad} g(p) - \frac{1}{λ}\log_p p_k\]</p><p>for a point <code>pk</code> and a proximal parameter <code>λ</code>.</p><p><strong>Fields</strong></p><ul><li><code>grad_g</code>  - a gradient function</li><li><code>pk</code> - a point on a manifold</li><li><code>λ</code>  - the prox parameter</li></ul><p>Both interims values can be set using <code>set_manopt_parameter!(::ProximalDCGrad, ::Val{:p}, p)</code> and <code>set_manopt_parameter!(::ProximalDCGrad, ::Val{:λ}, λ)</code>, respectively.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">ProximalDCGrad(grad_g, pk, λ; evaluation=AllocatingEvaluation())</code></pre><p>Where you specify whether <code>grad_g</code> is <a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a> or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a>, while this function still always provides <em>both</em> signatures.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/plans/difference_of_convex_plan.jl#L332-L366">source</a></section></article><h2 id="Further-helper-functions"><a class="docs-heading-anchor" href="#Further-helper-functions">Further helper functions</a><a id="Further-helper-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Further-helper-functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.get_subtrahend_gradient" href="#Manopt.get_subtrahend_gradient"><code>Manopt.get_subtrahend_gradient</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">X = get_subtrahend_gradient(amp, q)
get_subtrahend_gradient!(amp, X, q)</code></pre><p>Evaluate the (sub)gradient of the subtrahend <code>h</code> from within a <a href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>ManifoldDifferenceOfConvexObjective</code></a> <code>amp</code> at the point <code>q</code> (in place of <code>X</code>).</p><p>The evaluation is done in place of <code>X</code> for the <code>!</code>-variant. The <code>T=</code><a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a> problem might still allocate memory within. When the non-mutating variant is called with a <code>T=</code><a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> memory for the result is allocated.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/plans/difference_of_convex_plan.jl#L38-L49">source</a></section><section><div><pre><code class="nohighlight hljs">X = get_subtrahend_gradient(M::AbstractManifold, dcpo::ManifoldDifferenceOfConvexProximalObjective, p)
get_subtrahend_gradient!(M::AbstractManifold, X, dcpo::ManifoldDifferenceOfConvexProximalObjective, p)</code></pre><p>Evaluate the gradient of the subtrahend <span>$h$</span> from within a <a href="#Manopt.ManifoldDifferenceOfConvexProximalObjective"><code>ManifoldDifferenceOfConvexProximalObjective</code></a><code></code>P<code>at the point</code>p` (in place of X).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b6ba13f1ae31703c67ca7af1cef5b687fa5bfe2c/src/plans/difference_of_convex_plan.jl#L245-L251">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-FerreiraSantosSouza2021"><a class="tag is-link" href="#citeref-FerreiraSantosSouza2021">FerreiraSantosSouza2021</a><blockquote><p>Ferreira, O. P., Santos, E. M., Souza, J. C. O.: <em>The difference of convex algorithm on Riemannian manifolds</em>, 2021, arXiv: <a href="http://arxiv.org/abs/2112.05250">2112.05250</a>.</p></blockquote></li><li class="footnote" id="footnote-SouzaOliveira2015"><a class="tag is-link" href="#citeref-SouzaOliveira2015">SouzaOliveira2015</a><blockquote><p>de Oliveira Souza, J. C., Oliveira, P. R.: <em>A proximal point algorithm for {DC} fuctions on Hadamard manifolds</em>, Journal of Global Optimization (64), 4, 2015, pp. 797–810, doi: <a href="https://doi.org/10.1007/s10898-015-0282-7">10.1007/s10898-015-0282-7</a>.</p></blockquote></li><li class="footnote" id="footnote-AlmeidaNetoOliveiraSouza2020"><a class="tag is-link" href="#citeref-AlmeidaNetoOliveiraSouza2020">AlmeidaNetoOliveiraSouza2020</a><blockquote><p>Almeida, Y. T., de Cruz Neto, J. X. Oliveira, P. R., de Oliveira Souza, J. C.: <em>A modified proximal point method for DC functions on Hadamard manifolds</em>, Computational Optimization and Applications (76), 2020, pp. 649–673. doi: <a href="https://doi.org/10.1007/s10589-020-00173-3">10.1007/s10589-020-00173-3</a></p></blockquote></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../cyclic_proximal_point/">« Cyclic Proximal Point</a><a class="docs-footer-nextpage" href="../DouglasRachford/">Douglas–Rachford »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Monday 10 April 2023 06:49">Monday 10 April 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
