var documenterSearchIndex = {"docs":
[{"location":"solvers/cyclic_proximal_point.html#CPPSolver-1","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"The Cyclic Proximal Point (CPP) algorithm is a Proximal Problem.","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"It aims to minimize","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"F(x) = sum_i=1^c f_i(x)","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"assuming that the proximal maps operatornameprox_lambda f_i(x) are given in closed form or can be computed efficiently (at least approximately).","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"The algorithm then cycles through these proximal maps, where the type of cycle might differ and the proximal parameter lambda_k changes after each cycle k.","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"For a convergence result on Hadamard manifolds see [Bačák, 2014].","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"cyclic_proximal_point","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.cyclic_proximal_point","page":"Cyclic Proximal Point","title":"Manopt.cyclic_proximal_point","text":"cyclic_proximal_point(M, F, proxes, x)\n\nperform a cyclic proximal point algorithm.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\nproxes – an Array of proximal maps (Functions) (λ,x) -> y for the summands of F\nx – an initial value x  mathcal M\n\nOptional\n\nthe default values are given in brackets\n\nevaluationOrder – ( LinearEvalOrder ) – whether to use a randomly permuted sequence (FixedRandomEvalOrder), a per cycle permuted sequence (RandomEvalOrder) or the default linear one.\nλ – ( iter -> 1/iter ) a function returning the (square summable but not summable) sequence of λi\nstoppingCriterion – (StopWhenAny(StopAfterIteration(5000),StopWhenChangeLess(10.0^-8))) a StoppingCriterion.\nreturnOptions – (false) – if actiavated, the extended result, i.e. the complete Options are returned. This can be used to access recorded values. If set to false (default) just the optimal value xOpt if returned\n\nand the ones that are passed to decorate_options for decorators.\n\nOutput\n\nxOpt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see returnOptions)\n\n\n\n\n\n","category":"function"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"CyclicProximalPointOptions","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.CyclicProximalPointOptions","page":"Cyclic Proximal Point","title":"Manopt.CyclicProximalPointOptions","text":"CyclicProximalPointOptions <: Options\n\nstores options for the cyclic_proximal_point algorithm. These are the\n\nFields\n\nx0 – an point to start\nstoppingCriterion – a function @(iter,x,xnew,λ_k) based on the current   iter, x and xnew as well as the current value of λ.\nλ – (@(iter) -> 1/iter) a function for the values of λ_k per iteration/cycle\nevaluationOrder – (LinearEvalOrder()) how to cycle through the proximal maps.   Other values are RandomEvalOrder() that takes a new random order each   iteration, and FixedRandomEvalOrder() that fixes a random cycle for all iterations.\n\nSee also\n\ncyclic_proximal_point\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#Debug-Functions-1","page":"Cyclic Proximal Point","title":"Debug Functions","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"DebugProximalParameter","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.DebugProximalParameter","page":"Cyclic Proximal Point","title":"Manopt.DebugProximalParameter","text":"DebugProximalParameter <: DebugAction\n\nprint the current iterates proximal point algorithm parameter given by Optionss o.λ.\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#Record-Functions-1","page":"Cyclic Proximal Point","title":"Record Functions","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"RecordProximalParameter","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.RecordProximalParameter","page":"Cyclic Proximal Point","title":"Manopt.RecordProximalParameter","text":"RecordProximalParameter <: RecordAction\n\nrecoed the current iterates proximal point algorithm parameter given by in Optionss o.λ.\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#Literature-1","page":"Cyclic Proximal Point","title":"Literature","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"<ul>\n<li id=\"Bačák2014\">[<a>Bačák, 2014</a>]\n  Bačák, M: <emph>Computing Medians and Means in Hadamard Spaces.</emph>,\n  SIAM Journal on Optimization, Volume 24, Number 3, pp. 1542–1566,\n  doi: <a href=\"https://doi.org/10.1137/140953393\">10.1137/140953393</a>,\n  arxiv: <a href=\"https://arxiv.org/abs/1210.2145\">1210.2145</a>.\n  </li>\n</ul>","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/GradientOfSecondOrderDifference.jl\"","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#secondOrderDifferenceGrad-1","page":"Gradient of d_2","title":"Illustration of the Gradient of a Second Order Difference","text":"","category":"section"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"This example explains how to compute the gradient of the second order difference mid point model using adjoint_Jacobi_fields.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"This example also illustrates the PowerManifold manifold as well as ArmijoLinesearch.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"We first initialize the manifold","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"using Manopt, Manifolds","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"using Colors\nblack = RGBA{Float64}(colorant\"#000000\")\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\") # points\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\") # results\nTolVibrantCyan = RGBA{Float64}(colorant\"#33BBEE\") # vectors\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\") # geo\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"Assume we have two points xy on the equator of the Sphere mathcal M = mathbb S^2 and a point y near the north pole","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"M = Sphere(2)\np = [1., 0., 0.]\nq = [0., 1., 0.]\nc = mid_point(M,p,q)\nr = shortest_geodesic(M, [0., 0., 1.], c, 0.1)\n[c,r]","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"Now the second order absolute difference can be stated as (see [Bačák, Bergmann, Steidl, Weinmann, 2016])","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"d_2(xyz) = min_c  mathcal C_xz d_mathcal M(cy)qquad xyzmathcal M","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"where mathcal C_xz is the set of all mid points g(frac12xz), where g is a (not necessarily minimizing) geodesic connecting x and z.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"For illustration we further define the point opposite of","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"c2 = -c","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"and draw the geodesic connecting y and the nearest mid point c, namely","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"T = [0:0.1:1.0...]\ngeoPts_yc = shortest_geodesic(M,r,c,T)\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"looks as follows using the asymptote_export_S2_signals export","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"asymptote_export_S2_signals(\"secondOrderData.asy\";\n    render = asyResolution,\n    curves = [ geoPts_yc ],\n    points = [ [x,y,z], [c,c2] ],\n    colors=Dict(:curves => [TolVibrantTeal], :points => [black, TolVibrantBlue]),\n    dotSize = 3.5, lineWidth = 0.75, cameraPosition = (1.2,1.,.5)\n)\nrender_asymptote(\"SecondOrderData.asy\"; render=2)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"(Image: Three points $p,r,q$ and the midpoint $c=c(p,q)$ (blue))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"Since we moved r 10% along the geodesic from the north pole to c, the distance to c is frac9pi20approx 14137, and this is also what","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"costTV2(M, (p,r,q) )","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"returns, see costTV2 for reference. But also its gradient can be easily computed since it is just a distance with respect to y and a concatenation of a geodesic, where the start or end point is the argument, respectively, with a distance. Hence the adjoint differentials adjoint_differential_geodesic_startpoint and adjoint_differential_geodesic_endpoint can be employed, see ∇TV2 for details. we obtain","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"(Xp, Xr, Xq) = ∇TV2(M, (p,r,q) )","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"When we aim to minimize this, we look at the negative gradient, i.e. we can draw this as","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"asymptote_export_S2_signals(\"SecondOrderGradient.asy\";\n   points = [ [x,y,z], [c,c2] ],\n   colors=Dict(:tvectors => [TolVibrantCyan], :points => [black, TolVibrantBlue]),\n   dotSize = 3.5, lineWidth = 0.75, cameraPosition = (1.2,1.,.5)\n)\nrender_asymptote(\"SecondOrderGradient.asy\"; render=2)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"(Image: Three points $x,y,z$ and the negative gradient of the second order absolute difference)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"If we now perform a gradient step, we obtain the three points","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"pn, rn, qn = exp.(Ref(M), [p,r,q], [-Xp,-Xr,-Xq])","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"as well we the new mid point","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"cn = mid_point(M,pn,qn)\ngeoPts_yncn = shortest_geodesic(M,rn,cn,T)\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"and obtain the new situation","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"asymptote_export_S2_signals(\"SecondOrderMin1.asy\";\n    points = [ [x,y,z], [c,c2,cn], [xn,yn,zn] ],\n    curves = [ geoPts_yncn ] ,\n    tVectors = [Tuple.([ [p, -Xp], [r, Xr], [q, Xq] ])],\n    colors=Dict(:tvectors => [TolVibrantCyan],\n        :points => [black, TolVibrantBlue, TolVibrantOrange],\n        :curves => [TolVibrantTeal]\n    ),\n    dotSize = 3.5, lineWidth = 0.75, cameraPosition = (1.2,1.,.5)\n)\nrender_asymptote(\"SecondOrderMin1.asy\"; render=2)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"#md","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"(Image: A gradient Step)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"One can see, that this step slightly “overshoots”, i.e. r is now even below c. and the cost function is still at","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"costTV2(M, (pn, rn, qn) )","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"But we can also search for the best step size using ArmijoLinesearch on the PowerManifold manifold mathcal N = mathcal M^3 = (mathbb S^2)^3","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"x = [p,r,q]\nN = PowerManifold(M, NestedPowerRepresentation(),3)\ns = ArmijoLinesearch(1.0,exp,0.999,0.96)(N, x,\n    x -> costTV2(M, Tuple(x)),\n     [ ∇TV2(M, (p,r,q))... ]  # transform from tuple to PowTVector\n)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"and for the new points","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"pm, rm, qm = exp.(Ref(M), [p,r,q], s*[-Xp,-Xr,-Xq])\ncm = mid_point(M,pm,qm)\ngeoPts_xmzm = shortest_geodesic(M,pm,qm,T)\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"we obtain again with","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"asymptote_export_S2_signals(\"SecondOrderMin2.asy\";\n    points = [ [x,y,z], [c,c2,cm], [xm,ym,zm] ],\n    curves = [ geoPts_xmzm ] ,\n    tVectors = [Tuple.( [-ξx, -ξy, -ξz], [x, y, z] )],\n    colors=Dict(:tvectors => [TolVibrantCyan],\n                :points => [black, TolVibrantBlue, TolVibrantOrange],\n                :curves => [TolVibrantTeal]\n                ),\n    dotSize = 3.5, lineWidth = 0.75, cameraPosition = (1.2,1.,.5)\n)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"(Image: A gradient Step)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"Here, the cost function yields","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"costTV2( M, (pm, rm, qm) )","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"which is nearly zero, as one can also see, since the new center c and r are quite close.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#Literature-1","page":"Gradient of d_2","title":"Literature","text":"","category":"section"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"Gradient of d_2","title":"Gradient of d_2","text":"<ul>\n<li id=\"BačákBergmannSteidlWeinmann2016\">[<a>Bačák, Bergmann, Steidl, Weinmann, 2016</a>]\n  Bačák, M; Bergmann, R.; Steidl, G; Weinmann, A.: <emph>A second order nonsmooth\n  variational model for restoring manifold-valued images.</emph>,\n  SIAM Journal on Scientific Computations, Volume 38, Number 1, pp. A567–597,\n  doi: <a href=\"https://doi.org/10.1137/15M101988X\">10.1137/15M101988X</a></li>\n</ul>","category":"page"},{"location":"solvers/DouglasRachford.html#DRSolver-1","page":"Douglas–Rachford","title":"Douglas–Rachford Algorithm","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"The (Parallel) Douglas–Rachford ((P)DR) Algorithm was generalized to Hadamard manifolds in [Bergmann, Persch, Steidl, 2016].","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"The aim is to minimize the sum","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"F(x) = f(x) + g(x)","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"on a manifold, where the two summands have proximal maps operatornameprox_lambda f operatornameprox_lambda g that are easy to evaluate (maybe in closed form or not too costly to approximate). Further define the Reflection operator at the proximal map as","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"operatornamerefl_lambda f(x) = exp_operatornameprox_lambda f(x) bigl( -log_operatornameprox_lambda f(x) x bigr)","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":".","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Let alpha_k   01 with sum_k  mathbb N alpha_k(1-alpha_k) =   fty and lambda  0 which might depend on iteration k as well) be given.","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Then the (P)DRA algorithm for initial data x_0  mathcal H as","category":"page"},{"location":"solvers/DouglasRachford.html#Initialization-1","page":"Douglas–Rachford","title":"Initialization","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Initialize t_0 = x_0 and k=0","category":"page"},{"location":"solvers/DouglasRachford.html#Iteration-1","page":"Douglas–Rachford","title":"Iteration","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Repeat  until a convergence criterion is reached","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Compute s_k = operatornamerefl_lambda foperatornamerefl_lambda g(t_k)\nwithin that operation store x_k+1 = operatornameprox_lambda g(t_k) which is the prox the inner reflection reflects at.\nCompute t_k+1 = g(alpha_k t_k s_k)\nSet k = k+1","category":"page"},{"location":"solvers/DouglasRachford.html#Result-1","page":"Douglas–Rachford","title":"Result","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"The result is given by the last computed x_K.","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"For the parallel version, the first proximal map is a vectorial version, where in each component one prox is applied to the corresponding copy of t_k and the second proximal map corresponds to the indicator function of the set, where all copies are equal (in mathcal H^n, where n is the number of copies), leading to the second prox being the Riemannian mean.","category":"page"},{"location":"solvers/DouglasRachford.html#Interface-1","page":"Douglas–Rachford","title":"Interface","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"  DouglasRachford","category":"page"},{"location":"solvers/DouglasRachford.html#Manopt.DouglasRachford","page":"Douglas–Rachford","title":"Manopt.DouglasRachford","text":" DouglasRachford(M, F, proxMaps, x)\n\nComputes the Douglas-Rachford algorithm on the manifold mathcal M, initial data x_0 and the (two) proximal maps proxMaps.\n\nFor k2 proximal maps the problem is reformulated using the parallelDouglasRachford: a vectorial proximal map on the power manifold mathcal M^k and the proximal map of the set that identifies all entries again, i.e. the Karcher mean. This henve also boild down to two proximal maps, though each evauates proximal maps in parallel, i.e. component wise in a vector.\n\nInput\n\nM – a Riemannian Manifold mathcal M\nF – a cost function consisting of a sum of cost functions\nproxes – functions of the form (λ,x)->... performing a proximal map, where ⁠λ denotes the proximal parameter, for each of the summands of F.\nx0 – initial data x_0  mathcal M\n\nOptional values\n\nthe default parameter is given in brackets\n\nλ – ((iter) -> 1.0) function to provide the value for the proximal parameter during the calls\nα – ((iter) -> 0.9) relaxation of the step from old to new iterate, i.e. t_k+1 = g(α_k t_k s_k), where s_k is the result of the double reflection involved in the DR algorithm\nR – (reflect) method employed in the iteration to perform the reflection of x at the prox p.\nstoppingCriterion – (StopWhenAny(StopAfterIteration(200),StopWhenChangeLess(10.0^-5))) a StoppingCriterion.\nparallel – (false) clarify that we are doing a parallel DR, i.e. on a PowerManifold manifold with two proxes. This can be used to trigger parallel Douglas–Rachford if you enter with two proxes. Keep in mind, that a parallel Douglas–Rachford implicitly works on a PowerManifold manifold and its first argument is the result then (assuming all are equal after the second prox.\nreturnOptions – (false) – if actiavated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value xOpt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\nxOpt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see returnOptions)\n\n\n\n\n\n","category":"function"},{"location":"solvers/DouglasRachford.html#Options-1","page":"Douglas–Rachford","title":"Options","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"DouglasRachfordOptions","category":"page"},{"location":"solvers/DouglasRachford.html#Manopt.DouglasRachfordOptions","page":"Douglas–Rachford","title":"Manopt.DouglasRachfordOptions","text":"DouglasRachfordOptions <: Options\n\nStore all options required for the DouglasRachford algorithm,\n\nFields\n\nx - the current iterate (result) For the parallel Douglas-Rachford, this is not a value from the PowerManifold manifold but the mean.\ns – the last result of the double reflection at the proxes relaxed by α.\nλ – ((iter)->1.0) function to provide the value for the proximal parameter during the calls\nα – ((iter)->0.9) relaxation of the step from old to new iterate, i.e. x^(k+1) = g(α(k) x^(k) t^(k)), where t^(k) is the result of the double reflection involved in the DR algorithm\nR – (reflect) method employed in the iteration to perform the reflection of x at the prox p.\nstop – (StopAfterIteration(300)) a StoppingCriterion\nparallel – (false) inducate whether we are running a pallel Douglas-Rachford or not.\n\n\n\n\n\n","category":"type"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"For specific DebugActions and RecordActions see also Cyclic Proximal Point.","category":"page"},{"location":"solvers/DouglasRachford.html#Literature-1","page":"Douglas–Rachford","title":"Literature","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"<ul>\n<li id=\"BergmannPerschSteidl2016\">[<a>Bergmann, Persch, Steidl, 2016</a>]\n  Bergmann, R; Persch, J.; Steidl, G.: <emph>A Parallel Douglas–Rachford\n  Algorithm for Minimizing ROF-like Functionals on Images with Values in\n  Symmetric Hadamard Manifolds.</emph>\n  SIAM Journal on Imaging Sciences, Volume 9, Number 3, pp. 901–937, 2016.\n  doi: <a href=\"https://doi.org/10.1137/15M1052858\">10.1137/15M1052858</a>,\n  arXiv: <a href=\"https://arxiv.org/abs/1512.02814\">1512.02814</a>.\n</li>\n</ul>","category":"page"},{"location":"functions/jacobiFields.html#JacobiFieldFunctions-1","page":"JacobiFields","title":"Jacobi Fields","text":"","category":"section"},{"location":"functions/jacobiFields.html#","page":"JacobiFields","title":"JacobiFields","text":"A smooth tangent vector field Jcolon 01 to Tmathcal M along a geodesic g(cdotxy) is called Jacobi field if it fulfills the ODE","category":"page"},{"location":"functions/jacobiFields.html#","page":"JacobiFields","title":"JacobiFields","text":"displaystyle 0 = fracDdtJ + R(Jdot g)dot g","category":"page"},{"location":"functions/jacobiFields.html#","page":"JacobiFields","title":"JacobiFields","text":"where R is the Riemannian curvature tensor. Such Jacobi fields can be used to derive closed forms for the exponential map, the logarithmic map and the geodesic, all of them with respect to both arguments: Let Fcolonmathcal N to mathcal M be given (for the exp_xcdot   we have mathcal N = T_xmathcal M, otherwise mathcal N=mathcal M) and denote by Xi_1ldotsXi_d an orthonormal frame along g(cdotxy) that diagonalizes the curvature tensor with corresponding eigenvalues kappa_1ldotskappa_d. Note that on symmetric manifolds such a frame always exists.","category":"page"},{"location":"functions/jacobiFields.html#","page":"JacobiFields","title":"JacobiFields","text":"Then DF(x)eta = sum_k=1^d langle etaXi_k(0)rangle_xbeta(kappa_k)Xi_k(T) holds, where T also depends on the function F as the weights beta. The values stem from solving the corresponding system of (decoupled) ODEs.","category":"page"},{"location":"functions/jacobiFields.html#","page":"JacobiFields","title":"JacobiFields","text":"Note that in different references some factors might be a little different, for example when using unit speed geodesics.","category":"page"},{"location":"functions/jacobiFields.html#","page":"JacobiFields","title":"JacobiFields","text":"The following weights functions are available","category":"page"},{"location":"functions/jacobiFields.html#","page":"JacobiFields","title":"JacobiFields","text":"Modules = [Manopt]\nPages   = [\"jacobiFields.jl\"]","category":"page"},{"location":"functions/jacobiFields.html#Manopt.adjoint_Jacobi_field","page":"JacobiFields","title":"Manopt.adjoint_Jacobi_field","text":"Y = adjoint_Jacobi_field(M, p, q, t, X, β)\n\nCompute the AdjointJacobiField J along the geodesic γ_pq on the manifold mathcal M with initial conditions (depending on the application) X  T_γ_pq(t)mathcal M and weights β. The result is a vector Y  T_pmathcal M. The main difference to jacobi_field is the, that the input X and the output Y switched tangent spaces. For detais see jacobi_field\n\n\n\n\n\n","category":"function"},{"location":"functions/jacobiFields.html#Manopt.jacobi_field","page":"JacobiFields","title":"Manopt.jacobi_field","text":"Y = jacobi_field(M, p, q, t, X, β)\n\ncompute the Jacobi jield J along the geodesic γ_pq on the manifold mathcal M with initial conditions (depending on the application) X  T_pmathcal M and weights β. The result is a tangent vector Y from T_γ_pq(t)mathcal M.\n\nSee also\n\nadjoint_Jacobi_field\n\n\n\n","category":"function"},{"location":"functions/jacobiFields.html#Manopt.βdifferential_exp_argument-Tuple{Any,Any,Any}","page":"JacobiFields","title":"Manopt.βdifferential_exp_argument","text":"βdifferential_exp_argument(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_X exp_p XY. They are\n\nbeta(kappa) = begincases\nfracsinh(dsqrt-kappa)dsqrt-kappatext if kappa  0\n1  text if  kappa = 0\nfracsin(dsqrtkappa)sqrtdkappatext if kappa  0\nendcases\n\nSee also\n\ndifferential_exp_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/jacobiFields.html#Manopt.βdifferential_exp_basepoint-Tuple{Any,Any,Any}","page":"JacobiFields","title":"Manopt.βdifferential_exp_basepoint","text":"βdifferential_exp_basepoint(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_p exp_p X Y. They are\n\nbeta(kappa) = begincases\ncosh(sqrt-kappa)text if kappa  0\n1  text if  kappa = 0\ncos(sqrtkappa) text if kappa  0\nendcases\n\nSee also\n\ndifferential_exp_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/jacobiFields.html#Manopt.βdifferential_geodesic_startpoint-Tuple{Any,Any,Any}","page":"JacobiFields","title":"Manopt.βdifferential_geodesic_startpoint","text":"βdifferential_geodesic_startpoint(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_x g(tpq)X. They are\n\nbeta(kappa) = begincases\nfracsinh(d(1-t)sqrt-kappa)sinh(dsqrt-kappa)\ntext if kappa  0\n1-t  text if  kappa = 0\nfracsin((1-t)dsqrtkappa)sinh(dsqrtkappa)\ntext if kappa  0\nendcases\n\nDue to a symmetry agrument, these are also used to compute D_q g(t pq)eta\n\nSee also\n\ndifferential_geodesic_endpoint, differential_geodesic_startpoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/jacobiFields.html#Manopt.βdifferential_log_argument-Tuple{Number,Number,Number}","page":"JacobiFields","title":"Manopt.βdifferential_log_argument","text":"βdifferential_log_argument(κ,t,d)\n\nweights for the JacobiField corresponding to the differential of the logarithmic map with respect to its argument D_q log_p qX. They are\n\nbeta(kappa) = begincases\nfrac dsqrt-kappa sinh(dsqrt-kappa)text if kappa  0\n1  text if  kappa = 0\nfrac dsqrtkappa sin(dsqrtkappa)text if kappa  0\nendcases\n\nSee also\n\ndifferential_log_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/jacobiFields.html#Manopt.βdifferential_log_basepoint-Tuple{Number,Number,Number}","page":"JacobiFields","title":"Manopt.βdifferential_log_basepoint","text":"βdifferential_log_basepoint(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_p log_p qX. They are\n\nbeta(kappa) = begincases\n-sqrt-kappadfraccosh(dsqrt-kappa)sinh(dsqrt-kappa)text if kappa  0\n-1  text if  kappa = 0\n-sqrtkappadfraccos(dsqrtkappa)sin(dsqrtkappa)text if kappa  0\nendcases\n\nSee also\n\ndifferential_log_argument, differential_log_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"solvers/truncatedConjugateGradient.html#tCG-1","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint Truncated Conjugate-Gradient Method","text":"","category":"section"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"The aim is to solve the trust-region subproblem","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"operatorname*argmin_eta    T_xmathcalM m_x(eta) = F(x) +\nlangle nabla F(x) eta rangle_x + frac12 langle\noperatornameHessF(eta)_ x eta rangle_x","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"textst  langle eta eta rangle_x leq Delta^2","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"on a manifold by using the Steihaug-Toint truncated conjugate-gradient method. All terms involving the trust-region radius use an inner product w.r.t. the preconditioner; this is because the iterates grow in length w.r.t. the preconditioner, guaranteeing that we do not re-enter the trust-region.","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Initialization-1","page":"Steihaug-Toint TCG Method","title":"Initialization","text":"","category":"section"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"Initialize eta_0 = eta if using randomized approach and eta the zero tangent vector otherwise, r_0 = nabla F(x), z_0 = operatornameP(r_0), delta_0 = z_0 and k=0","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Iteration-1","page":"Steihaug-Toint TCG Method","title":"Iteration","text":"","category":"section"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"Repeat until a convergence criterion is reached","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"Set kappa = langle delta_k operatornameHessF (delta_k)_ x rangle_x,  alpha =fraclangle r_k z_k rangle_xkappa and  langle eta_k eta_k rangle_x^*  = langle eta_k operatornameP(eta_k) rangle_x +  2alpha langle eta_k operatornameP(delta_k) rangle_x +  alpha^2  langle delta_k operatornameP(delta_k) rangle_x.\nIf kappa leqq 0 or langle eta_k eta_k rangle_x^*  geqq Delta^2  return eta_k+1 = eta_k + tau delta_k and stop.\nSet eta_k^* = eta_k + alpha delta_k, if  langle eta_k eta_k rangle_x + frac12 langle eta_k  operatornameHessF (eta_k)_ x rangle_x leqq langle eta_k^*   eta_k^*  rangle_x + frac12 langle eta_k^*   operatornameHessF (eta_k)_ x rangle_x  set eta_k+1 = eta_k else set eta_k+1 = eta_k^* .\nSet r_k+1 = r_k + alpha operatornameHessF (delta_k)_ x,   z_k+1 = operatornameP(r_k+1),  beta = fraclangle r_k+1  z_k+1 rangle_xlangle r_k z_k  rangle_x and delta_k+1 = -z_k+1 + beta delta_k.\nSet k=k+1.","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Result-1","page":"Steihaug-Toint TCG Method","title":"Result","text":"","category":"section"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"The result is given by the last computed η_k.","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Remarks-1","page":"Steihaug-Toint TCG Method","title":"Remarks","text":"","category":"section"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"The operatornameP(cdot) denotes the symmetric, positive deﬁnite preconditioner. It is required if a randomized approach is used i.e. using a random tangent vector eta as initial vector. The idea behind it is to avoid saddle points. Preconditioning is simply a rescaling of the variables and thus a redeﬁnition of the shape of the trust region. Ideally operatornameP(cdot) is a cheap, positive approximation of the inverse of the Hessian of F at x. On default, the preconditioner is just the identity.","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"To step number 2: Obtain tau from the positive root of leftlVert eta_k + tau delta_k rightrVert_operatornameP x = Delta what becomes after the conversion of the equation to","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":" tau = frac-langle eta_k operatornameP(delta_k) rangle_x +\n sqrtlangle eta_k operatornameP(delta_k) rangle_x^2 +\n langle delta_k operatornameP(delta_k) rangle_x ( Delta^2 -\n langle eta_k operatornameP(eta_k) rangle_x)\n langle delta_k operatornameP(delta_k) rangle_x","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"It can occur that langle delta_k operatornameHessF (delta_k)_ x rangle_x = kappa leqq 0 at iteration k. In this case, the model is not strictly convex, and the stepsize alpha =fraclangle r_k z_k rangle_x kappa computed in step 1. does not give a reduction in the modelfunction m_x(cdot). Indeed, m_x(cdot) is unbounded from below along the line eta_k + alpha delta_k. If our aim is to minimize the model within the trust-region, it makes far more sense to reduce m_x(cdot) along eta_k + alpha delta_k as much as we can while staying within the trust-region, and this means moving to the trust-region boundary along this line. Thus when kappa leqq 0 at iteration k, we replace alpha = fraclangle r_k z_k rangle_xkappa with tau described as above. The other possibility is that eta_k+1 would lie outside the trust-region at iteration k (i.e. langle eta_k eta_k rangle_x^*  geqq Delta^2 what can be identified with the norm of eta_k+1). In particular, when operatornameHessF (cdot)_ x is positive deﬁnite and eta_k+1 lies outside the trust region, the solution to the trust-region problem must lie on the trust-region boundary. Thus, there is no reason to continue with the conjugate gradient iteration, as it stands, as subsequent iterates will move further outside the trust-region boundary. A sensible strategy, just as in the case considered above, is to move to the trust-region boundary by ﬁnding tau.","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Interface-1","page":"Steihaug-Toint TCG Method","title":"Interface","text":"","category":"section"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"  truncatedConjugateGradient","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Manopt.truncatedConjugateGradient","page":"Steihaug-Toint TCG Method","title":"Manopt.truncatedConjugateGradient","text":"truncatedConjugateGradient(M, F, ∇F, x, η, H, Δ)\n\nsolve the trust-region subproblem\n\noperatorname*argmin_eta    T_xM m_x(eta) = F(x) + langle nabla F(x) eta rangle_x + frac12 langle operatornameHessF(eta)_ x eta rangle_x\n\ntextst  langle eta eta rangle_x leqq Delta^2\n\nwith the Steihaug-Toint truncated conjugate-gradient method. For a description of the algorithm and theorems offering convergence guarantees, see the reference:\n\nP.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: 10.1007/s10208-005-0179-9\nA. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: 10.1137/1.9780898719857\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n∇F – the gradient nabla Fcolonmathcal Mto Tmathcal M of F\nx – a point on the manifold x  mathcal M\nη – an update tangential vector eta  mathcalT_xM\nH – the hessian H( mathcal M x xi) of F\nΔ – a trust-region radius\n\nOptional\n\npreconditioner – a preconditioner for the hessian H\nθ – 1+θ is the superlinear convergence target rate. The algorithm will   terminate early if the residual was reduced by a power of 1+theta.\nκ – the linear convergence target rate: algorithm will terminate   early if the residual was reduced by a factor of kappa.\nuseRandom – set to true if the trust-region solve is to be initiated with a   random tangent vector. If set to true, no preconditioner will be   used. This option is set to true in some scenarios to escape saddle   points, but is otherwise seldom activated.\nstoppingCriterion – (StopWhenAny, StopAfterIteration,   stopIfResidualIsReducedByFactor, stopIfResidualIsReducedByPower,   StopWhenCurvatureIsNegative, StopWhenTrustRegionIsExceeded )   a functor inheriting from StoppingCriterion indicating when to stop,   where for the default, the maximal number of iterations is set to the dimension of the   manifold, the power factor is θ, the reduction factor is κ.   .\nreturnOptions – (false) – if actiavated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value xOpt is returned\n\nand the ones that are passed to decorate_options for decorators.\n\nOutput\n\nη – an approximate solution of the trust-region subproblem in   mathcalT_xM.\n\nOR\n\noptions - the options returned by the solver (see returnOptions)\n\nsee also\n\ntrust_regions\n\n\n\n\n\n","category":"function"},{"location":"solvers/truncatedConjugateGradient.html#Options-1","page":"Steihaug-Toint TCG Method","title":"Options","text":"","category":"section"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"TruncatedConjugateGradientOptions","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Manopt.TruncatedConjugateGradientOptions","page":"Steihaug-Toint TCG Method","title":"Manopt.TruncatedConjugateGradientOptions","text":"TruncatedConjugateGradientOptions <: HessianOptions\n\ndescribe the Steihaug-Toint truncated conjugate-gradient method, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx : a point, where the trust-region subproblem needs   to be solved\nstop : a function s,r = @(o,iter,ξ,x,xnew) returning a stop   indicator and a reason based on an iteration number, the gradient and the   last and current iterates\nη : a tangent vector (called update vector), which solves the   trust-region subproblem after successful calculation by the algorithm\nδ : search direction\nΔ : the trust-region radius\nresidual : the gradient\nuseRand : indicates if the trust-region solve and so the algorithm is to be       initiated with a random tangent vector. If set to true, no       preconditioner will be used. This option is set to true in some       scenarios to escape saddle points, but is otherwise seldom activated.\n\nConstructor\n\nTruncatedConjugateGradientOptions(x, stop, eta, delta, Delta, res, uR)\n\nconstruct a truncated conjugate-gradient Option with the fields as above.\n\nSee also\n\ntruncatedConjugateGradient, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncatedConjugateGradient.html#Additional-Stopping-Criteria-1","page":"Steihaug-Toint TCG Method","title":"Additional Stopping Criteria","text":"","category":"section"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"stopIfResidualIsReducedByPower","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Manopt.stopIfResidualIsReducedByPower","page":"Steihaug-Toint TCG Method","title":"Manopt.stopIfResidualIsReducedByPower","text":"stopIfResidualIsReducedByPower <: StoppingCriterion\n\nA functor for testing if the norm of residual at the current iterate is reduced by a power of 1+θ compared to the norm of the initial residual, i.e. Vert r_k Vert_x leqq  Vert r_0 Vert_x^1+theta. In this case the algorithm reached superlinear convergence.\n\nFields\n\nθ – part of the reduction power\ninitialResidualNorm - stores the norm of the residual at the initial vector   eta of the Steihaug-Toint tcg mehtod truncatedConjugateGradient\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\n\nConstructor\n\nstopIfResidualIsReducedByPower(iRN, θ)\n\ninitialize the stopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN to the power of 1+θ.\n\nSee also\n\ntruncatedConjugateGradient, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"stopIfResidualIsReducedByFactor","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Manopt.stopIfResidualIsReducedByFactor","page":"Steihaug-Toint TCG Method","title":"Manopt.stopIfResidualIsReducedByFactor","text":"stopIfResidualIsReducedByFactor <: StoppingCriterion\n\nA functor for testing if the norm of residual at the current iterate is reduced by a factor compared to the norm of the initial residual, i.e. Vert r_k Vert_x leqq kappa Vert r_0 Vert_x. In this case the algorithm reached linear convergence.\n\nFields\n\nκ – the reduction factor\ninitialResidualNorm - stores the norm of the residual at the initial vector   eta of the Steihaug-Toint tcg mehtod truncatedConjugateGradient\nreason – stores a reason of stopping if the stopping criterion has one be reached, see get_reason.\n\nConstructor\n\nstopIfResidualIsReducedByFactor(iRN, κ)\n\ninitialize the stopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN times κ.\n\nSee also\n\ntruncatedConjugateGradient, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"StopWhenTrustRegionIsExceeded","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Manopt.StopWhenTrustRegionIsExceeded","page":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenTrustRegionIsExceeded","text":"StopWhenTrustRegionIsExceeded <: StoppingCriterion\n\nA functor for testing if the norm of the next iterate in the  Steihaug-Toint tcg mehtod is larger than the trust-region radius, i.e. Vert η_k^* Vert_x  Δ. terminate the algorithm when the trust region has been left.\n\nFields\n\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\nstorage – stores the necessary parameters η, δ, residual to check the   criterion.\n\nConstructor\n\nStopWhenTrustRegionIsExceeded([a])\n\ninitialize the StopWhenTrustRegionIsExceeded functor to indicate to stop after the norm of the next iterate is greater than the trust-region radius using the StoreOptionsAction a, which is initialized to store :η, :δ, :residual by default.\n\nSee also\n\ntruncatedConjugateGradient, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncatedConjugateGradient.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"StopWhenCurvatureIsNegative","category":"page"},{"location":"solvers/truncatedConjugateGradient.html#Manopt.StopWhenCurvatureIsNegative","page":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenCurvatureIsNegative","text":"StopWhenCurvatureIsNegative <: StoppingCriterion\n\nA functor for testing if the curvature of the model is negative, i.e. langle delta_k operatornameHessF(delta_k)rangle_x leqq 0. In this case, the model is not strictly convex, and the stepsize as computed does not give a reduction of the model.\n\nFields\n\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\nstorage – stores the necessary parameter δ to check the   criterion.\n\nConstructor\n\nStopWhenCurvatureIsNegative([a])\n\ninitialize the StopWhenCurvatureIsNegative functor to indicate to stop after the inner product of the search direction and the hessian applied on the search dircetion is less than zero using the StoreOptionsAction a, which is initialized to just store :δ by default.\n\nSee also\n\ntruncatedConjugateGradient, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"about.html#About-1","page":"About","title":"About","text":"","category":"section"},{"location":"about.html#","page":"About","title":"About","text":"Manopt.jl inherited its name from Manopt, a Matlab toolbox. It is currently Maintained by Ronny Bergmann (manopt@ronnybergmann.net) with contributions from Tom Christian Riemer, who implemented the trust regions solver.","category":"page"},{"location":"about.html#","page":"About","title":"About","text":"If you want to contribute a manifold or algorithm or have any questions, visit the GitHub repository to clone/fork the repository or open an issue.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/MeanAndMedian.jl\"","category":"page"},{"location":"tutorials/MeanAndMedian.html#Optimize-1","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"This example illustrates how to set up and solve optimization problems and how to further get data from the algorithm using DebugOptions and RecordOptions","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"To start from the quite general case: A Solver is an algorithm that aims to solve","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"operatorname*argmin_xmathcal M f(x)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"where mathcal M is a Manifold and fcolonmathcal M to mathbb R is the cost function.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"In Manopt.jl a Solver is an algorithm that requires a Problem p and Options o. While former contains static data, most prominently the manifold mathcal M (usually as p.M) and the cost function f (usually as p.cost), the latter contains dynamic data, i.e. things that usually change during the algorithm, are allowed to change, or specify the details of the algorithm to use. Together they form a plan. A plan uniquely determines the algorithm to use and provide all necessary information to run the algorithm.","category":"page"},{"location":"tutorials/MeanAndMedian.html#Example-1","page":"Getting Started: Optimize!","title":"Example","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"A gradient plan consists of a GradientProblem with the fields M, cost function f as well as gradient storing the gradient function corresponding to f. Accessing both functions can be done directly but should be encapsulated using get_cost(p,x) and getGradient(p,x), where in both cases x is a point on the Manifold M. Second, the GradientDescentOptions specify that the algorithm to solve the GradientProblem will be the gradient descent algorithm. It requires an initial value o.x0, a StoppingCriterion o.stop, a Stepsize o.stepsize and a retraction o.retraction and it internally stores the last evaluation of the gradient at o.∇ for convenience. The only mandatory parameter is the initial value x0, though the defaults for both the stopping criterion (StopAfterIteration(100)) as well as the stepsize (ConstantStepsize(1.) are quite conservative, but are chosen to be as simple as possible.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"With these two at hand, running the algorithm just requires to call xOpt = solve(p,o).","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"In the following two examples we will see, how to use a higher level interface that allows to more easily activate for example a debug output or record values during the iterations","category":"page"},{"location":"tutorials/MeanAndMedian.html#The-given-Dataset-1","page":"Getting Started: Optimize!","title":"The given Dataset","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"using Manopt, Manifolds\nusing Random, Colors","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"For a persistent random set we use","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"n = 100\nσ = π/8\nM = Sphere(2)\nx = 1/sqrt(2)*[1., 0., 1.]\nRandom.seed!(42)\ndata = [exp(M,x,random_tangent(M,x,Val(:Gaussian),σ)) for i ∈ 1:n ]\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"black = RGBA{Float64}(colorant\"#000000\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\")\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\")\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\")\nTolVibrantMagenta = RGBA{Float64}(colorant\"#EE3377\")\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"Then our data looks like","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"asymptote_export_S2_signals(\"startDataAndCenter.asy\";\n    points = [ [x], data],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal]),\n    dotSize = 3.5, cameraPosition = (1.,.5,.5)\n)\nrender_asymptote(\"startDataAndCenter.asy\"; render = 2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"(Image: The data of noisy versions of $x$)","category":"page"},{"location":"tutorials/MeanAndMedian.html#Computing-the-Mean-1","page":"Getting Started: Optimize!","title":"Computing the Mean","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"To compute the mean on the manifold we use the characterization, that the Euclidean mean minimizes the sum of squared distances, and end up with the following cost function. Its minimizer is called Riemannian Center of Mass.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"F = y -> sum(1/(2*n) * distance.(Ref(M),Ref(y),data).^2)\n∇F = y -> sum(1/n*∇distance.(Ref(M),data,Ref(y)))\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"note that the ∇distance defaults to the case p=2, i.e. the gradient of the squared distance. For details on convergence of the gradient descent for this problem, see [Afsari, Tron, Vidal, 2013]","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"The easiest way to call the gradient descent is now to call steepest_descent","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"xMean = steepest_descent(M,F,∇F,data[1])\nnothing; #hide\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"but in order to get more details, we further add the debug= options, which act as a decorator pattern using the DebugOptions and DebugActions. The latter store values if that's necessary, for example for the DebugChange that prints the change during the last iteration. The following debug prints","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"# i | x: | Last Change: | F(x):`","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"as well as the reason why the algorithm stopped at the end. Here, the format shorthand and the [DebugFactory] are used, which returns a DebugGroup of DebugAction performed each iteration and the stop, respectively.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"xMean = steepest_descent(M,F,∇F,data[1];\n   debug = [:Iteration,\" | \", :x, \" | \", :Change, \" | \", :Cost, \"\\n\", :Stop]\n)\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"asymptote_export_S2_signals(\"startDataCenterMean.asy\";\n    points = [ [x], data, [xMean] ],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal, TolVibrantOrange]),\n    dotSize = 3.5, cameraPosition = (1.,.5,.5)\n)\nrender_asymptote(\"startDataCenterMean.asy\"; render = 2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"(Image: The resulting mean (orange))","category":"page"},{"location":"tutorials/MeanAndMedian.html#Computing-the-Median-1","page":"Getting Started: Optimize!","title":"Computing the Median","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"Similar to the mean you can also define the median as the minimizer of the distances, see for example [Bačák, 2014], but since this problem is not differentiable, we employ the Cyclic Proximal Point (CPP) algorithm, described in the same reference. We define","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"F2 = y -> sum( 1/(2*n) * distance.(Ref(M),Ref(y),data))\nproxes = Function[ (λ,y) -> prox_distance(M,λ/n,di,y,1) for di in data ]\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"where the Function is a helper for global scope to infer the correct type.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"We then call the cyclic_proximal_point as","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"o = cyclic_proximal_point(M,F2,proxes,data[1];\n    debug = [:Iteration,\" | \", :x, \" | \", :Change, \" | \", :Cost, \"\\n\", 50, :Stop],\n    record = [:Iteration, :Change, :Cost],\n    returnOptions = true\n)\nxMedian = get_solver_result(o)\nvalues = get_record(o)\nnothing # hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"where the differences to steepest_descent are as follows","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"the third parameter is now an Array of proximal maps\ndebug is reduces to only every 50th iteration\nwe further activated a RecordAction using the record= optional parameter. These work very similar to those in debug, but they collect their data in an array. The high level interface then returns two variables; the values do contain an array of recorded datum per iteration. Here a Tuple containing the iteration, last change and cost respectively; see RecordGroup, RecordIteration, RecordChange, RecordCost as well as the RecordFactory for details. The values contains hence a tuple per iteration, that itself consists of (by order of specification) the iteration number, the last change and the cost function value.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"These recorded entries read","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"values","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"The resulting median and mean for the data hence are","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"asymptote_export_S2_signals(\"startDataCenterMean.asy\";\n    points = [ [x], data, [xMean], [xMedian] ],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal, TolVibrantOrange, TolVibrantMagenta]),\n    dotSize = 3.5, cameraPosition = (1.,.5,.5)\n)\nrender_asymptote(\"startDataCenterMedianAndMean.asy\"; render = 2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"(Image: The resulting mean (orange) and median (magenta))","category":"page"},{"location":"tutorials/MeanAndMedian.html#Literature-1","page":"Getting Started: Optimize!","title":"Literature","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"Getting Started: Optimize!","title":"Getting Started: Optimize!","text":"<ul>\n<li id=\"Bačák2014\">[<a>Bačák, 2014</a>]\n  Bačák, M: <emph>Computing Medians and Means in Hadamard Spaces.</emph>,\n  SIAM Journal on Optimization, Volume 24, Number 3, pp. 1542–1566,\n  doi: <a href=\"https://doi.org/10.1137/140953393\">10.1137/140953393</a>,\n  arxiv: <a href=\"https://arxiv.org/abs/1210.2145\">1210.2145</a>.</li>\n  <li id=\"AfsariTronVidal2013\">[<a>Afsari, Tron, Vidal, 2013</a>]\n   Afsari, B; Tron, R.; Vidal, R.: <emph>On the Convergence of Gradient\n   Descent for Finding the Riemannian Center of Mass</emph>,\n   SIAM Journal on Control and Optimization, Volume 51, Issue 3,\n   pp. 2230–2260.\n   doi: <a href=\"https://doi.org/10.1137/12086282X\">10.1137/12086282X</a>,\n   arxiv: <a href=\"https://arxiv.org/abs/1201.0925\">1201.0925</a></li>\n</ul>","category":"page"},{"location":"helpers/data.html#Data-1","page":"Data","title":"Data","text":"","category":"section"},{"location":"helpers/data.html#","page":"Data","title":"Data","text":"For some manifolds there are artificial or real application data available that can be loaded using the following data functions","category":"page"},{"location":"helpers/data.html#","page":"Data","title":"Data","text":"  artificial_S1_signal\n  artificial_S1_slope_signal\n  artificialIn_SAR_image\n  artificial_S2_lemniscate\n  artificial_S2_whirl_patch\n  artificial_S2_whirl_image\n  artificial_S2_rotation_image\n  artificial_SPD_image\n  artificial_SPD_image2","category":"page"},{"location":"helpers/data.html#Manopt.artificial_S1_signal","page":"Data","title":"Manopt.artificial_S1_signal","text":"artificial_S1_signal([pts=500])\n\ngenerate a real-valued signal having piecewise constant, linear and quadratic intervals with jumps in between. If the resulting manifold the data lives on, is the Circle the data is also wrapped to -pipi).\n\nOptional\n\npts – (500) number of points to sample the function\n\n\n\n\n\nartificial_S1_signal(x)\n\nevaluate the example signal f(x) x   01, of phase-valued data introduces in Sec. 5.1 of\n\nBergmann, Laus, Steidl, Weinmann, Second Order Differences of Cyclic Data and Applications in Variational Denoising, SIAM J. Imaging Sci., 7(4), 2916–2953, 2014. doi: 10.1137/140969993\n\nfor values outside that intervall, this Signal is missing.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S1_slope_signal","page":"Data","title":"Manopt.artificial_S1_slope_signal","text":"artificial_S1_slope_signal([pts=500, slope=4.])\n\nCreates a Signal of (phase-valued) data represented on the CircleManifold with increasing slope.\n\nOptional\n\npts – (500) number of points to sample the function.\nslope – (4.0) initial slope that gets increased afterwards\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificialIn_SAR_image","page":"Data","title":"Manopt.artificialIn_SAR_image","text":"artificialIn_SAR_image([pts=500])\n\ngenerate an artificial InSAR image, i.e. phase valued data, of size pts x pts points.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_lemniscate","page":"Data","title":"Manopt.artificial_S2_lemniscate","text":"artificial_S2_lemniscate(p [,pts=128,a=π/2,interval=[0,2π])\n\ngenerate a Signal on the Sphere mathbb S^2 by creating the Lemniscate of Bernoulli in the tangent space of p sampled at pts points and use exp to get a signal on the Sphere.\n\nInput\n\np – the tangent space the Lemniscate is created in\npts – (128) number of points to sample the Lemniscate\na – (π/2) defines a half axis of the Lemniscate to cover a  half sphere.\ninterval – ([0,2*π]) range to sample the lemniscate at, the default value refers to one closed curve\n\n\n\n\n\nartificial_S2_lemniscate(p,t; a=π/2)\n\ngenerate a point from the signal on the Sphere mathbb S^2 by creating the Lemniscate of Bernoulli in the tangent space of p sampled at t and use èxp` to obtain a point on the Sphere.\n\nInput\n\np – the tangent space the Lemniscate is created in\nt – value to sample the Lemniscate at\n\nOptional Values\n\na – (π/2) defines a half axis of the Lemniscate to cover a half sphere.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_whirl_patch","page":"Data","title":"Manopt.artificial_S2_whirl_patch","text":"artificial_S2_whirl_patch([pts=5])\n\ncreate a whirl within the ptstimespts patch of Sphere(@ref)(2)-valued image data.\n\nOptional Parameters\n\npts – (5) size of the patch. If the number is odd, the center is the north pole.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_whirl_image","page":"Data","title":"Manopt.artificial_S2_whirl_image","text":"artificial_S2_whirl_image([pts=64])\n\ngenerate an artificial image of data on the 2 sphere,\n\nArguments\n\npts – (64) size of the image in ptstimespts pixel.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_rotation_image","page":"Data","title":"Manopt.artificial_S2_rotation_image","text":"artificial_S2_rotation_image([pts=64, rotations=(.5,.5)])\n\ncreates an image with a rotation on each axis as a parametrization.\n\nOptional Parameters\n\npts – (64) number of pixels along one dimension\nrotations – ((.5,.5)) number of total rotations performed on the axes.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_SPD_image","page":"Data","title":"Manopt.artificial_SPD_image","text":"artificial_SPD_image([pts=64, stepsize=1.5])\n\ncreate an artificial image of symmetric positive definite matrices of size ptstimespts pixel with a jump of size stepsize.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_SPD_image2","page":"Data","title":"Manopt.artificial_SPD_image2","text":"artificial_SPD_image2([pts=64, fraction=.66])\n\ncreate an artificial image of symmetric positive definite matrices of size ptstimespts pixel with right hand side fraction is moved upwards.\n\n\n\n\n\n","category":"function"},{"location":"solvers/subgradient.html#SubgradientSolver-1","page":"Subgradient method","title":"Subgradient Method","text":"","category":"section"},{"location":"solvers/subgradient.html#","page":"Subgradient method","title":"Subgradient method","text":"subgradient_method","category":"page"},{"location":"solvers/subgradient.html#Manopt.subgradient_method","page":"Subgradient method","title":"Manopt.subgradient_method","text":"subgradient_method(M, F, ∂F, x)\n\nperform a subgradient method x_k+1 = mathrmretr(x_k s_kF(x_k)),\n\nwhere mathrmretr is a retraction, s_k can be specified as a function but is usually set to a constant value. Though the subgradient might be set valued, the argument ∂F should always return one element from the subgradient.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n∂F: the (sub)gradient partial Fcolonmathcal Mto Tmathcal M of F restricted to always only returning one value/element from the subgradient\nx – an initial value x  mathcal M\n\nOptional\n\nstepsize – (ConstantStepsize(1.)) specify a Stepsize\nretraction – (exp) a retraction(M,x,ξ) to use.\nstoppingCriterion – (StopWhenAny(StopAfterIteration(200),StopWhenGradientNormLess(10.0^-8))) a functor, seeStoppingCriterion, indicating when to stop.\nreturnOptions – (false) – if actiavated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value xOpt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\nxOpt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see returnOptions)\n\n\n\n\n\n","category":"function"},{"location":"solvers/subgradient.html#Options-1","page":"Subgradient method","title":"Options","text":"","category":"section"},{"location":"solvers/subgradient.html#","page":"Subgradient method","title":"Subgradient method","text":"SubGradientMethodOptions","category":"page"},{"location":"solvers/subgradient.html#Manopt.SubGradientMethodOptions","page":"Subgradient method","title":"Manopt.SubGradientMethodOptions","text":"SubGradientMethodOptions <: Options\n\nstories option values for a subgradient_method solver\n\nFields\n\nretraction – the retration to use within\nstepsize – a Stepsize\nstop – a StoppingCriterion\nx – (initial or current) value the algorithm is at\nxOptimal – optimal value\n∂ the current element from the possivle subgradients at x that is used\n\n\n\n\n\n","category":"type"},{"location":"solvers/subgradient.html#","page":"Subgradient method","title":"Subgradient method","text":"For DebugActions and RecordActions to record (sub)gradient, its norm and the step sizes, see the steepest Descent actions.","category":"page"},{"location":"functions/manifold.html#Specific-manifold-functions-1","page":"Specific manifold functions","title":"Specific manifold functions","text":"","category":"section"},{"location":"functions/manifold.html#","page":"Specific manifold functions","title":"Specific manifold functions","text":"This small section extends the functions available from ManifoldsBase.jl and Manifolds.jl, espcially a few random generators, that are simpler than the functions available.","category":"page"},{"location":"functions/manifold.html#","page":"Specific manifold functions","title":"Specific manifold functions","text":"Modules = [Manopt]\nPages   = [\"manifold.jl\"]","category":"page"},{"location":"functions/manifold.html#Manopt.mid_point-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Specific manifold functions","title":"Manopt.mid_point","text":"mid_point(M, p, q, x)\n\nCompute the mid point between p and q. If there is more than one mid point of (not neccessarily miniizing) geodesics (i.e. on the sphere), the one nearest to x is returned.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.mid_point-Tuple{ManifoldsBase.Manifold,Any,Any}","page":"Specific manifold functions","title":"Manopt.mid_point","text":"mid_point(M, p, q)\n\nCompute the (geodesic) mid point of the two points p and q on the manfold M. If the geodesic is not unique, either a deterministic choice is taken or an error is raised depending on the manifold. For the deteministic choixe, see mid_point(M, p, q, x), the mid point closest to a third point x.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.reflect-Tuple{ManifoldsBase.Manifold,Any,Any}","page":"Specific manifold functions","title":"Manopt.reflect","text":"reflect(M, p, x)\n\nreflect the point x from the manifold M at point x, i.e.\n\n    operatornamerefl_p(x) = exp_p(-log_p x)\n\nwhere exp and log denote the exponential and logarithmic map on M.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.sym_rem-Union{Tuple{N}, Tuple{N}, Tuple{N,Any}} where N<:Number","page":"Specific manifold functions","title":"Manopt.sym_rem","text":"sym_rem(x,[T=π])\n\nCompute symmetric remainder of x with respect to the interall 2*T, i.e. (x+T)%2T, where the default for T is π\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Simplified-random-functions-1","page":"Specific manifold functions","title":"Simplified random functions","text":"","category":"section"},{"location":"functions/manifold.html#","page":"Specific manifold functions","title":"Specific manifold functions","text":"While statistics are available in Manifolds.jl, the following functions provide default random points and vectors on manifolds.","category":"page"},{"location":"functions/manifold.html#","page":"Specific manifold functions","title":"Specific manifold functions","text":"Modules = [Manopt]\nPages   = [\"random.jl\"]","category":"page"},{"location":"functions/manifold.html#Manopt.random_point","page":"Specific manifold functions","title":"Manopt.random_point","text":"randomMPoint(M::Rotations [,type=:Gaussian, σ=1.0])\n\nreturn a random point p on the manifold Rotations by generating a (Gaussian) random orthogonal matrix with determinant +1. Let QR = A be the QR decomposition of a random matrix A, then the formula reads p = QD where D is a diagonal matrix with the signs of the diagonal entries of R, i.e.\n\nD_ij=begincases\noperatornamesgn(R_ij)  textif  i=j \n0   textotherwise\nendcases\n\nIt can happen that the matrix gets -1 as a determinant. In this case, the first and second columns are swapped.\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_point","page":"Specific manifold functions","title":"Manopt.random_point","text":"random_point(M::Sphere, :Gaussian, σ=1.0])\n\nreturn a random point on the Sphere by projecting a normal distirbuted vector from within the embedding to the sphere.\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{Manifolds.Circle,Val{:Uniform}}","page":"Specific manifold functions","title":"Manopt.random_point","text":"random_point(M, :Uniform)\n\nreturn a random point on the Circle mathbb S^1 by picking a random element from -pipi) uniformly.\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{Manifolds.Euclidean}","page":"Specific manifold functions","title":"Manopt.random_point","text":"random_point(M::Euclidean[,T=Float64])\n\ngenerate a random point on the Euclidean manifold M, where the optional parameter determines the type of the entries of the resulting point on the Euclidean space d.\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{Manifolds.ProductManifold,Vararg{Any,N} where N}","page":"Specific manifold functions","title":"Manopt.random_point","text":"random_point(M::ProductManifold [,type=:Gaussian, σ=1.0])\n\nreturn a random point x on Grassmannian manifold M by generating a random (Gaussian) matrix with standard deviation σ in matching size, which is orthonormal.\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{N}, Tuple{Manifolds.SymmetricPositiveDefinite{N},Val{:Gaussian}}, Tuple{Manifolds.SymmetricPositiveDefinite{N},Val{:Gaussian},Float64}} where N","page":"Specific manifold functions","title":"Manopt.random_point","text":"random_point(M::SymmetricPositiveDefinite, :Gaussian[, σ=1.0])\n\ngerenate a random symmetric positive definite matrix on the SymmetricPositiveDefinite manifold M.\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{𝔽}, Tuple{k}, Tuple{n}, Tuple{Manifolds.Grassmann{n,k,𝔽},Val{:Gaussian}}, Tuple{Manifolds.Grassmann{n,k,𝔽},Val{:Gaussian},Float64}} where 𝔽 where k where n","page":"Specific manifold functions","title":"Manopt.random_point","text":"random_point(M::Grassmannian [,type=:Gaussian, σ=1.0])\n\nreturn a random point x on Grassmannian manifold M by generating a random (Gaussian) matrix with standard deviation σ in matching size, which is orthonormal.\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{𝔽}, Tuple{k}, Tuple{n}, Tuple{Manifolds.Stiefel{n,k,𝔽},Val{:Gaussian}}, Tuple{Manifolds.Stiefel{n,k,𝔽},Val{:Gaussian},Float64}} where 𝔽 where k where n","page":"Specific manifold functions","title":"Manopt.random_point","text":"random_point(M::Stiefel, :Gaussian, σ=1.0])\n\nreturn a random (Gaussian) point x on the Stiefel manifold M by generating a (Gaussian) matrix with standard deviation σ and return the orthogonalized version, i.e. return ​​the Q component of the QR decomposition of the random matrix of size nk.\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific manifold functions","title":"Manopt.random_tangent","text":"random_tangent(M,x, Val(:Rician) [,σ = 0.01])\n\ngenerate a random tangent vector in the tangent space of x on the SymmetricPositiveDefinite manifold M by using a Rician distribution with standard deviation σ.\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific manifold functions","title":"Manopt.random_tangent","text":"random_tangent(M::GRassmann,x[,type=:Gaussian, σ=1.0])\n\nreturn a (Gaussian) random vector from the tangent space T_xmathrmGr(nk) with mean zero and standard deviation σ by projecting a random Matrix onto the  x.\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific manifold functions","title":"Manopt.random_tangent","text":"random_tangent(M::Rotations, p[, type=:Gaussian, σ=1.0])\n\nreturn a random tangent vector in the tangent space T_xmathrmSO(n) of the point x on the Rotations manifold M by generating a random skew-symmetric matrix. The function takes the real upper triangular matrix of a (Gaussian) random matrix A with dimension ntimes n and subtracts its transposed matrix. Finally, the matrix is ​​normalized.\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific manifold functions","title":"Manopt.random_tangent","text":"random_tangent(M::Sphere, x[, :Gaussian, σ=1.0])\n\nreturn a random tangent vector in the tangent space of x on the Sphere M.\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific manifold functions","title":"Manopt.random_tangent","text":"random_tangent(M, p[, :Gaussian, σ = 1.0])\n\ngenerate a random tangent vector in the tangent space of the point p on the SymmetricPositiveDefinite manifold M by using a Gaussian distribution with standard deviation σ on an ONB of the tangent space.\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific manifold functions","title":"Manopt.random_tangent","text":"random_tangent(M::Circle, x[, :Gaussian, σ=1.0])\n\nreturn a random tangent vector from the tangent space of the point x on the Circle mathbb S^1 by using a normal distribution with mean 0 and standard deviation 1.\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{Manifolds.Hyperbolic,Any,Val{:Gaussian}}","page":"Specific manifold functions","title":"Manopt.random_tangent","text":"random_tangent(M::Hyperpolic, p)\n\ngenerate a random point on the Hyperbolic manifold by projecting a point from the embedding with respect to the Minkowsky metric.\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{Manifolds.ProductManifold,Any,Vararg{Any,N} where N}","page":"Specific manifold functions","title":"Manopt.random_tangent","text":"random_tangent(M::ProductManifold, x)\n\ngenerate a random tangent vector in the tangent space of the point p on the ProductManifold M.\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{ManifoldsBase.Manifold,Any}","page":"Specific manifold functions","title":"Manopt.random_tangent","text":"random_tangent(M,p)\n\ngenerate a random tangent vector in the tangent space of p on M. By default this is a :Gaussian distribution.\n\n\n\n","category":"method"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/JacobiFields.jl\"","category":"page"},{"location":"tutorials/JacobiFields.html#Illustration-of-Jacobi-Fields-1","page":"Jacobi Fields","title":"Illustration of Jacobi Fields","text":"","category":"section"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"This tutorial illustrates the usage of Jacobi Fields within Manopt.jl. For this tutorial you should be familiar with the basic terminology on a manifold like the exponential and logarithmic map as well as shortest geodesics.","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"We first initialize the manifold","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"using Manopt, Manifolds","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"using Colors\nblack = RGBA{Float64}(colorant\"#000000\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\")\nTolVibrantCyan = RGBA{Float64}(colorant\"#33BBEE\")\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\")\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Assume we have two points on the equator of the Sphere mathcal M = mathbb S^2","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"M = Sphere(2)\np,q = [ [1.,0.,0.], [0.,1.,0.]]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"their connecting shortest geodesic (sampled at 100 points)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"geodesicCurve = shortest_geodesic(M,p,q,[0:0.1:1.0...]);\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"looks as follows using the asymptote_export_S2_signals export","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"asymptote_export_S2_signals(\"jacobiGeodesic.asy\";\n    render = asyResolution,\n    curves=[geodesicCurve], points = [ [x,y] ],\n    colors=Dict(:curves => [black], :points => [TolVibrantOrange]),\n    dotSize = 3.5, lineWidth = 0.75, cameraPosition = (1.,1.,.5)\n)\nrender_asymptote(\"jacobiGeodesic.asy\"; render = 2)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"(Image: A geodesic connecting two points on the equator)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"where x is on the left. Then this tutorial solves the following task:","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Given a direction X_p T_xmathcal M, for example","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"X = [0.,0.4,0.5]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"we move the start point x into, how does any point on the geodesic move?","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Or mathematically: Compute D_p g(t pq) for some fixed t01 and a given direction X_p. Of course two cases are quite easy: For t=0 we are in x and how x “moves” is already known, so D_x g(0pq) = X. On the other side, for t=1, g(1 pq) = q which is fixed, so D_p g(1 pq) is the zero tangent vector (in T_qmathcal M).","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"For all other cases we employ a jacobi_field, which is a (tangent) vector field along the shortest geodesic given as follows: The geodesic variation Gamma_gX(st) is defined for some varepsilon  0 as","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Gamma_gX(st)=expgamma_pX(s)tlog_g(spX)pqquad s(-varepsilonvarepsilon) t01","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Intuitively we make a small step s into direction xi using the geodesic g(cdot pX) and from r=g(s pX) we follow (in t) the geodesic g(cdot rq). The corresponding Jacobi field~(J_{g,X}) along~(g(\\cdot; p,q)) is given","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"J_gX(t)=fracDpartial sGamma_gX(st)Biglrvert_s=0","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"which is an ODE and we know the boundary conditions J_gX(0)=X and J_gX(t) = 0. In symmetric spaces we can compute the solution, since the system of ODEs decouples, see for example do Carmo, Chapter 4.2. Within Manopt.jl this is implemented as jacobi_field(M,p,q,t,X[,β]), where the optional parameter (function) β specifies, which Jacobi field we want to evaluate and the one used here is the default.","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"We can hence evaluate that on the points on the geodesic at","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"T = [0:0.1:1.0...]\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"namely","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"r = shortest_geodesic(M,p,q,T)\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"the geodesic moves as","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"W = jacobi_field.(Ref(M), Ref(p), Ref(q), T, Ref(X) )","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"which can also be called using differential_geodesic_startpoint. We can add to the image above by creating extended tangent vectors the include their base points","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"V = [ Tuple([a,b]) for (a,b) ∈ zip(r,W) ]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"and add that as one further set to the Asymptote export.","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"asymptote_export_S2_signals(\"jacobiGeodesicdifferential_geodesic_startpoint.asy\";\n    render = asyResolution,\n    curves=[geodesicCurve], points = [ [x,y], Z], tVectors = [Vx],\n    colors=Dict(\n        :curves => [black],\n        :points => [TolVibrantOrange,TolVibrantCyan],\n        :tvectors => [TolVibrantCyan]\n    ),\n    dotSizes = [3.5,2.], lineWidth = 0.75, cameraPosition = (1.,1.,.5)\n)\nrender_asymptote(\"jacobiGeodesicdifferential_geodesic_startpoint.asy\"; render = 2)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"(Image: A Jacobi field for $D_xg(t,x,y)[\\eta]$)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"If we further move the end point, too, we can derive that Differential in direction","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Xq = [0.2,0.,-0.5]\nW2 = differential_geodesic_endpoint.(Ref(M),Ref(p),Ref(q),T,Ref(Xq))\nV2 = [ Tuple([a,b]) for (a,b) ∈ zip(r,W2) ]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"and we can combine both keeping the base point","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"V3 = [ Tuple([a,b]) for (a,b) ∈ zip(r,W2+W) ]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"asymptote_export_S2_signals(\"jacobiGeodesicResult.asy\";\n   render = asyResolution,\n   curves=[geodesicCurve], points = [ [x,y], Z], tVectors = [Vx,Vy,Vb],\n   colors=Dict(\n       :curves => [black],\n       :points => [TolVibrantOrange,TolVibrantCyan],\n       :tvectors => [TolVibrantCyan,TolVibrantCyan,TolVibrantTeal]\n  ),\n  dotSizes = [3.5,2.], lineWidth = 0.75, cameraPosition = (1.,1.,0.)\n)\nrender_asymptote(\"jacobiGeodesicResult.asy\"; render = 2)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"(Image: A Jacobi field for the effect of two differentials (blue) in sum (teal))","category":"page"},{"location":"tutorials/JacobiFields.html#Literature-1","page":"Jacobi Fields","title":"Literature","text":"","category":"section"},{"location":"tutorials/JacobiFields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"<ul><li id=\"doCarmo1992\">[<a>doCarmo1992</a>] do Carmo, M. P.:\n   <emph>Riemannian Geometry</emph>, Mathematics: Theory & Applications,\n   Birkhäuser Basel, 1992, ISBN: 0-8176-3490-8</li>\n<li id=\"BergmannGousenbourger2018\">[<a>BergmannGousenbourger2018</a>]\n  Bergmann, R.; Gousenbourger, P.-Y.: <emph>A variational model for data\n  fitting on manifolds by minimizing the acceleration of a Bézier curve</emph>,\n  Frontiers in Applied Mathematics and Statistics, 2018.\n  doi: <a href=\"https://dx.doi.org/10.3389/fams.2018.00059\">10.3389/fams.2018.00059</a></li>\n</ul>","category":"page"},{"location":"functions/index.html#Functions-1","page":"Introduction","title":"Functions","text":"","category":"section"},{"location":"functions/index.html#","page":"Introduction","title":"Introduction","text":"There are several functions required within optimization, most prominently costFunctions and gradients. This package includes several cost functions and corresponding gradients, but also corresponding proximal maps for variational methods manifold-valued data. Most of these functions require the evaluation of Differentials or their AdjointDifferentials as well as JacobiFields (e.g. easily to evaluate for symmetric manifolds).","category":"page"},{"location":"functions/proximalMaps.html#proximalMapFunctions-1","page":"Proximal Maps","title":"Proximal Maps","text":"","category":"section"},{"location":"functions/proximalMaps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"For a function varphicolonmathcal M tomathbb R the proximal map is defined as","category":"page"},{"location":"functions/proximalMaps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"displaystyleoperatornameprox_lambdavarphi(x)\n= operatorname*argmin_y  mathcal M d_mathcal M^2(xy) + varphi(y)\nquad lambda  0","category":"page"},{"location":"functions/proximalMaps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"where d_mathcal Mcolon mathcal M times mathcal M to mathbb R denotes the geodesic distance on (\\mathcal M). While it might still be difficult to compute the minimizer, there are several proximal maps known (locally) in closed form. Furthermore if x^star  mathcal M is a minimizer of varphi, then","category":"page"},{"location":"functions/proximalMaps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"displaystyleoperatornameprox_lambdavarphi(x^star) = x^star","category":"page"},{"location":"functions/proximalMaps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"i.e. a minimizer is a fixed point of the proximal map.","category":"page"},{"location":"functions/proximalMaps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"This page lists all proximal maps available within Manopt. To add you own, just extend the functions/proximalMaps.jl file.","category":"page"},{"location":"functions/proximalMaps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"Modules = [Manopt]\nPages   = [\"proximalMaps.jl\"]","category":"page"},{"location":"functions/proximalMaps.html#Manopt.prox_TV-Union{Tuple{T}, Tuple{N}, Tuple{𝔽}, Tuple{Manifolds.PowerManifold{𝔽,N,T,TPR} where TPR<:Manifolds.AbstractPowerRepresentation,Any,Any}, Tuple{Manifolds.PowerManifold{𝔽,N,T,TPR} where TPR<:Manifolds.AbstractPowerRepresentation,Any,Any,Int64}} where T where N<:ManifoldsBase.Manifold where 𝔽","page":"Proximal Maps","title":"Manopt.prox_TV","text":"ξ = prox_TV(M,λ,x [,p=1])\n\ncompute the proximal maps operatornameprox_lambdavarphi of all forward differences orrucirng in the power manifold array, i.e. varphi(xixj) = d_mathcal M^p(xixj) with xi and xj are array elemets of x and j = i+e_k, where e_k is the kth unitvector. The parameter λ is the prox parameter.\n\nInput\n\nM – a Manifold\nλ – a real value, parameter of the proximal map\nx – a point.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\ny – resulting  point containinf with all mentioned proximal points evaluated (in a cylic order).\n\n\n\n\n\n","category":"method"},{"location":"functions/proximalMaps.html#Manopt.prox_TV-Union{Tuple{T}, Tuple{mT}, Tuple{mT,Number,Tuple{T,T}}, Tuple{mT,Number,Tuple{T,T},Int64}} where T where mT<:ManifoldsBase.Manifold","page":"Proximal Maps","title":"Manopt.prox_TV","text":"(y1,y2) = prox_TV(M,λ,(x1,x2) [,p=1])\n\nCompute the proximal map operatornameprox_lambdavarphi of varphi(xy) = d_mathcal M^p(xy) with parameter λ.\n\nInput\n\nM – a Manifold\nλ – a real value, parameter of the proximal map\n(x1,x2) – a tuple of two points,\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\n(y1,y2) – resulting tuple of points of the operatornameprox_lambdavarphi( (x1,x2) )\n\n\n\n\n\n","category":"method"},{"location":"functions/proximalMaps.html#Manopt.prox_TV2-Union{Tuple{T}, Tuple{ManifoldsBase.Manifold,Any,Tuple{T,T,T}}, Tuple{ManifoldsBase.Manifold,Any,Tuple{T,T,T},Int64}} where T","page":"Proximal Maps","title":"Manopt.prox_TV2","text":"(y1,y2,y3) = prox_TV2(M,λ,(x1,x2,x3),[p=1], kwargs...)\n\nCompute the proximal map operatornameprox_lambdavarphi of varphi(x_1x_2x_3) = d_mathcal M^p(c(x_1x_3)x_2) with parameter λ>0, where c(xz) denotes the mid point of a shortest geodesic from x1 to x3 that is closest to x2.\n\nInput\n\nM          – a manifold\nλ          – a real value, parameter of the proximal map\n(x1,x2,x3) – a tuple of three points\np – (1) exponent of the distance of the TV term\n\nOptional\n\nkwargs... – parameters for the internal subgradient_method     (if M is neither Euclidean nor Circle, since for these a closed form     is given)\n\nOutput\n\n(y1,y2,y3) – resulting tuple of points of the proximal map\n\n\n\n\n\n","category":"method"},{"location":"functions/proximalMaps.html#Manopt.prox_TV2-Union{Tuple{T}, Tuple{N}, Tuple{Manifolds.PowerManifold{N,T,TSize,TPR} where TPR<:Manifolds.AbstractPowerRepresentation where TSize,Any,Any}, Tuple{Manifolds.PowerManifold{N,T,TSize,TPR} where TPR<:Manifolds.AbstractPowerRepresentation where TSize,Any,Any,Int64}} where T where N","page":"Proximal Maps","title":"Manopt.prox_TV2","text":"ξ = prox_TV2(M,λ,x,[p])\n\ncompute the proximal maps operatornameprox_lambdavarphi of all centered second order differences orrucirng in the power manifold array, i.e. varphi(x_kx_ix_j) = d_2(x_kx_ix_j), where kj are backward and forward neighbors (along any dimension in the array of x). The parameter λ is the prox parameter.\n\nInput\n\nM – a Manifold\nλ – a real value, parameter of the proximal map\nx – a points.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\ny – resulting point with all mentioned proximal points evaluated (in a cylic order).\n\n\n\n\n\n","category":"method"},{"location":"functions/proximalMaps.html#Manopt.prox_collaborative_TV","page":"Proximal Maps","title":"Manopt.prox_collaborative_TV","text":"prox_collaborative_TV(M,λ,x [,p=2,q=1])\n\ncompute the prox of the collaborative TV prox for x on the PowerManifold manifold, i.e. of the function\n\nF^q(x) = sum_i  mathcal G\n  Bigl( sum_j  mathcal I_i\n    sum_k=1^d lVert X_ijrVert_x^pBigr)^fracqp\n\nwhere mathcal G is the set of indices for x  mathcal M and mathcal I_i is the set of its forward neighbors. This is adopted from the paper by Duran, Möller, Sbert, Cremers: Collaborative Total Variation: A General Framework for Vectorial TV Models (arxiv: 1508.01308), where the most inner norm is not on a manifold but on a vector space, see their Example 3 for details.\n\n\n\n\n\n","category":"function"},{"location":"functions/proximalMaps.html#Manopt.prox_distance","page":"Proximal Maps","title":"Manopt.prox_distance","text":"y = prox_distance(M,λ,f,x [,p=2])\n\ncompute the proximal map operatornameprox_lambdavarphi with parameter λ of varphi(x) = frac1pd_mathcal M^p(fx).\n\nInput\n\nM – a Manifold mathcal M\nλ – the prox parameter\nf – a point f  mathcal M (the data)\nx – the argument of the proximal map\n\nOptional argument\n\np – (2) exponent of the distance.\n\nOuput\n\ny – the result of the proximal map of varphi\n\n\n\n\n\n","category":"function"},{"location":"functions/proximalMaps.html#Manopt.prox_parallel_TV-Union{Tuple{T}, Tuple{Manifolds.PowerManifold,Any,Array{T,1}}, Tuple{Manifolds.PowerManifold,Any,Array{T,1},Int64}} where T","page":"Proximal Maps","title":"Manopt.prox_parallel_TV","text":"ξ = prox_parallel_TV(M,λ,x [,p=1])\n\ncompute the proximal maps operatornameprox_lambdavarphi of all forward differences orrucirng in the power manifold array, i.e. varphi(xixj) = d_mathcal M^p(xixj) with xi and xj are array elemets of x and j = i+e_k, where e_k is the kth unitvector. The parameter λ is the prox parameter.\n\nInput\n\nM     – a PowerManifold manifold\nλ     – a real value, parameter of the proximal map\nx     – a point\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\ny  – resulting Array of points with all mentioned proximal points evaluated (in a parallel within the arrays elements).\n\nSee also prox_TV\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#DifferentialFunctions-1","page":"Differentials","title":"Differentials","text":"","category":"section"},{"location":"functions/differentials.html#","page":"Differentials","title":"Differentials","text":"Modules = [Manopt]\nPages   = [\"differentials.jl\"]","category":"page"},{"location":"functions/differentials.html#Manopt.differential_exp_argument-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Differentials","title":"Manopt.differential_exp_argument","text":"differential_exp_argument(M, p, X, Y)\n\ncomputes D_Xexp_pXY. Note that X   T_X(T_pmathcal M) = T_pmathcal M is still a tangent vector.\n\nSee also\n\ndifferential_exp_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_exp_basepoint-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Differentials","title":"Manopt.differential_exp_basepoint","text":"differential_exp_basepoint(M, p, X, Y)\n\nCompute D_pexp_p XY.\n\nSee also\n\ndifferential_exp_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_forward_logs-Tuple{Manifolds.PowerManifold,Any,Any}","page":"Differentials","title":"Manopt.differential_forward_logs","text":"Y = differential_forward_logs(M, p, X)\n\ncompute the differenital of forward_logs F on the PowerManifold manifold M at p and direction X , in the power manifold array, the differential of the function\n\nF_i(x) = sum_j  mathcal I_i log_p_i p_j quad i    mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM     – a PowerManifold manifold\np     – a point.\nX     – a tangent vector.\n\nOuput\n\nY – resulting tangent vector in T_xmathcal N representing the differentials of the logs, where mathcal N is thw power manifold with the number of dimensions added to size(x).\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_geodesic_endpoint-Union{Tuple{mT}, Tuple{mT,Any,Any,Any,Any}} where mT<:ManifoldsBase.Manifold","page":"Differentials","title":"Manopt.differential_geodesic_endpoint","text":"differential_geodesic_endpoint(M,x,y,t,η)\n\ncomputes D_qg(tpq)eta.\n\nSee also\n\ndifferential_geodesic_startpoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_geodesic_startpoint-Union{Tuple{mT}, Tuple{mT,Any,Any,Any,Any}} where mT<:ManifoldsBase.Manifold","page":"Differentials","title":"Manopt.differential_geodesic_startpoint","text":"differential_geodesic_startpoint(M, p, q, t, X)\n\ncomputes D_p g(tpq)eta.\n\nSee also\n\ndifferential_geodesic_endpoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_log_argument-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Differentials","title":"Manopt.differential_log_argument","text":"differential_log_argument(M,p,q,X)\n\ncomputes D_qlog_pqX.\n\nSee also\n\ndifferential_log_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_log_basepoint-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Differentials","title":"Manopt.differential_log_basepoint","text":"differential_log_basepoint(M, p, q, X)\n\ncomputes D_plog_pqX.\n\nSee also\n\ndifferential_log_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"helpers/errorMeasures.html#ErrorMeasures-1","page":"Error Measures","title":"Error Measures","text":"","category":"section"},{"location":"helpers/errorMeasures.html#","page":"Error Measures","title":"Error Measures","text":"meanSquaredError\nmeanAverageError","category":"page"},{"location":"helpers/errorMeasures.html#Manopt.meanSquaredError","page":"Error Measures","title":"Manopt.meanSquaredError","text":"meanSquaredError(M, p, q)\n\nCompute the (mean) squared error between the two points p and q on the (power) manifold M.\n\n\n\n\n\n","category":"function"},{"location":"helpers/errorMeasures.html#Manopt.meanAverageError","page":"Error Measures","title":"Manopt.meanAverageError","text":"meanSquaredError(M,x,y)\n\nComputes the (mean) squared error between the two points x and y on the PowerManifold manifold M.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#planSection-1","page":"Plans","title":"Plans for solvers","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"CurrentModule = Manopt","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"In order to start a solver, both a Problem and Options are required. Together they form a plan and these are stored in this folder. For sub-problems there are maybe also only Options, since they than refer to the same problem.","category":"page"},{"location":"plans/index.html#Options-1","page":"Plans","title":"Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"For most algorithms a certain set of options can either be generated beforehand of the function with keywords can be used. Generally the type","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Options\nget_options","category":"page"},{"location":"plans/index.html#Manopt.Options","page":"Plans","title":"Manopt.Options","text":"Options\n\nA general super type for all options.\n\nFields\n\nThe following fields are assumed to be default. If you use different ones, provide the access functions accordingly\n\nx a point with the current iterate\nstop a StoppingCriterion.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_options","page":"Plans","title":"Manopt.get_options","text":"get_options(o::Options)\n\nreturn the undecorated Options of the (possibly) decorated o. As long as your decorated options store the options within o.options and the dispatch_options_decorator is set to Val{true}, the internal options are extracted.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Since the Options directly relate to a solver, they are documented with the corresponding Solvers. You can always access the options (since they might be decorated) by calling get_options.","category":"page"},{"location":"plans/index.html#Decorators-for-Options-1","page":"Plans","title":"Decorators for Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Options can be decorated using the following trait and function to initialize","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"dispatch_options_decorator\nis_options_decorator\ndecorate_options","category":"page"},{"location":"plans/index.html#Manopt.dispatch_options_decorator","page":"Plans","title":"Manopt.dispatch_options_decorator","text":"dispatch_options_decorator(o::Options)\n\nIndicate internally, whether an Options o to be of decorating type, i.e. it stores (encapsulates) options in itself, by default in the field o. options.\n\nDecorators indicate this by returning Val{true} for further dispatch.\n\nThe default is Val{false}, i.e. by default an options is not decorated.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.is_options_decorator","page":"Plans","title":"Manopt.is_options_decorator","text":"is_options_decorator(o::Options)\n\nIndicate, whether Options o are of decorator type.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.decorate_options","page":"Plans","title":"Manopt.decorate_options","text":"decorate_options(o)\n\ndecorate the Optionso with specific decorators.\n\nOptional Arguments\n\noptional arguments provide necessary details on the decorators. A specific one is used to activate certain decorators.\n\ndebug – (Array{Union{Symbol,DebugAction,String,Int},1}()) a set of symbols representing DebugActions, Strings used as dividers and a subsampling integer. These are passed as a DebugGroup within :All to the DebugOptions decorator dictionary. Only excention is :Stop that is passed to :Stop.\nrecord – (Array{Union{Symbol,RecordAction,Int},1}()) specify recordings by using Symbols or RecordActions directly. The integer can again be used for only recording every ith iteration.\n\nSee also\n\nDebugOptions, RecordOptions\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"In general decorators often perform actions so we introduce","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"AbstractOptionsAction","category":"page"},{"location":"plans/index.html#Manopt.AbstractOptionsAction","page":"Plans","title":"Manopt.AbstractOptionsAction","text":"AbstractOptionsAction\n\na common Type for AbstractOptionsActions that might be triggered in decoraters, for example DebugOptions or RecordOptions.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"as well as a helper for storing values using keys, i.e.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"StoreOptionsAction\nget_storage\nhas_storage\nupdate_storage!","category":"page"},{"location":"plans/index.html#Manopt.StoreOptionsAction","page":"Plans","title":"Manopt.StoreOptionsAction","text":"StoreTupleAction <: AbstractOptionsAction\n\ninternal storage for AbstractOptionsActions to store a tuple of fields from an Optionss\n\nThis functor posesses the usual interface of functions called during an iteration, i.e. acts on (p,o,i), where p is a Problem, o is an Options and i is the current iteration.\n\nFields\n\nvalues – a dictionary to store interims values based on certain Symbols\nkeys – an NTuple of Symbols to refer to fields of Options\nonce – whether to update the internal values only once per iteration\nlastStored – last iterate, where this AbstractOptionsAction was called (to determine once\n\nConstructiors\n\nStoreOptionsAction([keys=(), once=true])\n\nInitialize the Functor to an (empty) set of keys, where once determines whether more that one update per iteration are effective\n\nStoreOptionsAction(keys, once=true])\n\nInitialize the Functor to a set of keys, where the dictionary is initialized to be empty. Further, once determines whether more that one update per iteration are effective, otherwise only the first update is stored, all others are ignored.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_storage","page":"Plans","title":"Manopt.get_storage","text":"get_storage(a,key)\n\nreturn the internal value of the StoreOptionsAction a at the Symbol key.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.has_storage","page":"Plans","title":"Manopt.has_storage","text":"get_storage(a,key)\n\nreturn whether the StoreOptionsAction a has a value stored at the Symbol key.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.update_storage!","page":"Plans","title":"Manopt.update_storage!","text":"update_storage!(a,o)\n\nupdate the StoreOptionsAction a internal values to the ones given on the Options o.\n\n\n\n\n\nupdate_storage!(a,o)\n\nupdate the StoreOptionsAction a internal values to the ones given in the dictionary d. The values are merged, where the values from d are preferred.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#DebugOptions-1","page":"Plans","title":"Debug Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Modules = [Manopt]\nPages = [\"plans/debugOptions.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"plans/index.html#Manopt.DebugAction","page":"Plans","title":"Manopt.DebugAction","text":"DebugAction\n\nA DebugAction is a small functor to print/issue debug output. The usual call is given by (p,o,i) -> s that performs the debug based on a Problem p, Options o and the current iterate i.\n\nBy convention i=0 is interpreted as \"For Initialization only\", i.e. only debug info that prints initialization reacts, i<0 triggers updates of variables internally but does not trigger any output. Finally typemin(Int) is used to indicate a call from stop_solver! that returns true afterwards.\n\nFields (assumed by subtypes to exist)\n\nprint method to perform the actual print. Can for example be set to a file export,\n\nor to @info. The default is the print function on the default Base.stdout.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugChange","page":"Plans","title":"Manopt.DebugChange","text":"DebugChange(a,prefix,print)\n\ndebug for the amount of change of the iterate (stored in o.x of the Options) during the last iteration. See DebugEntryChange\n\nParameters\n\nx0 – an initial value to already get a Change after the first iterate. Can be left out\na – (StoreOptionsAction( (:x,) )) – the storage of the previous action\nprefix – (\"Last Change:\") prefix of the debug output\nprint – (print) default method to peform the print.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugCost","page":"Plans","title":"Manopt.DebugCost","text":"DebugCost <: DebugAction\n\nprint the current cost function value, see get_cost.\n\nConstructors\n\nDebugCost(long,print)\n\nwhere long indicated whether to print F(x): (default) or cost:\n\nDebugCost(prefix,print)\n\nset a prefix manually.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugDivider","page":"Plans","title":"Manopt.DebugDivider","text":"DebugDivider <: DebugAction\n\nprint a small divider (default \" | \").\n\nConstructor\n\nDebugDivider(div,print)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugEntry","page":"Plans","title":"Manopt.DebugEntry","text":"DebugEntry <: RecordAction\n\nprint a certain fields entry of type {T} during the iterates\n\nAddidtional Fields\n\nfield – Symbol the entry can be accessed with within Options\n\nConstructor\n\nDebugEntry(f[, prefix=\"$f:\", print=print])\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugEntryChange","page":"Plans","title":"Manopt.DebugEntryChange","text":"DebugEntryChange{T} <: DebugAction\n\nprint a certain entries change during iterates\n\nAdditional Fields\n\nprint – (print) function to print the result\nprefix – (\"Change of :x\") prefix to the print out\nfield – Symbol the field can be accessed with within Options\ndistance – function (p,o,x1,x2) to compute the change/distance between two values of the entry\nstorage – a StoreOptionsAction to store the previous value of :f\n\nConstructors\n\nDebugEntryChange(f,d[, a, prefix, print])\n\ninitialize the Debug to a field f and a distance d.\n\nDebugEntryChange(v,f,d[, a, prefix=\"Change of $f:\", print])\n\ninitialize the Debug to a field f and a distance d with initial value v for the history of o.field.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugEvery","page":"Plans","title":"Manopt.DebugEvery","text":"DebugEvery <: DebugAction\n\nevaluate and print debug only every ith iteration. Otherwise no print is performed. Whether internal variables are updates is determined by alwaysUpdate.\n\nThis method does not perform any print itself but relies on it's childrens print.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugGroup","page":"Plans","title":"Manopt.DebugGroup","text":"DebugGroup <: DebugAction\n\ngroup a set of DebugActions into one action, where the internal prints are removed by default and the resulting strings are concatenated\n\nConstructor\n\nDebugGroup(g)\n\nconstruct a group consisting of an Array of DebugActions g, that are evaluated en bloque; the method does not perform any print itself, but relies on the internal prints. It still concatenates the result and returns the complete string\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugIterate","page":"Plans","title":"Manopt.DebugIterate","text":"DebugIterate <: DebugAction\n\ndebug for the current iterate (stored in o.x).\n\nParameters\n\nlong::Bool whether to print x: or current iterate\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugIteration","page":"Plans","title":"Manopt.DebugIteration","text":"DebugIteration <: DebugAction\n\ndebug for the current iteration (prefixed with #)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugOptions","page":"Plans","title":"Manopt.DebugOptions","text":"DebugOptions <: Options\n\nThe debug options append to any options a debug functionality, i.e. they act as a decorator pattern. Internally a Dictionary is kept that stores a DebugAction for several occasions using a Symbol as reference. The default occasion is :All and for example solvers join this field with :Start, :Step and :Stop at the beginning, every iteration or the end of the algorithm, respectively\n\nThe original options can still be accessed using the get_options function.\n\nFields (defaults in brackets)\n\noptions – the options that are extended by debug information\ndebugDictionary – a Dict{Symbol,DebugAction} to keep track of Debug for different actions\n\nConstructors\n\nDebugOptions(o,dA)\n\nconstruct debug decorated options, where dD can be\n\na DebugAction, then it is stored within the dictionary at :All\nan Array of DebugActions, then it is stored as a debugDictionary within :All.\na Dict{Symbol,DebugAction}.\nan Array of Symbols, String and an Int for the DebugFactory\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugStoppingCriterion","page":"Plans","title":"Manopt.DebugStoppingCriterion","text":"DebugStoppingCriterion <: DebugAction\n\nprint the Reason provided by the stopping criterion. Usually this should be empty, unless the algorithm stops.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugActionFactory-Tuple{String}","page":"Plans","title":"Manopt.DebugActionFactory","text":"DebugActionFactory(s)\n\ncreate a DebugAction where\n\na Stringyields the correspoinding divider\na DebugAction is passed through\na [Symbol] creates DebugEntry of that symbol, with the exceptions of :Change, :Iterate, :Iteration, and :Cost.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.DebugFactory-Tuple{Array{#s19,1} where #s19}","page":"Plans","title":"Manopt.DebugFactory","text":"DebugFactory(a)\n\ngiven an array of Symbols, Strings DebugActions and Ints\n\nThe symbol :Stop creates an entry of to display the stoping criterion at the end (:Stop => DebugStoppingCriterion())\nThe symbol :Cost creates a DebugCost\nThe symbol :iteration creates a DebugIteration\nThe symbol :Change creates a DebugChange\nany other symbol creates debug output of the corresponding field in Options\nany string creates a DebugDivider\nany DebugAction is directly included\nan Integer kintroduces that debug is only printed every kth iteration\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"see DebugSolver for details on the decorated solver.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Further specific DebugActions can be found at the specific Options.","category":"page"},{"location":"plans/index.html#RecordOptions-1","page":"Plans","title":"Record Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Modules = [Manopt]\nPages = [\"plans/recordOptions.jl\"]\nOrder = [:type, :function]\nPrivate = false","category":"page"},{"location":"plans/index.html#Manopt.RecordAction","page":"Plans","title":"Manopt.RecordAction","text":"RecordAction\n\nA RecordAction is a small functor to record values. The usual call is given by (p,o,i) -> s that performs the record based on a Problem p, Options o and the current iterate i.\n\nBy convention i<=0 is interpreted as \"For Initialization only\", i.e. only initialize internal values, but not trigger any record, the same holds for i=typemin(Inf) which is used to indicate stop, i.e. that the record is called from within stop_solver! which returns true afterwards.\n\nFields (assumed by subtypes to exist)\n\nrecordedValues an Array of the recorded values.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordChange","page":"Plans","title":"Manopt.RecordChange","text":"RecordChange <: RecordAction\n\ndebug for the amount of change of the iterate (stored in o.x of the Options) during the last iteration.\n\nAdditional Fields\n\nstorage a StoreOptionsAction to store (at least) o.x to use this as the last value (to compute the change)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordCost","page":"Plans","title":"Manopt.RecordCost","text":"RecordCost <: RecordAction\n\nrecord the current cost function value, see get_cost.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordEntry","page":"Plans","title":"Manopt.RecordEntry","text":"RecordEntry{T} <: RecordAction\n\nrecord a certain fields entry of type {T} during the iterates\n\nFields\n\nrecordedValues – the recorded Iterates\nfield – Symbol the entry can be accessed with within Options\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordEntryChange","page":"Plans","title":"Manopt.RecordEntryChange","text":"RecordEntryChange{T} <: RecordAction\n\nrecord a certain entries change during iterates\n\nAdditional Fields\n\nrecordedValues – the recorded Iterates\nfield – Symbol the field can be accessed with within Options\ndistance – function (p,o,x1,x2) to compute the change/distance between two values of the entry\nstorage – a StoreOptionsAction to store (at least) getproperty(o, d.field)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordEvery","page":"Plans","title":"Manopt.RecordEvery","text":"RecordEvery <: RecordAction\n\nrecord only every ith iteration. Otherwise (optionally, but activated by default) just update internal tracking values.\n\nThis method does not perform any record itself but relies on it's childrens methods\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordGroup","page":"Plans","title":"Manopt.RecordGroup","text":"RecordGroup <: RecordAction\n\ngroup a set of RecordActions into one action, where the internal prints are removed by default and the resulting strings are concatenated\n\nConstructor\n\nRecordGroup(g)\n\nconstruct a group consisting of an Array of RecordActions g, that are recording en bloque; the method does not perform any record itself, but keeps an array of records. Accessing these yields a Tuple of the recorded values per iteration\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordIterate","page":"Plans","title":"Manopt.RecordIterate","text":"RecordIterate <: RecordAction\n\nrecord the iterate\n\nConstructors\n\nRecordIterate(x0)\n\ninitialize the iterate record array to the type of x0, e.g. your initial data.\n\nRecordIterate(P)\n\ninitialize the iterate record array to the data type T.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordIteration","page":"Plans","title":"Manopt.RecordIteration","text":"RecordIteration <: RecordAction\n\nrecord the current iteration\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordOptions","page":"Plans","title":"Manopt.RecordOptions","text":"RecordOptions <: Options\n\nappend to any Options the decorator with record functionality, Internally a Dictionary is kept that stores a RecordAction for several occasions using a Symbol as reference. The default occasion is :All and for example solvers join this field with :Start, :Step and :Stop at the beginning, every iteration or the end of the algorithm, respectively\n\nThe original options can still be accessed using the get_options function.\n\nFields\n\noptions – the options that are extended by debug information\nrecordDictionary – a Dict{Symbol,RecordAction} to keep track of all different recorded values\n\nConstructors\n\nRecordOptions(o,dR)\n\nconstruct record decorated Options, where dR can be\n\na RecordAction, then it is stored within the dictionary at :All\nan Array of RecordActions, then it is stored as a recordDictionary(@ref) within the dictionary at :All.\na Dict{Symbol,RecordAction}.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordActionFactory-Union{Tuple{A}, Tuple{O}, Tuple{O,A}} where A<:RecordAction where O<:Options","page":"Plans","title":"Manopt.RecordActionFactory","text":"RecordActionFactory(s)\n\ncreate a RecordAction where\n\na RecordAction is passed through\na [Symbol] creates RecordEntry of that symbol, with the exceptions of :Change, :Iterate, :Iteration, and :Cost.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.RecordFactory-Union{Tuple{O}, Tuple{O,Array{#s12,1} where #s12}} where O<:Options","page":"Plans","title":"Manopt.RecordFactory","text":"RecordFactory(a)\n\ngiven an array of Symbols and RecordActions and Ints\n\nThe symbol :Cost creates a RecordCost\nThe symbol :iteration creates a RecordIteration\nThe symbol :Change creates a RecordChange\nany other symbol creates a RecordEntry of the corresponding field in Options\nany RecordAction is directly included\nan Integer k introduces that record is only performed every kth iteration\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.get_record","page":"Plans","title":"Manopt.get_record","text":"get_record(o[,s=:Step])\n\nreturn the recorded values from within the RecordOptions o that where recorded with respect to the Symbol s as an Array. The default refers to any recordings during an Iteration represented by the Symbol :Step\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.get_record-Union{Tuple{R}, Tuple{R}} where R<:RecordAction","page":"Plans","title":"Manopt.get_record","text":"get_record(r)\n\nreturn the recorded values stored within a RecordAction r.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.has_record-Tuple{RecordOptions}","page":"Plans","title":"Manopt.has_record","text":"has_record(o)\n\ncheck whether the Optionso are decorated with RecordOptions\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"see RecordSolver for details on the decorated solver.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Further specific RecordActions can be found at the specific Options.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"there's one internal helper that might be useful for you own actions, namely","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"record_or_eset!","category":"page"},{"location":"plans/index.html#Manopt.record_or_eset!","page":"Plans","title":"Manopt.record_or_eset!","text":"record_or_eset!(r,v,i)\n\neither record (i>0 and not Inf) the value v within the RecordAction r or reset (i<0) the internal storage, where v has to match the internal value type of the corresponding Recordaction.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Stepsize-1","page":"Plans","title":"Stepsize and Linesearch","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"The step size determination is implemented as a Functor based on","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Stepsize","category":"page"},{"location":"plans/index.html#Manopt.Stepsize","page":"Plans","title":"Manopt.Stepsize","text":"Stepsize\n\nAn abstract type for the functors representing step sizes, i.e. they are callable structurs. The naming scheme is TypeOfStepSize, e.g. ConstantStepsize.\n\nEvery Stepsize has to provide a constructor and its function has to have the interface (p,o,i) where a Problem as well as Options and the current number of iterations are the arguments and returns a number, namely the stepsize to use.\n\nSee also\n\nLinesearch\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"in general there are","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Modules = [Manopt]\nPages = [\"plans/stepsize.jl\"]\nOrder = [:type]","category":"page"},{"location":"plans/index.html#Manopt.ArmijoLinesearch","page":"Plans","title":"Manopt.ArmijoLinesearch","text":"ArmijoLineseach <: Linesearch\n\nA functor representing Armijo line seach including the last runs state, i.e. a last step size.\n\nFields\n\ninitialStepsize – (1.0) and initial step size\nretraction – the rectraction used in line search\ncontractionFactor – (0.95) exponent for line search reduction\nsufficientDecrease – (0.1) gain within Armijo's rule\nlastStepSize – (initialstepsize) the last step size we start the search with\n\nConstructor\n\nArmijoLineSearch()\n\nwith the Fields above in their order as optional arguments.\n\nThis method returns the functor to perform Armijo line search, where two inter faces are available:\n\nbased on a tuple (p,o,i) of a GradientProblem p, Options o and a current iterate i.\nwith (M,x,F,∇Fx[,η=-∇Fx]) -> s where Manifold M, a current point x a function F, that maps from the manifold to the reals, its gradient (a tangent vector) ∇F=nabla F(x) at  x and an optional search direction tangent vector η-∇F are the arguments.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.ConstantStepsize","page":"Plans","title":"Manopt.ConstantStepsize","text":"ConstantStepsize <: Stepsize\n\nA functor that always returns a fixed step size.\n\nFields\n\nlength – constant value for the step size.\n\nConstructor\n\nConstantStepSize(s)\n\ninitialize the stepsie to a constant s\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DecreasingStepsize","page":"Plans","title":"Manopt.DecreasingStepsize","text":"DecreasingStepsize()\n\nA functor that represents several decreasing step sizes\n\nFields\n\nlength – (1) the initial step size l.\nfactor – (1) a value f to multiply the initial step size with every iteration\nsubtrahend – (0) a value a that is subtracted every iteration\nexponent – (1) a value e the current iteration numbers eth exponential is taken of\n\nIn total the complete formulae reads for the ith iterate as\n\n$ s_i = \\frac{(l-i\\cdot a)f^i}{i^e}$\n\nand hence the default simplifies to just $ s_i = \\frac{l}{i} $\n\nConstructor\n\nConstantStepSize(l,f,a,e)\n\ninitialiszes all fields above, where none of them is mandatory.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.Linesearch","page":"Plans","title":"Manopt.Linesearch","text":"Linesearch <: Stepsize\n\nAn abstract functor to represent line search type step size deteminations, see Stepsize for details. One example is the ArmijoLinesearch functor.\n\nCompared to simple step sizes, the linesearch functors provide an interface of the form (p,o,i,η) -> s with an additional (but optional) fourth parameter to proviade a search direction; this should default to something reasonable, e.g. the negative gradient.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Problems-1","page":"Plans","title":"Problems","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"A problem usually contains its cost function and provides and implementation to access the cost","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Problem\nget_cost","category":"page"},{"location":"plans/index.html#Manopt.Problem","page":"Plans","title":"Manopt.Problem","text":"Problem\n\nSpecify properties (values) and related functions for computing a certain optimization problem.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_cost","page":"Plans","title":"Manopt.get_cost","text":"get_cost(p,x)\n\nevaluate the cost function F stored within a Problem at the point x.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"For any algorithm that involves a cyclic evalutaion, e.g. cyclic_proximal_point, one can specify the EvalOrder as","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"EvalOrder\nLinearEvalOrder\nRandomEvalOrder\nFixedRandomEvalOrder","category":"page"},{"location":"plans/index.html#Manopt.EvalOrder","page":"Plans","title":"Manopt.EvalOrder","text":"EvalOrder\n\ntype for specifying an evaluation order for any cyclicly evaluated algorithms\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.LinearEvalOrder","page":"Plans","title":"Manopt.LinearEvalOrder","text":"LinearEvalOrder <: EvalOrder\n\nevaluate in a linear order, i.e. for each cycle of length l evaluate in the order 1,2,...,l.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RandomEvalOrder","page":"Plans","title":"Manopt.RandomEvalOrder","text":"RandomEvalOrder <: EvalOrder\n\nchoose a random order for each evaluation of the l functionals.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.FixedRandomEvalOrder","page":"Plans","title":"Manopt.FixedRandomEvalOrder","text":"FixedRandomEvalOrder <: EvalOrder\n\nChoose a random order once and evaluate always in this order, i.e. for l elements there is one chosen permutation used for each iteration cycle.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Cost-based-problem-1","page":"Plans","title":"Cost based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"CostProblem","category":"page"},{"location":"plans/index.html#Manopt.CostProblem","page":"Plans","title":"Manopt.CostProblem","text":"CostProblem <: Problem\n\nspeficy a problem for solvers just based on cost functions, i.e. gradient free ones.\n\nFields\n\nM            – a manifold mathcal M\ncost – a function Fcolonmathcal Mtomathbb R to minimize\n\nSee also\n\nNelderMead\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Gradient-based-problem-1","page":"Plans","title":"Gradient based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"GradientProblem\ngetGradient","category":"page"},{"location":"plans/index.html#Manopt.GradientProblem","page":"Plans","title":"Manopt.GradientProblem","text":"GradientProblem <: Problem\n\nspecify a problem for gradient based algorithms.\n\nFields\n\nM            – a manifold mathcal M\ncost – a function Fcolonmathcal Mtomathbb R to minimize\ngradient     – the gradient nabla Fcolonmathcal M to mathcal Tmathcal M of the cost function F\n\nSee also\n\nsteepest_descent GradientDescentOptions\n\n\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.getGradient","page":"Plans","title":"Manopt.getGradient","text":"getGradient(p,x)\n\nevaluate the gradient of a GradientProblemp at the point x.\n\n\n\n\n\ngetGradient(p,x)\n\nevaluate the gradient of a HessianProblemp at the point x.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Subgradient-based-problem-1","page":"Plans","title":"Subgradient based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"SubGradientProblem\nget_subgradient","category":"page"},{"location":"plans/index.html#Manopt.SubGradientProblem","page":"Plans","title":"Manopt.SubGradientProblem","text":"SubGradientProblem <: Problem\n\nA structure to store information about a subgradient based optimization problem\n\nFields\n\nmanifold – a Manifold\ncost – the function F to be minimized\nsubgradient – a function returning a subgradient partial F of F\n\nConstructor\n\nSubGradientProblem(M, f, ∂f)\n\nGenerate the [Problem] for a subgradient problem, i.e. a function f on the manifold M and a function ∂f that returns an element from the subdifferential at a point.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_subgradient","page":"Plans","title":"Manopt.get_subgradient","text":"get_subgradient(p,x)\n\nEvaluate the (sub)gradient of a SubGradientProblemp at the point x.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#ProximalProblem-1","page":"Plans","title":"Proximal Map(s) based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"ProximalProblem\ngetProximalMap","category":"page"},{"location":"plans/index.html#Manopt.ProximalProblem","page":"Plans","title":"Manopt.ProximalProblem","text":"ProximalProblem <: Problem\n\nspecify a problem for solvers based on the evaluation of proximal map(s).\n\nFields\n\nM            - a Manifold mathcal M\ncost - a function Fcolonmathcal Mtomathbb R to minimize\nproxes - proximal maps operatornameprox_lambdavarphicolonmathcal Mtomathcal M as functions (λ,x) -> y, i.e. the prox parameter λ also belongs to the signature of the proximal map.\nnumber_of_proxes - (length(proxes)) number of proxmal Maps, e.g. if one of the maps is a compined one such that the proximal Maps functions return more than one entry per function\n\nSee also\n\ncyclic_proximal_point, get_cost, getProximalMap\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.getProximalMap","page":"Plans","title":"Manopt.getProximalMap","text":"getProximalMap(p,λ,x,i)\n\nevaluate the ith proximal map of ProximalProblem p at the point x of p.M with parameter λ0.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Further-planned-problems-1","page":"Plans","title":"Further planned problems","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"HessianProblem\ngetHessian\ngetPreconditioner","category":"page"},{"location":"plans/index.html#Manopt.HessianProblem","page":"Plans","title":"Manopt.HessianProblem","text":"HessianProblem <: Problem\n\nspecify a problem for hessian based algorithms.\n\nFields\n\nM            : a manifold mathcal M\ncost : a function Fcolonmathcal Mtomathbb R to minimize\ngradient     : the gradient nabla Fcolonmathcal M to mathcal Tmathcal M of the cost function F\nhessian      : the hessian operatornameHessF (cdot)_ x colon mathcal T_x mathcal M to mathcal T_x mathcal M of the cost function F\nprecon       : the symmetric, positive deﬁnite   preconditioner (approximation of the inverse of the Hessian of F)\n\nSee also\n\ntruncatedConjugateGradient, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.getHessian","page":"Plans","title":"Manopt.getHessian","text":"getHessian(p,x,ξ)\n\nevaluate the Hessian of a HessianProblem p at the point x applied to a tangent vector ξ.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.getPreconditioner","page":"Plans","title":"Manopt.getPreconditioner","text":"getPreconditioner(p,x,ξ)\n\nevaluate the symmetric, positive deﬁnite preconditioner (approximation of the inverse of the Hessian of the cost function F) of a HessianProblem p at the point xapplied to a tangent vector ξ.\n\n\n\n\n\n","category":"function"},{"location":"helpers/exports.html#Exports-1","page":"Exports","title":"Exports","text":"","category":"section"},{"location":"helpers/exports.html#","page":"Exports","title":"Exports","text":"Exports aim to provide a consistent generation of images of your results. For example if you record the trace your algorithm walks on the Sphere, you yan easily export this trace to a rendered image using asymptote_export_S2_signals and render the result with Asymptote. Despite these, you can always record values during your iterations, and export these, for example to csv.","category":"page"},{"location":"helpers/exports.html#Asymptote-1","page":"Exports","title":"Asymptote","text":"","category":"section"},{"location":"helpers/exports.html#","page":"Exports","title":"Exports","text":"The following functions provide exports both in graphics and/or raw data using Asymptote.","category":"page"},{"location":"helpers/exports.html#","page":"Exports","title":"Exports","text":"Modules = [Manopt]\nPages   = [\"Asymptote.jl\"]","category":"page"},{"location":"helpers/exports.html#Manopt.asymptote_export_S2_data-Tuple{String}","page":"Exports","title":"Manopt.asymptote_export_S2_data","text":"asymptote_export_S2_data(filename)\n\nExport given data as an array of points on the sphere, i.e. one-, two- or three-dimensional data with points on the Sphere mathbb S^2.\n\nInput\n\nfilename – a file to store the Asymptote code in.\n\nOptional Arguments (Data)\n\ndata – a point representing the 1-,2-, or 3-D array of points\nelevationColorScheme - A ColorScheme for elevation\nscaleAxes - ((1/3,1/3,1/3)) move spheres closer to each other by a factor per direction\n\nOptional Arguments (Asymptote)\n\narrowHeadSize - (1.8) size of the arrowheads of the vectors (in mm)\ncameraPosition - position of the camrea (default: centered above xy-plane) szene\ntarget - position the camera points at (default: center of xy-plane within data).\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Manopt.asymptote_export_S2_signals-Tuple{String}","page":"Exports","title":"Manopt.asymptote_export_S2_signals","text":"asymptote_export_S2_signals(filename; points, curves, tVectors, colors, options...)\n\nExport given points, curves, and tVectors on the sphere mathbb S^2 to Asymptote.\n\nInput\n\nfilename – a file to store the Asymptote code in.\n\nOptional Arguments (Data)\n\ncolors - dictionary of color arrays (indexed by symbols :points, :curves and :tvector) where each entry has to provide as least as many colors as the length of the corresponding sets.\ncurves – an Array of Arrays of points on the sphere, where each inner array is interpreted as a curve and is accompanied by an entry within colors\npoints – an Array of Arrays of points on the sphere where each inner array is itnerpreted as a set of points and is accompanied by an entry within colors\ntVectors – an Array of Arrays of tuples, where the first is a points, the second a tangent vector and each set of vectors is accompanied by an entry from within colors\n\nOptional Arguments (Asymptote)\n\narrowHeadSize - (6.0) size of the arrowheads of the tangent vectors\ncameraPosition - ((1., 1., 0.)) position of the camera in the Asymptote szene\nlineWidth – (1.0) size of the lines used to draw the curves.\nlineWidths – overrides the previous value to specify a value per curve and tVector set.\ndotSize – (1.0) size of the dots used to draw the points.\ndotSizes – overrides the previous value to specify a value per point set.\nsphereColor – (RGBA{Float64}(0.85, 0.85, 0.85, 0.6)) color of the sphere the data is drawn on\nsphereLineColor –  (RGBA{Float64}(0.75, 0.75, 0.75, 0.6)) color of the lines on the sphere\nsphereLineWidth – (0.5) line width of the lines on the sphere\ntarget – ((0.,0.,0.)) position the camera points at\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Manopt.asymptote_export_SPD-Tuple{String}","page":"Exports","title":"Manopt.asymptote_export_SPD","text":"asymptote_export_SPD(filename)\n\nexport given data as a point on a Power{SPDPoint} manifold, i.e. one-, two- or three-dimensional data with points on the manifold of symmetric positive definite matrices.\n\nInput\n\nfilename – a file to store the Asymptote code in.\n\nOptional Arguments (Data)\n\ndata – a point representing the 1-,2-, or 3-D array of SPDPoints\ncolorScheme - A ColorScheme for Geometric Anisotropy Index\nscaleAxes - ((1/3,1/3,1/3)) move symmetric positive definite matrices closer to each other by a factor per direction compared to the distance esimated by the maximal eigenvalue of all involved SPD points\n\nOptional Arguments (Asymptote)\n\ncameraPosition - position of the camrea (default: centered above xy-plane) szene.\ntarget - position the camera points at (default: center of xy-plane within data).\n\nBoth values cameraPosition and target are scaled by scaledAxes*EW, where EW is the maximal eigenvalue in the data.\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Manopt.render_asymptote-Tuple{Any}","page":"Exports","title":"Manopt.render_asymptote","text":"render_asymptote(filename; render=4, format=\"png\", ...)\n\nrender an exported asymptote file specified in the filename, which can also be given as a relative or full path\n\nInput\n\nfilename – filename of the exported asy and rendered image\n\nKeyword Arguments\n\nthe default values are given in brackets\n\nrender – (4) render level of asymptote, i.e. its -render option\nformat – (\"png\") final rendered format, i.e. asymptote's -f option\nexport_file - (the filename with format as ending) specify the export filename\n\n\n\n\n\n","category":"method"},{"location":"solvers/trust_regions.html#trust_regions-1","page":"Riemannian Trust-Regions Solver","title":"The Riemannian Trust-Regions Solver","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"The aim is to solve an optimization problem on a manifold","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"operatorname*min_x    mathcalM F(x)","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"by using the Riemannian trust-regions solver. It is number one choice for smooth optimization. This trust-region method uses the Steihaug-Toint truncated conjugate-gradient method truncatedConjugateGradient to solve the inner minimization problem called the trust-regions subproblem. This inner solve can be preconditioned by providing a preconditioner (symmetric and positive deﬁnite, an approximation of the inverse of the Hessian of F). If no Hessian of the cost function F is provided, a standard approximation of the Hessian based on the gradient nabla F with approxHessianFD will be computed.","category":"page"},{"location":"solvers/trust_regions.html#Initialization-1","page":"Riemannian Trust-Regions Solver","title":"Initialization","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"Initialize x_0 = x with an initial point x on the manifold. It can be given by the caller or set randomly. Set the initial trust-region radius Delta =frac18 barDelta where barDelta is the maximum radius the trust-region can have. Usually one uses the root of the manifold dimension operatornamedim(mathcalM). For accepting the next iterate and evaluating the new trust-region radius one needs an accept/reject threshold rho    0frac14), which is rho = 01 on default. Set k=0.","category":"page"},{"location":"solvers/trust_regions.html#Iteration-1","page":"Riemannian Trust-Regions Solver","title":"Iteration","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"Repeat until a convergence criterion is reached","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"Set eta as a random tangent vector if using randomized approach. Else  set eta as the zero vector in the tangential space T_x_kmathcalM.\nSet eta^*  as the solution of the trust-region subproblem, computed by  the tcg-method with eta as initial vector.\nIf using randomized approach compare eta^*  with the Cauchy point  eta_c^*  = -tau_c fracDeltaoperatornamenorm(operatornameGradf (x_k)) operatornameGradF (x_k) by the model function m_x_k(cdot). If the  model decrease is larger by using the Cauchy point, set  eta^*  = eta_c^* .\nSet x^*  = operatornameRetr_x_k(eta^* ).\nSet rho = fracF(x_k)-F(x^* )m_x_k(eta)-m_x_k(eta^* ), where  m_x_k(cdot) describes the quadratic model function.\nUpdate the trust-region radius:  Delta = begincases frac14 Delta  rho  frac14   textor  m_x_k(eta)-m_x_k(eta^* ) leq 0  textor    rho = pm   fty   operatornamemin(2 Delta barDelta)   rho  frac34  textand the tcg-method stopped because of negative  curvature or exceeding the trust-region  Delta   textotherwise  endcases\nIf m_x_k(eta)-m_x_k(eta^* ) geq 0 and rho  rho set  x_k = x^* .\nSet k = k+1.","category":"page"},{"location":"solvers/trust_regions.html#Result-1","page":"Riemannian Trust-Regions Solver","title":"Result","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"The result is given by the last computed x_k.","category":"page"},{"location":"solvers/trust_regions.html#Remarks-1","page":"Riemannian Trust-Regions Solver","title":"Remarks","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"To the Initialization: A random point on the manifold.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"To step number 1: Using randomized approach means using a random tangent vector as initial vector for the approximal solve of the trust-regions subproblem. If this is the case, keep in mind that the vector must be in the trust-region radius. This is achieved by multiplying η by sqrt(4,eps(Float64)) as long as its norm is greater than the current trust-region radius Delta. For not using randomized approach, one can get the zero tangent vector.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"To step number 2: Obtain eta^*  by (approximately) solving the trust-regions subproblem","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"operatorname*argmin_eta    T_x_kmathcalM m_x_k(eta) = F(x_k) +\nlangle nabla F(x_k) eta rangle_x_k + frac12 langle\noperatornameHessF(eta)_ x_k eta rangle_x_k","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"textst  langle eta eta rangle_x_k leq Delta^2","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"with the Steihaug-Toint truncated conjugate-gradient (tcg) method. The problem as well as the solution method is described in the truncatedConjugateGradient.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"To step number 3: If using a random tangent vector as an initial vector, compare the result of the tcg-method with the Cauchy point. Convergence proofs assume that one achieves at least (a fraction of) the reduction of the Cauchy point. The idea is to go in the direction of the gradient to an optimal point. This can be on the edge, but also before. The parameter tau_c for the optimal length is defined by","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"tau_c = begincases 1  langle operatornameGradF (x_k) \noperatornameHessF (eta_k)_ x_krangle_x_k leq 0  \noperatornamemin(fracoperatornamenorm(operatornameGradF (x_k))^3\nDelta langle operatornameGradF (x_k) \noperatornameHessF (eta_k)_ x_krangle_x_k 1)   textotherwise\nendcases","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"To check the model decrease one compares m_x_k(eta_c^* ) = F(x_k) + langle eta_c^*  operatornameGradF (x_k)rangle_x_k + frac12langle eta_c^*  operatornameHessF (eta_c^* )_ x_krangle_x_k with m_x_k(eta^* ) = F(x_k) + langle eta^*  operatornameGradF (x_k)rangle_x_k + frac12langle eta^*  operatornameHessF (eta^* )_ x_krangle_x_k. If m_x_k(eta_c^* )  m_x_k(eta^* ) then is m_x_k(eta_c^* ) the better choice.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"To step number 4: operatornameRetr_x_k(cdot) denotes the retraction, a mapping operatornameRetr_x_kT_x_kmathcalM rightarrow mathcalM wich approximates the exponential map. In some cases it is cheaper to use this instead of the exponential.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"To step number 6: One knows that the truncatedConjugateGradient algorithm stopped for these reasons when the stopping criteria StopWhenCurvatureIsNegative, StopWhenTrustRegionIsExceeded are activated.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"To step number 7: The last step is to decide if the new point x^*  is accepted.","category":"page"},{"location":"solvers/trust_regions.html#Interface-1","page":"Riemannian Trust-Regions Solver","title":"Interface","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"trust_regions","category":"page"},{"location":"solvers/trust_regions.html#Manopt.trust_regions","page":"Riemannian Trust-Regions Solver","title":"Manopt.trust_regions","text":"trust_regions(M, F, ∇F, x, H)\n\nevaluate the Riemannian trust-regions solver for optimization on manifolds. It will attempt to minimize the cost function F on the Manifold M. If no Hessian H is provided, a standard approximation of the Hessian based on the gradient ∇F will be computed. For solving the the inner trust-region subproblem of finding an update-vector, it uses the Steihaug-Toint truncated conjugate-gradient method. For a description of the algorithm and more details see\n\nP.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: 10.1007/s10208-005-0179-9\nA. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: 10.1137/1.9780898719857\n\nInput\n\nM – a manifold mathcal M\nF – a cost function F colon mathcal M to mathbb R to minimize\n∇F- the gradient nabla F colon mathcal M to T mathcal M of F\nx – an initial value x    mathcal M\nH – the hessian H( mathcal M x xi) of F\n\nOptional\n\nretraction – approximation of the exponential map\npreconditioner – a preconditioner (a symmetric, positive definite operator that should approximate the inverse of the Hessian)\nstoppingCriterion – (StopWhenAny(StopAfterIteration(1000), StopWhenGradientNormLess(10^(-6))) a functor inheriting from StoppingCriterion indicating when to stop.\nΔ_bar – the maximum trust-region radius\nΔ - the (initial) trust-region radius\nuseRandom – set to true if the trust-region solve is to be initiated with a random tangent vector. If set to true, no preconditioner will be used. This option is set to true in some scenarios to escape saddle points, but is otherwise seldom activated.\nρ_prime – Accept/reject threshold: if ρ (the performance ratio for the iterate) is at least ρ', the outer iteration is accepted. Otherwise, it is rejected. In case it is rejected, the trust-region radius will have been decreased. To ensure this, ρ' >= 0 must be strictly smaller than 1/4. If ρ_prime is negative, the algorithm is not guaranteed to produce monotonically decreasing cost values. It is strongly recommended to set ρ' > 0, to aid convergence.\nρ_regularization – Close to convergence, evaluating the performance ratio ρ is numerically challenging. Meanwhile, close to convergence, the quadratic model should be a good fit and the steps should be accepted. Regularization lets ρ go to 1 as the model decrease and the actual decrease go to zero. Set this option to zero to disable regularization (not recommended). When this is not zero, it may happen that the iterates produced are not monotonically improving the cost when very close to convergence. This is because the corrected cost improvement could change sign if it is negative but very small.\nreturnOptions – (false) – if actiavated, the extended result, i.e. the complete Options are returned. This can be used to access recorded values. If set to false (default) just the optimal value xOpt is returned\n\nOutput\n\nx – the last reached point on the manifold\n\nsee also\n\ntruncatedConjugateGradient\n\n\n\n\n\n","category":"function"},{"location":"solvers/trust_regions.html#Options-1","page":"Riemannian Trust-Regions Solver","title":"Options","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"TrustRegionsOptions","category":"page"},{"location":"solvers/trust_regions.html#Manopt.TrustRegionsOptions","page":"Riemannian Trust-Regions Solver","title":"Manopt.TrustRegionsOptions","text":"TrustRegionsOptions <: HessianOptions\n\ndescribe the trust-regions solver, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx : a point as starting point\nstop : a function s,r = @(o,iter) returning a stop   indicator and a reason based on an iteration number and the gradient\nΔ : the (initial) trust-region radius\nΔ_bar : the maximum trust-region radius\nuseRand : indicates if the trust-region solve is to be initiated with a       random tangent vector. If set to true, no preconditioner will be       used. This option is set to true in some scenarios to escape saddle       points, but is otherwise seldom activated.\nρ_prime : a lower bound of the performance ratio for the iterate that       decides if the iteration will be accepted or not. If not, the       trust-region radius will have been decreased. To ensure this,       ρ'>= 0 must be strictly smaller than 1/4. If ρ' is negative,       the algorithm is not guaranteed to produce monotonically decreasing       cost values. It is strongly recommended to set ρ' > 0, to aid       convergence.\nρ_regularization : Close to convergence, evaluating the performance ratio ρ       is numerically challenging. Meanwhile, close to convergence, the       quadratic model should be a good fit and the steps should be       accepted. Regularization lets ρ go to 1 as the model decrease and       the actual decrease go to zero. Set this option to zero to disable       regularization (not recommended). When this is not zero, it may happen       that the iterates produced are not monotonically improving the cost       when very close to convergence. This is because the corrected cost       improvement could change sign if it is negative but very small.\n\nConstructor\n\nTrustRegionsOptions(x, stop, delta, delta_bar, uR, rho_prime, rho_reg)\n\nconstruct a trust-regions Option with the fields as above.\n\nSee also\n\ntrust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/trust_regions.html#Approximation-of-the-Hessian-1","page":"Riemannian Trust-Regions Solver","title":"Approximation of the Hessian","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Riemannian Trust-Regions Solver","title":"Riemannian Trust-Regions Solver","text":"approxHessianFD","category":"page"},{"location":"solvers/trust_regions.html#Manopt.approxHessianFD","page":"Riemannian Trust-Regions Solver","title":"Manopt.approxHessianFD","text":"approxHessianFD(p,x,ξ,[stepsize=2.0^(-14)])\n\nreturn an approximated solution of the Hessian of the cost function applied to a tangent vector ξ by using a generic finite difference approximation based on computations of the gradient.\n\nInput\n\np – a Manopt problem structure (already containing the manifold and enough       information to compute the cost gradient)\nx – a point where the Hessian is ​​to be approximated\nξ – a tangent vector on which the approximated Hessian is ​​to be applied\n\nOptional\n\nstepsize – the length of the step with which the method should work\n\nOutput\n\na tangent vector generated by applying the approximated Hessian to the   tangent vector ξ\n\n\n\n\n\n","category":"function"},{"location":"functions/adjointDifferentials.html#adjointDifferentialFunctions-1","page":"Adjoint Differentials","title":"Adjoint Differentials","text":"","category":"section"},{"location":"functions/adjointDifferentials.html#","page":"Adjoint Differentials","title":"Adjoint Differentials","text":"Modules = [Manopt]\nPages   = [\"adjointDifferentials.jl\"]","category":"page"},{"location":"functions/adjointDifferentials.html#Manopt.adjoint_differential_exp_argument-Union{Tuple{mT}, Tuple{mT,Any,Any,Any}} where mT<:ManifoldsBase.Manifold","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_exp_argument","text":"adjoint_differential_exp_argument(M, p, X, Y)\n\nCompute the adjoint of D_Xexp_p XY. Note that X   T_p(T_pmathcal M) = T_pmathcal M is still a tangent vector.\n\nSee also\n\ndifferential_exp_argument, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointDifferentials.html#Manopt.adjoint_differential_exp_basepoint-Union{Tuple{MT}, Tuple{MT,Any,Any,Any}} where MT<:ManifoldsBase.Manifold","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_exp_basepoint","text":"adjoint_differential_exp_basepoint(M, p, X, Y)\n\nComputes the adjoint of D_p exp_p XY.\n\nSee also\n\ndifferential_exp_basepoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointDifferentials.html#Manopt.adjoint_differential_forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{Manifolds.PowerManifold{𝔽,TM,TSize,TPR},Any,Any}} where TPR where TSize where TM where 𝔽","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_forward_logs","text":"Y = adjoint_differential_forward_logs(M, p, X)\n\nCompute the adjoint differential of forward_logs F orrucirng, in the power manifold array p, the differential of the function\n\nF_i(p) = sum_j  mathcal I_i log_p_i p_j\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i Let n be the number dimensions of the PowerManifold manifold (i.e. length(size(x))). Then the input tangent vector lies on the manifold mathcal M = mathcal M^n.\n\nInput\n\nM     – a PowerManifold manifold\np     – an array of points on a manifold\nX     – a tangent vector to from the n-fold power of p, where n is the ndims of p\n\nOuput\n\nY – resulting tangent vector in T_pmathcal M representing the adjoint   differentials of the logs.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointDifferentials.html#Manopt.adjoint_differential_geodesic_endpoint-Union{Tuple{MT}, Tuple{MT,Any,Any,Any,Any}} where MT<:ManifoldsBase.Manifold","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_geodesic_endpoint","text":"adjoint_differential_geodesic_endpoint(M, p, q, t, X)\n\nCompute the adjoint of D_q γ(t p q)X.\n\nSee also\n\ndifferential_geodesic_endpoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointDifferentials.html#Manopt.adjoint_differential_geodesic_startpoint-Union{Tuple{MT}, Tuple{MT,Any,Any,Any,Any}} where MT<:ManifoldsBase.Manifold","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_geodesic_startpoint","text":"adjoint_differential_geodesic_startpoint(M,p, q, t, X)\n\nCompute the adjoint of D_p γ(t p q)X.\n\nSee also\n\ndifferential_geodesic_startpoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointDifferentials.html#Manopt.adjoint_differential_log_argument-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_log_argument","text":"adjoint_differential_log_argument(M, p, q, X)\n\nCompute the adjoint of D_q log_p qX.\n\nSee also\n\ndifferential_log_argument, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointDifferentials.html#Manopt.adjoint_differential_log_basepoint-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_log_basepoint","text":"adjoint_differential_log_basepoint(M, p, q, X)\n\ncomputes the adjoint of D_p log_p qX.\n\nSee also\n\ndifferential_log_basepoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"solvers/gradientDescent.html#GradientDescentSolver-1","page":"Gradient Descent","title":"Gradient Descent","text":"","category":"section"},{"location":"solvers/gradientDescent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/gradientDescent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"  steepest_descent","category":"page"},{"location":"solvers/gradientDescent.html#Manopt.steepest_descent","page":"Gradient Descent","title":"Manopt.steepest_descent","text":"steepest_descent(M, F, ∇F, x)\n\nperform a steepestdescent x{k+1} = \\mathrm{retr}{xk} sk\\nabla f(xk)$ with different choices of s_k available (see stepsize option below).\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n∇F – the gradient nabla Fcolonmathcal Mto Tmathcal M of F\nx – an initial value x  mathcal M\n\nOptional\n\nstepsize – (ConstantStepsize(1.)) specify a Stepsize functor.\nretraction – (exp) a retraction(M,x,ξ) to use.\nstoppingCriterion – ([StopWhenAny](@ref)([StopAfterIteration](@ref)(200), [StopWhenGradientNormLess](@ref)(10.0^-8))) a functor inheriting from [StoppingCriterion`](@ref) indicating when to stop.\nreturnOptions – (false) – if actiavated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value xOpt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\nxOpt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see returnOptions)\n\n\n\n\n\n","category":"function"},{"location":"solvers/gradientDescent.html#Options-1","page":"Gradient Descent","title":"Options","text":"","category":"section"},{"location":"solvers/gradientDescent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"GradientDescentOptions","category":"page"},{"location":"solvers/gradientDescent.html#Manopt.GradientDescentOptions","page":"Gradient Descent","title":"Manopt.GradientDescentOptions","text":"GradientDescentOptions{P,T} <: Options\n\nDescribes a Gradient based descent algorithm, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx0 – an a point (of type P) on a manifold as starting point\nstoppingCriterion – (StopAfterIteration(100)) a StoppingCriterion\nstepsize – (ConstantStepsize(1.))a Stepsize\nretraction – (exp) the rectraction to use\n\nConstructor\n\nGradientDescentOptions(x, stop, s [, retr=exp])\n\nconstruct a Gradient Descent Option with the fields and defaults as above\n\nSee also\n\nsteepest_descent, GradientProblem\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradientDescent.html#Debug-Actions-1","page":"Gradient Descent","title":"Debug Actions","text":"","category":"section"},{"location":"solvers/gradientDescent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"DebugGradient\nDebugGradientNorm\nDebugStepsize","category":"page"},{"location":"solvers/gradientDescent.html#Manopt.DebugGradient","page":"Gradient Descent","title":"Manopt.DebugGradient","text":"DebugGradient <: DebugAction\n\ndebug for the gradient evaluated at the current iterate\n\nConstructors\n\nDebugGradient([long=false,p=print])\n\ndisplay the short (false) or long (true) default text for the gradient.\n\nDebugGradient(prefix[, p=print])\n\ndisplay the a prefix in front of the gradient.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradientDescent.html#Manopt.DebugGradientNorm","page":"Gradient Descent","title":"Manopt.DebugGradientNorm","text":"DebugGradientNorm <: DebugAction\n\ndebug for gradient evaluated at the current iterate.\n\nConstructors\n\nDebugGradientNorm([long=false,p=print])\n\ndisplay the short (false) or long (true) default text for the gradient norm.\n\nDebugGradientNorm(prefix[, p=print])\n\ndisplay the a prefix in front of the gradientnorm.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradientDescent.html#Manopt.DebugStepsize","page":"Gradient Descent","title":"Manopt.DebugStepsize","text":"DebugStepsize <: DebugAction\n\ndebug for the current step size.\n\nConstructors\n\nDebugStepsize([long=false,p=print])\n\ndisplay the short (false) or long (true) default text for the step size.\n\nDebugStepsize(prefix[, p=print])\n\ndisplay the a prefix in front of the step size.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradientDescent.html#Record-Actions-1","page":"Gradient Descent","title":"Record Actions","text":"","category":"section"},{"location":"solvers/gradientDescent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"RecordGradient\nRecordGradientNorm\nRecordStepsize","category":"page"},{"location":"solvers/gradientDescent.html#Manopt.RecordGradient","page":"Gradient Descent","title":"Manopt.RecordGradient","text":"RecordGradient <: RecordAction\n\nrecord the gradient evaluated at the current iterate\n\nConstructors\n\nRecordGradient(ξ)\n\ninitialize the RecordAction to the corresponding type of the tangent vector.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradientDescent.html#Manopt.RecordGradientNorm","page":"Gradient Descent","title":"Manopt.RecordGradientNorm","text":"RecordGradientNorm <: RecordAction\n\nrecord the norm of the current gradient\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradientDescent.html#Manopt.RecordStepsize","page":"Gradient Descent","title":"Manopt.RecordStepsize","text":"RecordStepsize <: RecordAction\n\nrecord the step size\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Solvers-1","page":"Introduction","title":"Solvers","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Solvers can be applied to Problems with solver specific Options.","category":"page"},{"location":"solvers/index.html#List-of-Algorithms-1","page":"Introduction","title":"List of Algorithms","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The following algorithms are currently available","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Solver File Problem & Option\nsteepest Descent steepest_descent.jl GradientProblem, GradientDescentOptions\nCyclic Proximal Point cyclic_proximal_point.jl ProximalProblem, CyclicProximalPointOptions\nDouglas–Rachford DouglasRachford.jl ProximalProblem, DouglasRachfordOptions\nNelder-Mead NelderMead.jl CostProblem, NelderMeadOptions\nSubgradient Method subgradient_method.jl SubGradientProblem, SubGradientMethodOptions\nSteihaug-Toint Truncated Conjugate-Gradient Method truncatedConjugateGradient.jl HessianProblem,","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"TruncatedConjugateGradientOptions The Riemannian Trust-Regions Solver | trust_regions.jl | HessianProblem, TrustRegionsOptions","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Note that the Options can also be decorated to enhance your algorithm by general additional properties.","category":"page"},{"location":"solvers/index.html#StoppingCriteria-1","page":"Introduction","title":"StoppingCriteria","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Stopping criteria are implemented as a functor, i.e. inherit from the base type","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"StoppingCriterion\nStoppingCriterionSet","category":"page"},{"location":"solvers/index.html#Manopt.StoppingCriterion","page":"Introduction","title":"Manopt.StoppingCriterion","text":"StoppingCriterion\n\nAn abstract type for the functors representing stoping criteria, i.e. they are callable structures. The naming Scheme follows functions, see for example StopAfterIteration.\n\nEvery StoppingCriterion has to provide a constructor and its function has to have the interface (p,o,i) where a Problem as well as Options and the current number of iterations are the arguments and returns a Bool whether to stop or not.\n\nBy default each StoppingCriterion should provide a fiels reason to provide details when a criteion is met (and that is empty otherwise).\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StoppingCriterionSet","page":"Introduction","title":"Manopt.StoppingCriterionSet","text":"StoppingCriterionGroup <: StoppingCriterion\n\nAn abstract type for a Stopping Criterion that itself consists of a set of Stopping criteria. In total it acts as a stopping criterion itself. Examples are StopWhenAny and StopWhenAll that can be used to combine stopping criteria.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Modules = [Manopt]\nPages = [\"plans/stoppingCriterion.jl\"]\nOrder = [:type]","category":"page"},{"location":"solvers/index.html#Manopt.StopAfter","page":"Introduction","title":"Manopt.StopAfter","text":"StopAfter <: StoppingCriterion\n\nstore a threshold when to stop looking at the complete runtime. It uses time_ns() to measure the time and you provide a Period as a time limit, i.e. Minute(15)\n\nConstructor\n\nStopAfter(t)\n\ninitialize the stopping criterion to a Period t to stop after.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopAfterIteration","page":"Introduction","title":"Manopt.StopAfterIteration","text":"StopAfterIteration <: StoppingCriterion\n\nA functor for an easy stopping criterion, i.e. to stop after a maximal number of iterations.\n\nFields\n\nmaxIter – stores the maximal iteration number where to stop at\nreason – stores a reason of stopping if the stopping criterion has one be reached, see get_reason.\n\nConstructor\n\nStopAfterIteration(maxIter)\n\ninitialize the stopafterIteration functor to indicate to stop after maxIter iterations.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenAll","page":"Introduction","title":"Manopt.StopWhenAll","text":"StopWhenAll <: StoppingCriterion\n\nstore an array of StoppingCriterion elements and indicates to stop, when all indicate to stop. The reseason is given by the concatenation of all reasons.\n\nConstructor\n\nStopWhenAll(c::Array{StoppingCriterion,1})\nStopWhenAll(c::StoppingCriterion,...)\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenAny","page":"Introduction","title":"Manopt.StopWhenAny","text":"StopWhenAny <: StoppingCriterion\n\nstore an array of StoppingCriterion elements and indicates to stop, when any single one indicates to stop. The reseason is given by the concatenation of all reasons (assuming that all non-indicating return \"\").\n\nConstructor\n\nStopWhenAny(c::Array{StoppingCriterion,1})\nStopWhenAny(c::StoppingCriterion,...)\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenChangeLess","page":"Introduction","title":"Manopt.StopWhenChangeLess","text":"StopWhenChangeLess <: StoppingCriterion\n\nstores a threshold when to stop looking at the norm of the change of the optimization variable from within a Options, i.e o.x. For the storage a StoreOptionsAction is used\n\nConstructor\n\nStopWhenChangeLess(ε[, a])\n\ninitialize the stopping criterion to a threshold ε using the StoreOptionsAction a, which is initialized to just store :x by default.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenCostLess","page":"Introduction","title":"Manopt.StopWhenCostLess","text":"StopWhenCostLess <: StoppingCriterion\n\nstore a threshold when to stop looking at the cost function of the optimization problem from within a Problem, i.e get_cost(p,o.x).\n\nConstructor\n\nStopWhenCostLess(ε)\n\ninitialize the stopping criterion to a threshold ε.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenGradientNormLess","page":"Introduction","title":"Manopt.StopWhenGradientNormLess","text":"StopWhenGradientNormLess <: StoppingCriterion\n\nstores a threshold when to stop looking at the norm of the gradient from within a GradientProblem.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"as well as the functions","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"get_reason\nget_stopping_criteria\nget_active_stopping_criteria","category":"page"},{"location":"solvers/index.html#Manopt.get_reason","page":"Introduction","title":"Manopt.get_reason","text":"get_reason(o)\n\nreturn the current reason stored within the StoppingCriterion from within the Options This reason is empty if the criterion has never been met.\n\n\n\n\n\nget_reason(c)\n\nreturn the current reason stored within a StoppingCriterion c. This reason is empty if the criterion has never been met.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.get_stopping_criteria","page":"Introduction","title":"Manopt.get_stopping_criteria","text":"get_stopping_criteria(c)\n\nreturn the array of internally stored StoppingCriterions for a StoppingCriterionSet c.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.get_active_stopping_criteria","page":"Introduction","title":"Manopt.get_active_stopping_criteria","text":"get_active_stopping_criteria(c)\n\nreturns all active stopping criteria, if any, that are within a StoppingCriterion c, and indicated a stop, i.e. their reason is nonempty. To be precise for a simple stopping criterion, this returns either an empty array if no stop is incated or the stopping criterion as the only element of an array. For a StoppingCriterionSet all internal (even nested) criteria that indicate to stop are returned.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"further stopping criteria might be available for individual Solvers.","category":"page"},{"location":"solvers/index.html#DecoratedSolvers-1","page":"Introduction","title":"Decorated Solvers","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The following decorators are available.","category":"page"},{"location":"solvers/index.html#DebugSolver-1","page":"Introduction","title":"Debug Solver","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The decorator to print debug during the iterations can be activated by decorating the Options with DebugOptions and implementing your own DebugActions. For example printing a gradient from the GradientDescentOptions is automatically available, as explained in the steepest_descent solver.","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Modules = [Manopt]\nPages   = [\"debug_solver.jl\"]","category":"page"},{"location":"solvers/index.html#Manopt.get_solver_result-Tuple{DebugOptions}","page":"Introduction","title":"Manopt.get_solver_result","text":"get_solver_result(o)\n\nReturn the final result after all iterations that is stored within the (modified during the iterations) Options o.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.initialize_solver!-Tuple{Problem,DebugOptions}","page":"Introduction","title":"Manopt.initialize_solver!","text":"initialize_solver!(p,o)\n\nInitialize the solver to the optimization Problem by initializing all values in the DebugOptionso.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.step_solver!-Tuple{Problem,DebugOptions,Any}","page":"Introduction","title":"Manopt.step_solver!","text":"step_solver!(p,o,iter)\n\nDo one iteration step (the iterth) for Problemp by modifying the values in the Optionso.options and print Debug.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.stop_solver!-Tuple{Problem,DebugOptions,Int64}","page":"Introduction","title":"Manopt.stop_solver!","text":"stop_solver!(p,o,i)\n\ndetermine whether the solver for Problem p and the DebugOptions o should stop at iteration i. If so, print all debug from :All and :Final.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#RecordSolver-1","page":"Introduction","title":"Record Solver","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The decorator to record certain values during the iterations can be activated by decorating the Options with RecordOptions and implementing your own RecordActions. For example recording the gradient from the GradientDescentOptions is automatically available, as explained in the steepest_descent solver.","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Modules = [Manopt]\nPages   = [\"record_solver.jl\"]","category":"page"},{"location":"solvers/index.html#Manopt.get_solver_result-Tuple{RecordOptions}","page":"Introduction","title":"Manopt.get_solver_result","text":"get_solver_result(o)\n\nReturn the final result after all iterations that is stored within the (modified during the iterations) Optionso.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.initialize_solver!-Tuple{Problem,RecordOptions}","page":"Introduction","title":"Manopt.initialize_solver!","text":"initialize_solver!(p,o)\n\nInitialize the solver to the optimization Problem by initializing the encapsulated options from within the RecordOptionso.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.step_solver!-Tuple{Problem,RecordOptions,Any}","page":"Introduction","title":"Manopt.step_solver!","text":"step_solver!(p,o,iter)\n\nDo one iteration step (the iterth) for Problemp by modifying the values in the Optionso.options and record the result(s).\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.stop_solver!-Tuple{Problem,RecordOptions,Int64}","page":"Introduction","title":"Manopt.stop_solver!","text":"stop_solver!(p,o,i)\n\ndetermine whether the solver for Problem p and the RecordOptions o should stop at iteration i. If so, do a (final) record to :All and :Stop.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Technical-Details-1","page":"Introduction","title":"Technical Details","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The main function a solver calls is","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"solve(p::Problem, o::Options)","category":"page"},{"location":"solvers/index.html#Manopt.solve-Tuple{Problem,Options}","page":"Introduction","title":"Manopt.solve","text":"solve(p,o)\n\nrun the solver implemented for the Problemp and the Optionso employing initialize_solver!, step_solver!, as well as the stop_solver! of the solver.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"which is a framework, that you in general should not change or redefine. It uses the following methods, which also need to be implemented on your own algorithm, if you want to provide one.","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"initialize_solver!\nstep_solver!\nget_solver_result\nstop_solver!(p::Problem, o::Options, i::Int)","category":"page"},{"location":"solvers/index.html#Manopt.initialize_solver!","page":"Introduction","title":"Manopt.initialize_solver!","text":"initialize_solver!(p,o)\n\nInitialize the solver to the optimization Problem by initializing all values in the Optionso.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.step_solver!","page":"Introduction","title":"Manopt.step_solver!","text":"step_solver!(p,o,iter)\n\nDo one iteration step (the iterth) for Problemp by modifying the values in the Options o.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.get_solver_result","page":"Introduction","title":"Manopt.get_solver_result","text":"get_solver_result(o)\n\nReturn the final result after all iterations that is stored within the (modified during the iterations) Options o.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.stop_solver!-Tuple{Problem,Options,Int64}","page":"Introduction","title":"Manopt.stop_solver!","text":"stop_solver!(p,o,i)\n\ndepending on the current Problem p, the current state of the solver stored in Options o and the current iterate i this function determines whether to stop the solver by calling the StoppingCriterion.\n\n\n\n\n\n","category":"method"},{"location":"list.html#Table-of-Contents,-Types-and-Functions-1","page":"Function Index","title":"Table of Contents, Types and Functions","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"This page lists all pages of this documentations, all available types and functions.","category":"page"},{"location":"list.html#Complete-List-of-Contents-1","page":"Function Index","title":"Complete List of Contents","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Depth = 3","category":"page"},{"location":"list.html#Available-Types-1","page":"Function Index","title":"Available Types","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Modules = [Manopt]\nOrder   = [:type]","category":"page"},{"location":"list.html#Solver-Functions-1","page":"Function Index","title":"Solver Functions","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Modules = [Manopt]\nPages = [\"plans/index.md\", \"solvers/index.md\"]","category":"page"},{"location":"list.html#Functions-1","page":"Function Index","title":"Functions","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Modules = [Manopt]\nPages = [\"functions/adjointDifferentials.md\", \"functions/costs.md\", \"functions/differentials.md\", \"functions/gradients.md\", \"functions/jacobiFields.md\", \"functions/proximalMaps.md\"]","category":"page"},{"location":"functions/costFunctions.html#CostFunctions-1","page":"cost functions","title":"Cost Functions","text":"","category":"section"},{"location":"functions/costFunctions.html#","page":"cost functions","title":"cost functions","text":"The following cost functions are available","category":"page"},{"location":"functions/costFunctions.html#","page":"cost functions","title":"cost functions","text":"Modules = [Manopt]\nPages   = [\"costs.jl\"]","category":"page"},{"location":"functions/costFunctions.html#Manopt.costIntrICTV12-Tuple{ManifoldsBase.Manifold,Any,Any,Any,Any,Any}","page":"cost functions","title":"Manopt.costIntrICTV12","text":"costIntrICTV12(M, f, u, v, α, β)\n\nCompute the intrinsic infimal convolution model, where the addition is replaced by a mid point approach and the two functions involved are costTV2 and costTV. The model reads\n\nE(uv) =\n  frac12sum_i  mathcal G\n    d_mathcal Mbigl(g(frac12v_iw_i)f_ibigr)\n  +alphabigl( betamathrmTV(v) + (1-beta)mathrmTV_2(w) bigr)\n\n\n\n\n\n","category":"method"},{"location":"functions/costFunctions.html#Manopt.costL2TV-NTuple{4,Any}","page":"cost functions","title":"Manopt.costL2TV","text":"costL2TV(M, f, α, x)\n\ncompute the ell^2-TV functional on the PowerManifold manifoldMfor given (fixed) dataf(onM), a nonnegative weightα, and evaluated atx(onM`), i.e.\n\nE(x) = d_mathcal M^2(fx) + alpha operatornameTV(x)\n\nSee also\n\ncostTV\n\n\n\n\n\n","category":"method"},{"location":"functions/costFunctions.html#Manopt.costL2TV2-Tuple{Manifolds.PowerManifold,Any,Any,Any}","page":"cost functions","title":"Manopt.costL2TV2","text":"costL2TV2(M, f, β, x)\n\ncompute the ell^2-TV2 functional on the PowerManifold manifold M for given data f, nonnegative parameter β, and evaluated at x, i.e.\n\nE(x) = d_mathcal M^2(fx) + betaoperatornameTV_2(x)\n\nSee also\n\ncostTV2\n\n\n\n\n\n","category":"method"},{"location":"functions/costFunctions.html#Manopt.costL2TVTV2-Tuple{Manifolds.PowerManifold,Any,Any,Any,Any}","page":"cost functions","title":"Manopt.costL2TVTV2","text":"costL2TVTV2(M, f, α, β, x)\n\ncompute the ell^2-TV-TV2 functional on the PowerManifold manifold M for given (fixed) data f (on M), nonnegative weight α, β, and evaluated at x (on M), i.e.\n\nE(x) = d_mathcal M^2(fx) + alphaoperatornameTV(x)\n  + betaoperatornameTV_2(x)\n\nSee also\n\ncostTV, costTV2\n\n\n\n\n\n","category":"method"},{"location":"functions/costFunctions.html#Manopt.costTV","page":"cost functions","title":"Manopt.costTV","text":"costTV(M,x [,p=2,q=1])\n\nCompute the operatornameTV^p functional for data xon the PowerManifold manifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i denote the forward neighbors, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I_i = i+e_j j=1ldotskcap mathcal G. The formula reads\n\nE^q(x) = sum_i  mathcal G\n  bigl( sum_j   mathcal I_i d^p_mathcal M(x_ix_j) bigr)^qp\n\nSee also\n\n∇TV, prox_TV\n\n\n\n\n\n","category":"function"},{"location":"functions/costFunctions.html#Manopt.costTV-Union{Tuple{T}, Tuple{ManifoldsBase.Manifold,Tuple{T,T}}, Tuple{ManifoldsBase.Manifold,Tuple{T,T},Int64}} where T","page":"cost functions","title":"Manopt.costTV","text":"costTV(M, x, p)\n\nCompute the operatornameTV^p functional for a tuple pT of pointss on a Manifold M, i.e.\n\nE(x_1x_2) = d_mathcal M^p(x_1x_2) quad x_1x_2  mathcal M\n\nSee also\n\n∇TV, prox_TV\n\n\n\n\n\n","category":"method"},{"location":"functions/costFunctions.html#Manopt.costTV2","page":"cost functions","title":"Manopt.costTV2","text":"costTV2(M,x [,p=1])\n\ncompute the operatornameTV_2^p functional for data x on the PowerManifold manifoldmanifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i^pm denote the forward and backward neighbors, respectively, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I^pm_i = ipm e_j j=1ldotskcap mathcal I. The formula then reads\n\nE(x) = sum_i  mathcal I j_1   mathcal I^+_i j_2   mathcal I^-_i\nd^p_mathcal M(c_i(x_j_1x_j_2) x_i)\n\nwhere c_i(cdotcdot) denotes the mid point between its two arguments that is nearest to x_i.\n\nSee also\n\n∇TV2, prox_TV2\n\n\n\n\n\n","category":"function"},{"location":"functions/costFunctions.html#Manopt.costTV2-Union{Tuple{T}, Tuple{MT}, Tuple{MT,Tuple{T,T,T}}, Tuple{MT,Tuple{T,T,T},Any}} where T where MT<:ManifoldsBase.Manifold","page":"cost functions","title":"Manopt.costTV2","text":"costTV2(M,(x1,x2,x3) [,p=1])\n\nCompute the operatornameTV_2^p functional for the 3-tuple of points (x1,x2,x3)on the Manifold M. Denote by\n\n  mathcal C = bigl c   mathcal M   g(tfrac12x_1x_3) text for some geodesic gbigr\n\nthe set of mid points between x_1 and x_3. Then the functionr reads\n\nd_2^p(x_1x_2x_3) = min_c  mathcal C d_mathcal M(cx_2)\n\nSee also\n\n∇TV2, prox_TV2\n\n\n\n\n\n","category":"method"},{"location":"index.html#Welcome-to-Manopt.jl-1","page":"Home","title":"Welcome to Manopt.jl","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"CurrentModule = Manopt","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"Manopt.Manopt","category":"page"},{"location":"index.html#Manopt.Manopt","page":"Home","title":"Manopt.Manopt","text":"Manopt.jl – Optimization on Manifolds in Julia.\n\n\n\n\n\n","category":"module"},{"location":"index.html#","page":"Home","title":"Home","text":"For a function fcolonmathcal M to mathbb R defined on a Riemannian manifold mathcal M we aim to solve","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"operatorname*argmin_x  mathcal M f(x)","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"or in other words: find the point x on the manifold, where f reaches its minimal function value.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"Manopt.jl provides a framework for optimization on manifolds. Based on Manopt and MVIRT, both implemented in Matlab, this toolbox provide an easy access to optimization methods on manifolds for Julia, including example data and visualization methods.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"If you want to delve right into Manopt.jl check out the Getting Started: Optimize! tutorial.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"Manopt.jl makes it easy to use an algorithm for your favorite manifold as well as a manifold for your favorite algorithm. It already provides many manifolds and algorithms, which can easily be enhanced, for example to record certain data or display information throughout iterations.","category":"page"},{"location":"index.html#Main-Features-1","page":"Home","title":"Main Features","text":"","category":"section"},{"location":"index.html#Functions-on-Manifolds-1","page":"Home","title":"Functions on Manifolds","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"Several functions are available, implemented on an arbitrary manifold, cost functions, differentials, and gradients as well as proximal maps, but also several jacobi Fields and their adjoints.","category":"page"},{"location":"index.html#Optimization-Algorithms-(Solvers)-1","page":"Home","title":"Optimization Algorithms (Solvers)","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"For every optimization algorithm, a solver is implemented based on a Problem that describes the problem to solve and its Options that set up the solver, store interims values. Together they form a plan.","category":"page"},{"location":"index.html#Visualization-1","page":"Home","title":"Visualization","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"To visualize and interpret results, Manopt.jl aims to provide both easy plot functions as well as exports. Furthermore a system to get debug during the iterations of an algorithms as well as record capabilities, i.e. to record a specified tuple of values per iteration, most prominently RecordCost and RecordIterate. Take a look at the Getting Started: Optimize! tutorial how to easily activate this.","category":"page"},{"location":"index.html#Manifolds-1","page":"Home","title":"Manifolds","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"This project is build upon ManifoldsBase.jl, a generic interface to implement manifolds. Certain functions are extended for specific manifolds from Manifolds.jl, but all other manifolds from that package can be used here, too.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"The notation in the documentation aims to follow the same notation from these packages.","category":"page"},{"location":"index.html#Literature-1","page":"Home","title":"Literature","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"If you want to get started with manifolds, one book is [do Carmo, 1992], and if you want do directly dive into optimization on manifolds, my favourite reference is [Absil, Mahony, Sepulchre, 2008], which is also available online for free.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"<ul>\n<li id=\"AbsilMahonySepulchre2008\">\n    [<a>Absil, Mahony, Sepulchre, 2008</a>]\n    P.-A. Absil, R. Mahony and R. Sepulchre,\n    <emph>Optimization Algorithms on Matrix Manifolds</emph>,\n    Princeton University Press, 2008,\n    doi: <a href=\"https://doi.org/10.1515/9781400830244\">10.1515/9781400830244</a>,\n    <a href=\"http://press.princeton.edu/chapters/absil/\">open access</a>.\n</li>\n<li id=\"doCarmo1992\">\n    [<a>doCarmo, 1992</a>]\n    M. P. do Carmo,\n    <emph>Riemannian Geometry</emph>,\n    Birkhäuser Boston, 1992,\n    ISBN: 0-8176-3490-8.\n</li>\n</ul>","category":"page"},{"location":"functions/gradients.html#GradientFunctions-1","page":"Gradients","title":"Gradients","text":"","category":"section"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"For a function fcolonmathcal Mtomathbb R the Riemannian gradient nabla f(x) at xmathcal M is given by the unique tangent vector fulfilling","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"langle nabla f(x) xirangle_x = D_xfxiquad\nforall xi  T_xmathcal M","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"where D_xfxi denotes the differential of f at x with respect to the tangent direction (vector) xi or in other words the directional derivative.","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"This page collects the available gradients.","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"Modules = [Manopt]\nPages   = [\"gradients.jl\"]","category":"page"},{"location":"functions/gradients.html#Manopt.forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{Manifolds.PowerManifold{𝔽,TM,TSize,TPR},Any}} where TPR where TSize where TM where 𝔽","page":"Gradients","title":"Manopt.forward_logs","text":"ξ = forward_logs(M,x)\n\ncompute the forward logs F (generalizing forward differences) orrucirng, in the power manifold array, the function\n\nF_i(x) = sum_j  mathcal I_i log_x_i x_jquad i    mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOuput\n\nξ – resulting tangent vector in T_xmathcal M representing the logs, where mathcal N is thw power manifold with the number of dimensions added to size(x).\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.∇TV","page":"Gradients","title":"Manopt.∇TV","text":"ξ = ∇TV(M,λ,x,[p])\n\nCompute the (sub)gradient partial F of all forward differences orrucirng, in the power manifold array, i.e. of the function\n\nF(x) = sum_isum_j  mathcal I_i d^p(x_ix_j)\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOuput\n\nξ – resulting tangent vector in T_xmathcal M.\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.∇TV-Union{Tuple{T}, Tuple{MT}, Tuple{MT,Tuple{T,T}}, Tuple{MT,Tuple{T,T},Any}} where T where MT<:ManifoldsBase.Manifold","page":"Gradients","title":"Manopt.∇TV","text":"∇TV(M,(x,y),[p=1])\n\ncompute the (sub) gradient of frac1pd^p_mathcal M(xy) with respect to both x and y.\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.∇TV2","page":"Gradients","title":"Manopt.∇TV2","text":"∇TV2(M,q [,p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(x_1x_2x_3) with respect to all x_1x_2x_3 occuring along any array dimension in the point x, where M is the corresponding PowerManifold.\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.∇TV2-Union{Tuple{MT}, Tuple{MT,Any}, Tuple{MT,Any,Number}} where MT<:ManifoldsBase.Manifold","page":"Gradients","title":"Manopt.∇TV2","text":"∇TV2(M,(x,y,z),p)\n\ncomputes the (sub) gradient of frac1pd_2^p(xyz) with respect to x, y, and z, where d_2 denotes the second order absolute difference using the mid point model, i.e. let\n\n  mathcal C = bigl c   mathcal M   g(tfrac12x_1x_3) text for some geodesic gbigr\n\ndenote the mid points between x and z on the manifold mathcal M. Then the absolute second order difference is defined as\n\nd_2(xyz) = min_c  mathcal C_xz d(cy)\n\nWhile the (sub)gradient with respect to y is easy, the other two require the evaluation of an adjoint_Jacobi_field. See Illustration of the Gradient of a Second Order Difference for its derivation.\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.∇distance","page":"Gradients","title":"Manopt.∇distance","text":"∇distance(M,y,x[, p=2])\n\ncompute the (sub)gradient of the distance (squared)\n\nf(x) = frac12 d^p_mathcal M(xy)\n\nto a fixed point y on the manifold M and p is an integer. The gradient reads\n\n  nabla f(x) = -d_mathcal M^p-2(xy)log_xy\n\nfor pneq 1 or xneq  y. Note that for the remaining case p=1, x=y the function is not differentiable. In this case, the function returns the corresponding zero tangent vector, since this is an element of the subdifferential.\n\nOptional\n\np – (2) the exponent of the distance,  i.e. the default is the squared distance\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.∇intrinsic_infimal_convolution_TV12-Union{Tuple{mT}, Tuple{mT,Any,Any,Any,Any,Any}} where mT<:ManifoldsBase.Manifold","page":"Gradients","title":"Manopt.∇intrinsic_infimal_convolution_TV12","text":"∇u,⁠∇v = ∇intrinsic_infimal_convolution_TV12(M,f,u,v,α,β)\n\ncompute (sub)gradient of the intrinsic infimal convolution model using the mid point model of second order differences, see costTV2, i.e. for some f  mathcal M on a PowerManifold manifold mathcal M this function computes the (sub)gradient of\n\nE(uv) =\nfrac12sum_i  mathcal G d_mathcal M(g(frac12v_iw_i)f_i)\n+ alpha\nbigl(\nbetamathrmTV(v) + (1-beta)mathrmTV_2(w)\nbigr)\n\nwhere both total variations refer to the intrinsic ones, ∇TV and ∇TV2, respectively.\n\n\n\n\n\n","category":"method"},{"location":"solvers/NelderMead.html#NelderMeadSolver-1","page":"Nelder–Mead","title":"Nelder Mead Method","text":"","category":"section"},{"location":"solvers/NelderMead.html#","page":"Nelder–Mead","title":"Nelder–Mead","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/NelderMead.html#","page":"Nelder–Mead","title":"Nelder–Mead","text":"    NelderMead","category":"page"},{"location":"solvers/NelderMead.html#Manopt.NelderMead","page":"Nelder–Mead","title":"Manopt.NelderMead","text":"NelderMead(M, F [, p])\n\nperform a nelder mead minimization problem for the cost funciton F on the manifold M. If the initial population p is not given, a random set of points is chosen.\n\nThis algorithm is adapted from the Euclidean Nelder-Mead method, see https://en.wikipedia.org/wiki/Nelder–Mead_method and http://www.optimization-online.org/DB_FILE/2007/08/1742.pdf.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\npopulation – (n+1 random_point(M)) an initial population of n+1 points, where n is the dimension of the manifold M.\n\nOptional\n\nstoppingCriterion – (StopAfterIteration(2000)) a StoppingCriterion\nretraction – (exp) a retraction(M,x,ξ) to use.\nα – (1.) reflection parameter (alpha  0)\nγ – (2.) expansion parameter (gamma)\nρ – (1/2) contraction parameter, 0  rho leq frac12,\nσ – (1/2) shrink coefficient, 0  sigma leq 1\n\nand the ones that are passed to decorate_options for decorators.\n\nOutput\n\neither x the last iterate or the complete options depending on the optional keyword returnOptions, which is false by default (hence then only x is returned).\n\n\n\n\n\n","category":"function"},{"location":"solvers/NelderMead.html#Options-1","page":"Nelder–Mead","title":"Options","text":"","category":"section"},{"location":"solvers/NelderMead.html#","page":"Nelder–Mead","title":"Nelder–Mead","text":"    NelderMeadOptions","category":"page"},{"location":"solvers/NelderMead.html#Manopt.NelderMeadOptions","page":"Nelder–Mead","title":"Manopt.NelderMeadOptions","text":"NelderMeadOptions <: Options\n\nDescribes all parameters and the state of a Nealer-Mead heuristic based optimization algorithm.\n\nFields\n\nThe naming of these parameters follows the Wikipedia article of the Euclidean case. The default is given in brackets, the required value range after the description\n\npopulation – an Array{point,1} of n+1 points x_i, i=1ldotsn+1, where n is the dimension of the manifold.\nstoppingCriterion – (StopAfterIteration(2000)) a StoppingCriterion\nretraction – (exp) the rectraction to use\nα – (1.) reflection parameter (alpha  0)\nγ – (2.) expansion parameter (gamma0)\nρ – (1/2) contraction parameter, 0  rho leq frac12,\nσ – (1/2) shrink coefficient, 0  sigma leq 1\nx – (p[1]) - a field to collect the current best value\n\nConstructors\n\nNelderMead(M,stop, retr; α=1. , γ=2., ρ=1/2, σ=1/2)\n\nconstruct a Nelder-Mead Option with a set of dimension(M)+1 random points.\n\nNelderMead(p, stop retr; α=1. , γ=2., ρ=1/2, σ=1/2)\n\nconstruct a Nelder-Mead Option with a set p of points\n\n\n\n\n\n","category":"type"}]
}
