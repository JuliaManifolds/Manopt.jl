---
title: "üèîÔ∏è Get started: optimize."
author: Ronny Bergmann
---

This tutorial both introduces the basics of optimisation on manifolds as well as
how to use [`Manopt.jl`](https://manoptjl.org) to perform optimisation on manifolds in [Julia](https://julialang.org).

For more theoretical background, see for example [doCarmo:1992](@cite) for an introduction to Riemannian manifolds
and [AbsilMahonySepulchre:2008](@cite) or [Boumal:2023](@cite) to read more about optimisation thereon.

Let $\mathcal M$ denote a (``[Riemannian manifold](@extref `ManifoldsBase.AbstractManifold`)``{=commonmark}
and let $f:  \mathcal M ‚Üí ‚Ñù$ be a cost function.
The aim is to determine or obtain a point $p^*$ where $f$ is _minimal_ or in other words $p^*$ is a _minimizer_ of $f$.

This can also be written as

```math
    \operatorname*{arg\,min}_{p ‚àà \mathcal M} f(p)
```

where the aim is to compute the minimizer $p^*$ numerically.
As an example, consider the generalisation of the [(arithemtic) mean](https://en.wikipedia.org/wiki/Arithmetic_mean).
In the Euclidean case with $d‚àà\mathbb N$, that is for $n‚àà\mathbb N$ data points $y_1,\ldots,y_n ‚àà ‚Ñù^d$ the mean

```math
  \frac{1}{n}\sum_{i=1}^n y_i
```

can not be directly generalised to data $q_1,\ldots,q_n ‚àà \mathcal M$, since on a manifold there is no addition available.
But the mean can also be characterised as the following minimizer

```math
  \operatorname*{arg\,min}_{x‚àà‚Ñù^d} \frac{1}{2n}\sum_{i=1}^n \lVert x - y_i\rVert^2
```

and using the Riemannian distance $d_\mathcal M$, this can be written on Riemannian manifolds, which is the so called _Riemannian Center of Mass_ [Karcher:1977](@cite)

```math
  \operatorname*{arg\,min}_{p‚àà\mathcal M}
  \frac{1}{2n} \sum_{i=1}^n d_{\mathcal M}^2(p, q_i)
```

Fortunately the gradient can be computed and is

```math
 \frac{1}{n} \sum_{i=1}^n -\log_p q_i
```

## Loading the necessary packages

```{julia}
#| echo: false
#| code-fold: true
#| output: false
using Pkg;
cd(@__DIR__)
Pkg.activate("."); # for reproducibility use the local tutorial environment.
```

Let's assume you have already installed both `Manopt.jl` and `Manifolds.jl` in Julia (using for example `using Pkg; Pkg.add(["Manopt", "Manifolds"])`).
Then we can get started by loading both packages as well as `Random.jl` for persistency in this tutorial.

```{julia}
using Manopt, Manifolds, Random, LinearAlgebra, ManifoldDiff
using ManifoldDiff: grad_distance, prox_distance
Random.seed!(42);
```

Now assume we are on the [Sphere](https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/sphere.html)
$\mathcal M = \mathbb S^2$ and we generate some random points ‚Äúaround‚Äù some initial point $p$

```{julia}
n = 100
œÉ = œÄ / 8
M = Sphere(2)
p = 1 / sqrt(2) * [1.0, 0.0, 1.0]
data = [exp(M, p,  œÉ * rand(M; vector_at=p)) for i in 1:n];
```

Now we can define the cost function $f$ and its (Riemannian) gradient $\operatorname{grad} f$
for the Riemannian center of mass:

```{julia}
f(M, p) = sum(1 / (2 * n) * distance.(Ref(M), Ref(p), data) .^ 2)
grad_f(M, p) = sum(1 / n * grad_distance.(Ref(M), data, Ref(p)));
```

and just call [`gradient_descent`](https://manoptjl.org/stable/solvers/gradient_descent/).
For a first start, we do not have to provide more than the manifold, the cost, the gradient,
and a starting point, which we just set to the first data point

```{julia}
m1 = gradient_descent(M, f, grad_f, data[1])
```

In order to get more details, we further add the `debug=` keyword argument, which
act as a [decorator pattern](https://en.wikipedia.org/wiki/Decorator_pattern).

This way we can easily specify a certain debug to be printed.
The goal is to get an output of the form

```{shell}
# i | Last Change: [...] | F(x): [...] |
```

but where we also want to fix the display format for the change and the cost
numbers (the `[...]`) to have a certain format. Furthermore, the reason why the solver stopped should be printed at the end

These can easily be specified using either a Symbol when using the default format for numbers, or a tuple of a symbol and a format-string in the `debug=` keyword that is available for every solver.
We can also,¬†for illustration reasons,¬†just look at the first 6 steps by setting a [`stopping_criterion=`](https://manoptjl.org/stable/plans/stopping_criteria/)

```{julia}
m2 = gradient_descent(M, f, grad_f, data[1];
    debug=[:Iteration,(:Change, "|Œîp|: %1.9f |"),
        (:Cost, " F(x): %1.11f | "), "\n", :Stop],
    stopping_criterion = StopAfterIteration(6)
  )
```

See [here](https://manoptjl.org/stable/plans/debug/#Manopt.DebugActionFactory-Tuple{Symbol}) for the list of available symbols.

```{=commonmark}
!!! info "Technical Detail"
    The `debug=` keyword is actually a list of [`DebugActions`](https://manoptjl.org/stable/plans/debug/#Manopt.DebugAction) added to every iteration, allowing you to write your own ones even. Additionally, `:Stop` is an action added to the end of the solver to display the reason why the solver stopped.
```

The default stopping criterion for [`gradient_descent`](https://manoptjl.org/stable/solvers/gradient_descent/) is, to either stop when the gradient is small (`<1e-9`) or a max number of iterations is reached (as a fallback).
Combining stopping-criteria can be done by `|` or `&`.
We further pass a number `25` to `debug=` to only an output every `25`th iteration:

```{julia}
m3 = gradient_descent(M, f, grad_f, data[1];
    debug=[:Iteration,(:Change, "|Œîp|: %1.9f |"),
        (:Cost, " F(x): %1.11f | "), "\n", :Stop, 25],
    stopping_criterion = StopWhenGradientNormLess(1e-14) |¬†StopAfterIteration(400),
)
```

We can finally use another way to determine the stepsize, for example
a little more expensive [`ArmijoLineSeach`](https://manoptjl.org/stable/plans/stepsize/#Manopt.ArmijoLinesearch) than the default [stepsize](https://manoptjl.org/stable/plans/stepsize/) rule used on the Sphere.

```{julia}
m4 = gradient_descent(M, f, grad_f, data[1];
    debug=[:Iteration,(:Change, "|Œîp|: %1.9f |"),
        (:Cost, " F(x): %1.11f | "), "\n", :Stop, 2],
      stepsize = ArmijoLinesearch(; contraction_factor=0.999, sufficient_decrease=0.5),
    stopping_criterion = StopWhenGradientNormLess(1e-14) |¬†StopAfterIteration(400),
)
```

Then we reach approximately the same point as in the previous run, but in far less steps

```{julia}
[f(M, m3)-f(M,m4), distance(M, m3, m4)]
```

## Using the tutorial mode

Since a few things on manifolds are a bit different from (classical) Euclidean optimization, `Manopt.jl` has a mode to warn about a few pitfalls.

It can be set using

```{julia}
#| warning: true
Manopt.set_parameter!(:Mode, "Tutorial")
```

to activate these. Continuing from the example before, one might argue, that the
minimizer of $f$ does not depend on the scaling of the function.
In theory this is of course also the case on manifolds, but for the optimizations there is a caveat. When we define the Riemannian mean without the scaling

```{julia}
f2(M, p) = sum(1 / 2 * distance.(Ref(M), Ref(p), data) .^ 2)
grad_f2(M, p) = sum(grad_distance.(Ref(M), data, Ref(p)));
```

And we consider the gradient at the starting point in norm

```{julia}
norm(M, data[1], grad_f2(M, data[1]))
```

On the sphere, when we follow a geodesic, we ‚Äúreturn‚Äù to the start point after length $2œÄ$.
If we ‚Äúland‚Äù short before the starting point due to a gradient of length just shy of $2œÄ$,
the line search would take the gradient direction (and not the negative gradient direction)
as a start. The line search is still performed, but in this case returns a much too small,
maybe even nearly zero step size.

In other words, we have to be careful that the optimisation stays a ‚Äúlocal‚Äù argument we use.

This is also warned for in `"Tutorial"` mode. Calling

```{julia}
#| warning: true
mX = gradient_descent(M, f2, grad_f2, data[1])
```

So just by chance it seems we still got nearly the same point as before, but
when we for example look when this one stops, that is takes more steps.

```{julia}
#| warning: true
gradient_descent(M, f2, grad_f2, data[1], debug=[:Stop]);
```


This also illustrates one way to deactivate the hints, namely by overwriting the `debug=`
keyword, that in `Tutorial` mode contains additional warnings.
The other option is to globally reset the `:Mode` back to
```{julia}
#| warning: true
Manopt.set_parameter!(:Mode, "")
```

## Example 2: computing the median of symmetric positive definite matrices

For the second example let's consider the manifold of [$3 √ó 3$ symmetric positive definite matrices](https://juliamanifolds.github.io/Manifolds.jl/stable/manifolds/symmetricpositivedefinite.html) and again 100 random points

```{julia}
N = SymmetricPositiveDefinite(3)
m = 100
œÉ = 0.005
q = Matrix{Float64}(I, 3, 3)
data2 = [exp(N, q, œÉ * rand(N; vector_at=q)) for i in 1:m];
```

Instead of the mean, let's consider a non-smooth optimisation task:
the median can be generalized to Manifolds as the minimiser of the sum of distances,
see [Bacak:2014](@cite).
We define

```{julia}
g(N, q) = sum(1 / (2 * m) * distance.(Ref(N), Ref(q), data2))
```

Since the function is non-smooth, we can not use a gradient-based approach.
But since for every summand the [proximal map](https://manoptjl.org/stable/functions/proximal_maps/#Manopt.prox_distance) is available, we can use the
[cyclic proximal point algorithm (CPPA)](https://manoptjl.org/stable/solvers/cyclic_proximal_point/). We hence define the vector of proximal maps as

```{julia}
proxes_g = Function[(N, Œª, q) -> prox_distance(N, Œª / m, di, q, 1) for di in data2];
```

Besides also looking at a some debug prints, we can also easily record these values. Similarly to `debug=`, `record=` also accepts Symbols, see list [here](https://manoptjl.org/stable/plans/record/#Manopt.RecordFactory-Tuple{AbstractManoptSolverState,%20Vector}), to indicate things to record. We further set `return_state` to true to obtain not just the (approximate) minimizer.

```{julia}
res = cyclic_proximal_point(N, g, proxes_g, data2[1];
  debug=[:Iteration," | ",:Change," | ",(:Cost, "F(x): %1.12f"),"\n", 1000, :Stop,
        ],
        record=[:Iteration, :Change, :Cost, :Iterate],
        return_state=true,
    );
```

```{=commonmark}
!!! note "Technical Detail"
    The recording is realised by [`RecordActions`](https://manoptjl.org/stable/plans/record/#Manopt.RecordAction) that are (also) executed at every iteration. These can also be individually implemented and added to the `record=` array instead of symbols.
```

First, the computed median can be accessed as

```{julia}
median = get_solver_result(res)
```

but we can also look at the recorded values. For simplicity (of output), lets just look
at the recorded values at iteration 42

```{julia}
get_record(res)[42]
```

But we can also access whole series and see that the cost does not decrease that fast; actually, the CPPA might converge relatively slow. For that we can for
example access the `:Cost` that was recorded every `:Iterate` as well as the (maybe a little boring) `:Iteration`-number in a semi-log-plot.

```{julia}
x = get_record(res, :Iteration, :Iteration)
y = get_record(res, :Iteration, :Cost)
using Plots
plot(x,y,xaxis=:log, label="CPPA Cost")
```

## Technical details

This tutorial is cached. It was last run on the following package versions.

```{julia}
#| code-fold: true
using Pkg
Pkg.status()
```
```{julia}
#| code-fold: true
using Dates
now()
```

## Literature

````{=commonmark}
```@bibliography
Pages = ["Optimize.md"]
Canonical=false
```
````