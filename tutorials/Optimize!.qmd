---
title: "Get Started: Optimize!"
---

In this tutorial, we will both introduce the basics of optimisation on manifolds as well as
how to use [`Manopt.jl`](https://manoptjl.org) to perform optimisation on manifolds in [Julia](https://julialang.org).

For more theoretical background, see e.g. [@doCarmo1992] for an introduction to Riemannian manifolds
and [@AbsilMahonySepulchre2008] or [@Boumal2023] to read more about optimisation thereon.

Let $\mathcal M$ denote a [Riemannian manifold](https://juliamanifolds.github.io/Manifolds.jl/stable/interface.html#ManifoldsBase.Manifold)
and let $f\colon \mathcal M → ℝ$ be a cost function.
We aim to compute a point $p^*$ where $f$ is _minimal_ or in other words $p^*$ is a _minimizer_ of $f$.

We also write this as

```math
    \operatorname*{arg\,min}_{p ∈ \mathcal M} f(p)
```

and would like to find $p^*$ numerically.
As an example we take the generalisation of the [(arithemtic) mean](https://en.wikipedia.org/wiki/Arithmetic_mean).
In the Euclidean case with$d\in\mathbb N$, that is for $n\in \mathbb N$ data points $y_1,\ldots,y_n \in \mathbb R^d$ the mean

```math
  \sum_{i=1}^n y_i
```


can not be directly generalised to data $q_1,\ldots,q_n$, since on a manifold we do not have an addition.
But the mean can also be charcterised as

```math
  \operatorname*{arg\,min}_{x\in\mathbb R^d} \frac{1}{2n}\sum_{i=1}^n \lVert x - y_i\rVert^2
```

and using the Riemannian distance $d_\mathcal M$, this can be written on Riemannian manifolds. We obtain the _Riemannian Center of Mass_ [@Karcher1977]

```math
  \operatorname*{arg\,min}_{p\in\mathbb R^d}
  \frac{1}{2n} \sum_{i=1}^n d_{\mathcal M}^2(p, q_i)
```

Fortunately the gradient can be computed and is

```math
  \operatorname*{arg\,min}_{p\in\mathbb R^d} \frac{1}{n} \sum_{i=1}^n -\log_p q_i
```

## Loading the necessary packages

```{julia}
#| echo: false
#| code-fold: true
#| output: false
using Pkg;
Pkg.activate("."); # for reproducibility use the local tutorial environment.
```

Let's assume you have already installed both Manotp and Manifolds in Julia (using e.g. `using Pkg; Pkg.add(["Manopt", "Manifolds"])`).
Then we can get started by loading both packages – and `Random` for persistency in this tutorial.

```{julia}
using Manopt, Manifolds, Random
Random.seed!(42);
```

Now assume we are on the [Sphere](https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/sphere.html)
$\mathcal M = \mathbb S^2$ and we generate some random points “around” some initial point $p$

```{julia}
    n = 100
    σ = π / 8
    M = Sphere(2)
    p = 1 / sqrt(2) * [1.0, 0.0, 1.0]
    data = [exp(M, p,  σ * rand(M; vector_at=p)) for i in 1:n];
```

Now we can define the cost function $f$ and its (Riemannian) gradient $\operatorname{grad} f$
for the Riemannian center of mass:

```{julia}
f(M, p) = sum(1 / (2 * n) * distance.(Ref(M), Ref(p), data) .^ 2)
grad_f(M, p) = sum(1 / n * grad_distance.(Ref(M), data, Ref(p)));
```

and just call [`gradient_descent`](https://manoptjl.org/stable/solvers/gradient_descent/).
For a first start, we do not have to provide more than the manifold, the cost, the gradient,
and a startig point, which we just set to the first data point

```{julia}
m1 = gradient_descent(M, f, grad_f, data[1])
```

This is nice as a first run and agrees with the [`mean`](https://juliamanifolds.github.io/Manifolds.jl/stable/features/statistics.html#Statistics.mean-Tuple{AbstractManifold,%20Vararg{Any}})

```{julia}
m2 = mean(M, data)
distance(M, m1, m2)
```


On the other hand we do not get much insight into the run of the solber.
We could add some debug output first

```{julia}
m1 = gradient_descent(
  M, f, grad_f, data[1];
  debug = [:Iteration, " ", :Cost, " | ", :Change, :Stop, "\n"],
  )
```



## Literature

::: {#refs}
:::