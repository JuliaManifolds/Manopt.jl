---
title: "Optimize on your own manifold"
author: "Ronny Bergmann"
---

````{=commonmark}
```@meta
CurrentModule = Manopt
```
````

When you have used a few solvers from [`Manopt.jl`](https://manoptjl.org/) for example like in the opening
tutorial [üèîÔ∏è Get started: optimize!](https://manoptjl.org/stable/tutorials/Optimize!.html)
and also familiarized yourself with how to work with manifolds in general at
[üöÄ Get Started with `Manifolds.jl`](https://juliamanifolds.github.io/Manifolds.jl/stable/tutorials/getstarted.html),
you might come across the point that you want to
``[implementing a manifold](@extref :doc:`tutorials/implement-a-manifold`)``{=commonmark}
yourself and use it within [`Manopt.jl`](https://manoptjl.org/).
A challenge might be, which functions are necessary, since the overall interface of ``[ManifoldsBase.jl](@extref ManifoldsBase :doc:`index`)``{=commonmark}
is maybe not completely necessary.

This tutorial aims to help you through these steps to implement necessary parts of a manifold
to get started with the [solver](../solvers/index.md) you have in mind.

## An example problem

We get started by loading the packages we need.

```{julia}
#| echo: false
#| code-fold: true
#| output: false
using Pkg;
cd(@__DIR__)
Pkg.activate("."); # for reproducibility use the local tutorial environment.
Pkg.develop(path="../") # a trick to work on the local dev version
```

```{julia}
#| output: false
using LinearAlgebra, Manifolds, ManifoldsBase, Random
using Manopt
Random.seed!(42)
```

```{julia}
#| echo: false
#| code-fold: true
#| output: false
# to keep the output and usage simple let's deactivate tutorial mode here
Manopt.set_parameter!(:Mode, "None")
```

We also define the same manifold as in
the ``[implementing a manifold](@extref :doc:`tutorials/implement-a-manifold`)``{=commonmark}
tutorial.

```{julia}
#| output: false
"""
    ScaledSphere <: AbstractManifold{‚Ñù}

Define a sphere of fixed radius

# Fields

* `dimension` dimension of the sphere
* `radius` the radius of the sphere

# Constructor

    ScaledSphere(dimension,radius)

Initialize the manifold to a certain `dimension` and `radius`,
which by default is set to `1.0`
"""
struct ScaledSphere <: AbstractManifold{‚Ñù}
    dimension::Int
    radius::Float64
end
```

We would like to compute a mean and/or median similar to [üèîÔ∏è Get started: optimize!](https://manoptjl.org/stable/tutorials/Optimize!.html).
For given a set of points $q_1,\ldots,q_n$ we want to compute [Karcher:1977](@cite)

```math
  \operatorname*{arg\,min}_{p‚àà\mathcal M}
  \frac{1}{2n} \sum_{i=1}^n d_{\mathcal M}^2(p, q_i)
```

On the `ScaledSphere` we just defined.
We define a few parameters first

```{julia}
d = 5  # dimension of the sphere - embedded in R^{d+1}
r = 2.0 # radius of the sphere
N = 100 # data set size

M = ScaledSphere(d,r)
```


 If we generate a few points

```{julia}
#| output : false
# generate 100 points around the north pole
pts = [ [zeros(d)..., M.radius] .+ 0.5.*([rand(d)...,0.5] .- 0.5) for _=1:N]
# project them onto the r-sphere
pts = [ r/norm(p) .* p for p in pts]
```

Then, before starting with optimization, we need the distance on the manifold,
to define the cost function, as well as the logarithmic map to defined the gradient.
For both, we here use the ‚Äúlazy‚Äù approach of using the [Sphere](https://juliamanifolds.github.io/Manifolds.jl/stable/manifolds/sphere.html) as a fallback.
Finally, we have to provide information about how points and tangent vectors are stored on the
manifold by implementing their ``[`representation_size`](@extref `ManifoldsBase.representation_size-Tuple{AbstractManifold}`)``{=commonmark} function, which is often required when allocating memory.
While we could

```{julia}
#| output : false
import ManifoldsBase: distance, log, representation_size
function distance(M::ScaledSphere, p, q)
    return M.radius * distance(Sphere(M.dimension), p ./ M.radius, q ./ M.radius)
end
function log(M::ScaledSphere, p, q)
    return M.radius * log(Sphere(M.dimension), p ./ M.radius, q ./ M.radius)
end
representation_size(M::ScaledSphere) = (M.dimension+1,)
```

## Define the cost and gradient

```{julia}
#| output : false
f(M, q) = sum(distance(M, q, p)^2 for p in pts)
grad_f(M,q) = sum( - log(M, q, p) for p in pts)
```

## Defining the necessary functions to run a solver

The documentation usually lists the necessary functions in a
section ‚ÄúTechnical Details‚Äù close to the end of the documentation of a solver,
for our case that is [The gradient descent's Technical Details](https://manoptjl.org/stable/solvers/gradient_descent.html#Technical-Details),

They list all details, but we can start even step by step here if we are a bit careful.

### A retraction

We first implement a ``[retract](@extref ManifoldsBase :doc:`retractions`)``{=commonmark}ion. Informally,¬†given a current point and a direction to ‚Äúwalk into‚Äù we need a function that performs that walk.
Since we take an easy one that just projects onto
the sphere, we use the ``[`ProjectionRetraction`](@extref `ManifoldsBase.ProjectionRetraction`)``{=commonmark} type.
To be precise, we have to implement the ``[in-place variant](@extref ManifoldsBase `inplace-and-noninplace`)``{=commonmark} ``[`retract_project!`](@extref `ManifoldsBase.inverse_retract_project!-Tuple{AbstractManifold, Any, Any, Any}`)``{=commonmark}

```{julia}
import ManifoldsBase: retract_project!
function retract_project!(M::ScaledSphere, q, p, X, t::Number)
    q .= p .+ t .* X
    q .*= M.radius / norm(q)
    return q
end
```

The other two technical remarks refer to the step size and the stopping criterion,
so if we set these to something simpler, we should already be able to do a first run.

We have to specify

* that we want to use the new retraction,
* a simple step size and stopping criterion

We start with a certain point of cost
```{julia}
p0 = [zeros(d)...,1.0]
f(M,p0)
```

Then we can run our first solver,¬†where we have to overwrite a few
defaults, which would use functions we do not (yet) have.
Let's discuss these in the next steps.

```{julia}
q1 = gradient_descent(M, f, grad_f, p0;
    retraction_method = ProjectionRetraction(),   # state, that we use the retraction from above
    stepsize = DecreasingLength(M; length=1.0), # A simple step size
    stopping_criterion = StopAfterIteration(10),  # A simple stopping criterion
    X = zeros(d+1),                               # how we define/represent tangent vectors
)
f(M,q1)
```

We at least see, that the function value decreased.

### Norm and maximal step size

To use more advanced stopping criteria and step sizes we first need an ``[`inner`](@extref `ManifoldsBase.inner-Tuple{AbstractManifold, Any, Any, Any}`)``{=commonmark}`(M, p, X)`.
We also need a [`max_stepsize`](@ref)`(M)`, to avoid having too large steps
on positively curved manifolds like our scaled sphere in this example

```{julia}
import ManifoldsBase: inner
import Manopt: max_stepsize
inner(M::ScaledSphere, p, X,Y) = dot(X,Y) # inherited from the embedding
 # set the maximal allowed stepsize to injectivity radius.
Manopt.max_stepsize(M::ScaledSphere) = M.radius*œÄ
```

Then we can use the default step size ([`ArmijoLinesearch`](@ref)) and
the default stopping criterion, which checks for a small gradient Norm

```{julia}
q2 = gradient_descent(M, f, grad_f, p0;
    retraction_method = ProjectionRetraction(), # as before
    X = zeros(d+1), # as before
)
f(M, q2)
```

### Making life easier: default retraction and zero vector

To initialize tangent vector memory, the function ``[`zero_vector`](@extref `ManifoldsBase.zero_vector-Tuple{AbstractManifold, Any}`)``{=commonmark}`(M,p)` is called. Similarly,
the most-used retraction is returned by ``[`default_retraction_method`](@extref `ManifoldsBase.default_retraction_method-Tuple{AbstractManifold}`)``{=commonmark}

We can use both here, to make subsequent calls to the solver less verbose.
We define

```{julia}
import ManifoldsBase: zero_vector, default_retraction_method
zero_vector(M::ScaledSphere, p) = zeros(M.dimension+1)
default_retraction_method(M::ScaledSphere) = ProjectionRetraction()
```

and now we can even just call

```{julia}
q3 = gradient_descent(M, f, grad_f, p0)
f(M, q3)
```

But we for example automatically also get the possibility to obtain debug information like

```{julia}
gradient_descent(M, f, grad_f, p0; debug = [:Iteration, :Cost, :Stepsize, 25, :GradientNorm, :Stop, "\n"]);
```

see [How to Print Debug Output](https://manoptjl.org/stable/tutorials/HowToDebug.html)
for more details.

## Technical details

This tutorial is cached. It was last run on the following package versions.

```{julia}
#| code-fold: true
using Pkg
Pkg.status()
```
```{julia}
#| code-fold: true
using Dates
now()
```

## Literature

````{=commonmark}
```@bibliography
Pages = ["ImplementOwnManifold.md"]
Canonical=false
```
````