var documenterSearchIndex = {"docs":
[{"location":"solvers/cyclic_proximal_point.html#CPPSolver-1","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"The Cyclic Proximal Point (CPP) algorithm is a Proximal Problem.","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"It aims to minimize","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"F(x) = sum_i=1^c f_i(x)","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"assuming that the proximal maps operatornameprox_lambda f_i(x) are given in closed form or can be computed efficiently (at least approximately).","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"The algorithm then cycles through these proximal maps, where the type of cycle might differ and the proximal parameter lambda_k changes after each cycle k.","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"For a convergence result on Hadamard manifolds see [Bačák, 2014].","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"cyclic_proximal_point\ncyclic_proximal_point!","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.cyclic_proximal_point","page":"Cyclic Proximal Point","title":"Manopt.cyclic_proximal_point","text":"cyclic_proximal_point(M, F, proxes, x)\n\nperform a cyclic proximal point algorithm.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\nproxes – an Array of proximal maps (Functions) (λ,x) -> y for the summands of F\nx – an initial value x  mathcal M\n\nOptional\n\nthe default values are given in brackets\n\nevaluation_order – (:Linear) – whether to use a randomly permuted sequence (:FixedRandom), a per cycle permuted sequence (Random) or the default linear one.\nλ – ( iter -> 1/iter ) a function returning the (square summable but not summable) sequence of λi\nstopping_criterion – (StopWhenAny(StopAfterIteration(5000),StopWhenChangeLess(10.0^-8))) a StoppingCriterion.\nreturn_options – (false) – if actiavated, the extended result, i.e. the complete Options are returned. This can be used to access recorded values. If set to false (default) just the optimal value x_opt if returned\n\nand the ones that are passed to decorate_options for decorators.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/cyclic_proximal_point.html#Manopt.cyclic_proximal_point!","page":"Cyclic Proximal Point","title":"Manopt.cyclic_proximal_point!","text":"cyclic_proximal_point!(M, F, proxes, x)\n\nperform a cyclic proximal point algorithm in place of x.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\nproxes – an Array of proximal maps (Functions) (λ,x) -> y for the summands of F\nx – an initial value x  mathcal M\n\nfor all options, see cyclic_proximal_point.\n\n\n\n\n\n","category":"function"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"CyclicProximalPointOptions","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.CyclicProximalPointOptions","page":"Cyclic Proximal Point","title":"Manopt.CyclicProximalPointOptions","text":"CyclicProximalPointOptions <: Options\n\nstores options for the cyclic_proximal_point algorithm. These are the\n\nFields\n\nx – the current iterate\nstopping_criterion – a StoppingCriterion\nλ – (@(iter) -> 1/iter) a function for the values of λ_k per iteration/cycle\nevaluation_order – (:LinearOrder) – whether to use a randomly permuted sequence (:FixedRandomOrder), a per cycle permuted sequence (RandomOrder) or the default linear one.\n\nSee also\n\ncyclic_proximal_point\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#Debug-Functions-1","page":"Cyclic Proximal Point","title":"Debug Functions","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"DebugProximalParameter","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.DebugProximalParameter","page":"Cyclic Proximal Point","title":"Manopt.DebugProximalParameter","text":"DebugProximalParameter <: DebugAction\n\nprint the current iterates proximal point algorithm parameter given by Optionss o.λ.\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#Record-Functions-1","page":"Cyclic Proximal Point","title":"Record Functions","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"RecordProximalParameter","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.RecordProximalParameter","page":"Cyclic Proximal Point","title":"Manopt.RecordProximalParameter","text":"RecordProximalParameter <: RecordAction\n\nrecoed the current iterates proximal point algorithm parameter given by in Optionss o.λ.\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#Literature-1","page":"Cyclic Proximal Point","title":"Literature","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"<ul>\n<li id=\"Bačák2014\">[<a>Bačák, 2014</a>]\n  Bačák, M: <emph>Computing Medians and Means in Hadamard Spaces.</emph>,\n  SIAM Journal on Optimization, Volume 24, Number 3, pp. 1542–1566,\n  doi: <a href=\"https://doi.org/10.1137/140953393\">10.1137/140953393</a>,\n  arxiv: <a href=\"https://arxiv.org/abs/1210.2145\">1210.2145</a>.\n  </li>\n</ul>","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/StochasticGradientDescent.jl\"","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#SGDTutorial-1","page":"do stochastic gradient descent","title":"Stochastic Gradient Descent","text":"","category":"section"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"This tutorial illustrates how to use the stochastic_gradient_descent solver and different DirectionUpdateRules in order to introduce the average or momentum variant, see Stochastic Gradient Descent.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Computationally we look at a very simple but large scale problem, the Riemannian Center of Mass or Fréchet mean: For given points p_i in mathcal M, i=1ldotsN this optimization problem reads","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"operatorname*argmin_xinmathcal M frac12sum_i=1^N\n  operatornamed^2_mathcal M(xp_i)","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"which of course can be (and is) solved by a gradient descent, see the introductionary tutorial. If N is very large it might be quite expensive to evaluate the complete gradient. A remedy is, to evaluate only one of the terms at a time and choose a random order for these.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"We first initialize the manifold (see [])","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"using Manopt, Manifolds, Random, Colors","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"black = RGBA{Float64}(colorant\"#000000\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\") # Start\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\") # a path\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\") # points\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"And optain a large data set","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"n = 5000\nσ = π / 12\nM = Sphere(2)\nx = 1 / sqrt(2) * [1.0, 0.0, 1.0]\nRandom.seed!(42)\ndata = [exp(M, x, random_tangent(M, x, Val(:Gaussian), σ)) for i in 1:n]\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"which looks like","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"asymptote_export_S2_signals(\"centerAndLargeData.asy\";\n    points = [ [x], data],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal]),\n    dot_sizes = [2.5, 1.0], camera_position = (1.,.5,.5)\n)\nrender_asymptote(\"centerAndLargeData.asy\"; render = 2)","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"(Image: The data of noisy versions of $x$)","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Note that due to the construction of the points as zero mean tangent vectors, the mean should be very close to our initial point x.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"In order to use the stochastic gradient, we now need a function that returns the vector of gradients. There are two ways to define it in Manopt.jl: as one function, that returns a vector or a vector of funtions.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"The first variant is of course easier to define, but the second is more efficient when only evaluating one of the gradients. For the mean we have as a gradient","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":" F(x) = sum_i=1^N f_i(x) quad textwhere f_i(x) = -log_x p_i","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Which we define as","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"F(x) = 1 / (2 * n) * sum(map(p -> distance(M, x, p)^2, data))\n∇F(x) = [∇distance(M, p, x) for p in data]\n∇f = [x -> ∇distance(M, p, x) for p in data];\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"The calls are only slightly different, but notice that accessing the 2nd gradient element requires evaluating all logs in the first function. So while you can use both ∇F and ∇f in the following call, the second one is faster:","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt1 = stochastic_gradient_descent(M, ∇F, x);\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"versus","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt2 = stochastic_gradient_descent(M, ∇f, x);\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"This result is reasonably close. But we can improve it by using a DirectionUpdateRule, namely: On the one hand MomentumGradient, which requires both the manifold and the initial value,    in order to keep track of the iterate and parallel transport the last direction to the current iterate.    you can also set a vector_transport_method, if ParallelTransport() is not    available on your manifold. Here we simply do","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt3 = stochastic_gradient_descent(\n    M, ∇f, x; direction=MomentumGradient(M, x, StochasticGradient())\n);\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"And on the other hand the AverageGradient computes an average of the last n gradients, i.e.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt4 = stochastic_gradient_descent(\n    M, ∇f, x; direction=AverageGradient(M, x, 10, StochasticGradient())\n);\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"note that the default StoppingCriterion is a fixed number of iterations.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Note that since you can apply both also in case of gradient_descent, i.e. to use IdentityUpdateRule and evaluate the classical gradient, both constructors have to know that internally the default evaluation of the Stochastic gradient (choosing one gradient f_k at random) has to be specified.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"For this small example you can of course also use a gradient descent with ArmijoLinesearch, but it will be a little slower usually","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt5 = gradient_descent(M, F, x -> sum(∇F(x)), x; stepsize=ArmijoLinesearch());\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"but it is for sure faster than the variant above that evaluates the full gradient on every iteration, since stochastic gradient descent takes more iterations.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Note that all 5 of couse yield the same result","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"[distance(M, x, y) for y in [x_opt1, x_opt2, x_opt3, x_opt4, x_opt5]]","category":"page"},{"location":"functions/costs.html#CostFunctions-1","page":"Cost functions","title":"Cost Functions","text":"","category":"section"},{"location":"functions/costs.html#","page":"Cost functions","title":"Cost functions","text":"The following cost functions are available","category":"page"},{"location":"functions/costs.html#","page":"Cost functions","title":"Cost functions","text":"Modules = [Manopt]\nPages   = [\"costs.jl\"]","category":"page"},{"location":"functions/costs.html#Manopt.costIntrICTV12-Tuple{ManifoldsBase.Manifold,Any,Any,Any,Any,Any}","page":"Cost functions","title":"Manopt.costIntrICTV12","text":"costIntrICTV12(M, f, u, v, α, β)\n\nCompute the intrinsic infimal convolution model, where the addition is replaced by a mid point approach and the two functions involved are costTV2 and costTV. The model reads\n\nE(uv) =\n  frac12sum_i  mathcal G\n    d_mathcal Mbigl(g(frac12v_iw_i)f_ibigr)\n  +alphabigl( betamathrmTV(v) + (1-beta)mathrmTV_2(w) bigr)\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costL2TV-NTuple{4,Any}","page":"Cost functions","title":"Manopt.costL2TV","text":"costL2TV(M, f, α, x)\n\ncompute the ell^2-TV functional on the PowerManifold manifoldMfor given (fixed) dataf(onM), a nonnegative weightα, and evaluated atx(onM`), i.e.\n\nE(x) = d_mathcal M^2(fx) + alpha operatornameTV(x)\n\nSee also\n\ncostTV\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costL2TV2-Tuple{ManifoldsBase.PowerManifold,Any,Any,Any}","page":"Cost functions","title":"Manopt.costL2TV2","text":"costL2TV2(M, f, β, x)\n\ncompute the ell^2-TV2 functional on the PowerManifold manifold M for given data f, nonnegative parameter β, and evaluated at x, i.e.\n\nE(x) = d_mathcal M^2(fx) + betaoperatornameTV_2(x)\n\nSee also\n\ncostTV2\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costL2TVTV2-Tuple{ManifoldsBase.PowerManifold,Any,Any,Any,Any}","page":"Cost functions","title":"Manopt.costL2TVTV2","text":"costL2TVTV2(M, f, α, β, x)\n\ncompute the ell^2-TV-TV2 functional on the PowerManifold manifold M for given (fixed) data f (on M), nonnegative weight α, β, and evaluated at x (on M), i.e.\n\nE(x) = d_mathcal M^2(fx) + alphaoperatornameTV(x)\n  + betaoperatornameTV_2(x)\n\nSee also\n\ncostTV, costTV2\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costTV","page":"Cost functions","title":"Manopt.costTV","text":"costTV(M,x [,p=2,q=1])\n\nCompute the operatornameTV^p functional for data xon the PowerManifold manifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i denote the forward neighbors, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I_i = i+e_j j=1ldotskcap mathcal G. The formula reads\n\nE^q(x) = sum_i  mathcal G\n  bigl( sum_j   mathcal I_i d^p_mathcal M(x_ix_j) bigr)^qp\n\nSee also\n\n∇TV, prox_TV\n\n\n\n\n\n","category":"function"},{"location":"functions/costs.html#Manopt.costTV-Union{Tuple{T}, Tuple{ManifoldsBase.Manifold,Tuple{T,T}}, Tuple{ManifoldsBase.Manifold,Tuple{T,T},Int64}} where T","page":"Cost functions","title":"Manopt.costTV","text":"costTV(M, x, p)\n\nCompute the operatornameTV^p functional for a tuple pT of pointss on a Manifold M, i.e.\n\nE(x_1x_2) = d_mathcal M^p(x_1x_2) quad x_1x_2  mathcal M\n\nSee also\n\n∇TV, prox_TV\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costTV2","page":"Cost functions","title":"Manopt.costTV2","text":"costTV2(M,x [,p=1])\n\ncompute the operatornameTV_2^p functional for data x on the PowerManifold manifoldmanifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i^pm denote the forward and backward neighbors, respectively, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I^pm_i = ipm e_j j=1ldotskcap mathcal I. The formula then reads\n\nE(x) = sum_i  mathcal I j_1   mathcal I^+_i j_2   mathcal I^-_i\nd^p_mathcal M(c_i(x_j_1x_j_2) x_i)\n\nwhere c_i(cdotcdot) denotes the mid point between its two arguments that is nearest to x_i.\n\nSee also\n\n∇TV2, prox_TV2\n\n\n\n\n\n","category":"function"},{"location":"functions/costs.html#Manopt.costTV2-Union{Tuple{T}, Tuple{MT}, Tuple{MT,Tuple{T,T,T}}, Tuple{MT,Tuple{T,T,T},Any}} where T where MT<:ManifoldsBase.Manifold","page":"Cost functions","title":"Manopt.costTV2","text":"costTV2(M,(x1,x2,x3) [,p=1])\n\nCompute the operatornameTV_2^p functional for the 3-tuple of points (x1,x2,x3)on the Manifold M. Denote by\n\n  mathcal C = bigl c   mathcal M   g(tfrac12x_1x_3) text for some geodesic gbigr\n\nthe set of mid points between x_1 and x_3. Then the functionr reads\n\nd_2^p(x_1x_2x_3) = min_c  mathcal C d_mathcal M(cx_2)\n\nSee also\n\n∇TV2, prox_TV2\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.cost_L2_acceleration_bezier-Union{Tuple{P}, Tuple{ManifoldsBase.Manifold,AbstractArray{P,1},AbstractArray{#s31,1} where #s31<:Integer,AbstractArray{#s30,1} where #s30<:AbstractFloat,AbstractFloat,AbstractArray{P,1}}} where P","page":"Cost functions","title":"Manopt.cost_L2_acceleration_bezier","text":"cost_L2_acceleration_bezier(M,B,pts,λ,d)\n\ncompute the value of the discrete Acceleration of the composite Bezier curve together with a data term, i.e.\n\nfraclambda2sum_i=0^N d_mathcal M(d_i c_B(i))^2+\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i and d_2 refers to the second order absolute difference costTV2 (squared), the junction points are denoted by p_i, and to each p_i corresponds one data item in the manifold points given in d. For details on the acceleration approximation, see cost_acceleration_bezier. Note that the Beziér-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segmenents can internally be reconstructed.\n\nSee also\n\n∇L2_acceleration_bezier, cost_acceleration_bezier, ∇acceleration_bezier\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.cost_acceleration_bezier-Union{Tuple{P}, Tuple{ManifoldsBase.Manifold,AbstractArray{P,1},AbstractArray{#s31,1} where #s31<:Integer,AbstractArray{#s30,1} where #s30<:AbstractFloat}} where P","page":"Cost functions","title":"Manopt.cost_acceleration_bezier","text":"cost_acceleration_bezier(\n    M::Manifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector{<:AbstractFloat},\n) where {P}\n\ncompute the value of the discrete Acceleration of the composite Bezier curve\n\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i, i=1ldotsN, and d_2 refers to the second order absolute difference costTV2 (squared). Note that the Beziér-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segmenents can internally be reconstructed.\n\nThis acceleration discretization was introduced in[BergmannGousenbourger2018].\n\nSee also\n\n∇acceleration_bezier, cost_L2_acceleration_bezier, ∇L2_acceleration_bezier\n\n[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics (2018). doi 10.3389/fams.2018.00059, arXiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"solvers/DouglasRachford.html#DRSolver-1","page":"Douglas–Rachford","title":"Douglas–Rachford Algorithm","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"The (Parallel) Douglas–Rachford ((P)DR) Algorithm was generalized to Hadamard manifolds in [Bergmann, Persch, Steidl, 2016].","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"The aim is to minimize the sum","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"F(x) = f(x) + g(x)","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"on a manifold, where the two summands have proximal maps operatornameprox_lambda f operatornameprox_lambda g that are easy to evaluate (maybe in closed form or not too costly to approximate). Further define the Reflection operator at the proximal map as","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"operatornamerefl_lambda f(x) = exp_operatornameprox_lambda f(x) bigl( -log_operatornameprox_lambda f(x) x bigr)","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":".","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Let alpha_k   01 with sum_k  mathbb N alpha_k(1-alpha_k) =   fty and lambda  0 which might depend on iteration k as well) be given.","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Then the (P)DRA algorithm for initial data x_0  mathcal H as","category":"page"},{"location":"solvers/DouglasRachford.html#Initialization-1","page":"Douglas–Rachford","title":"Initialization","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Initialize t_0 = x_0 and k=0","category":"page"},{"location":"solvers/DouglasRachford.html#Iteration-1","page":"Douglas–Rachford","title":"Iteration","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Repeat  until a convergence criterion is reached","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Compute s_k = operatornamerefl_lambda foperatornamerefl_lambda g(t_k)\nwithin that operation store x_k+1 = operatornameprox_lambda g(t_k) which is the prox the inner reflection reflects at.\nCompute t_k+1 = g(alpha_k t_k s_k)\nSet k = k+1","category":"page"},{"location":"solvers/DouglasRachford.html#Result-1","page":"Douglas–Rachford","title":"Result","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"The result is given by the last computed x_K.","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"For the parallel version, the first proximal map is a vectorial version, where in each component one prox is applied to the corresponding copy of t_k and the second proximal map corresponds to the indicator function of the set, where all copies are equal (in mathcal H^n, where n is the number of copies), leading to the second prox being the Riemannian mean.","category":"page"},{"location":"solvers/DouglasRachford.html#Interface-1","page":"Douglas–Rachford","title":"Interface","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"  DouglasRachford\n  DouglasRachford!","category":"page"},{"location":"solvers/DouglasRachford.html#Manopt.DouglasRachford","page":"Douglas–Rachford","title":"Manopt.DouglasRachford","text":" DouglasRachford(M, F, proxMaps, x)\n\nComputes the Douglas-Rachford algorithm on the manifold mathcal M, initial data x_0 and the (two) proximal maps proxMaps.\n\nFor k2 proximal maps the problem is reformulated using the parallelDouglasRachford: a vectorial proximal map on the power manifold mathcal M^k and the proximal map of the set that identifies all entries again, i.e. the Karcher mean. This henve also boild down to two proximal maps, though each evauates proximal maps in parallel, i.e. component wise in a vector.\n\nInput\n\nM – a Riemannian Manifold mathcal M\nF – a cost function consisting of a sum of cost functions\nproxes – functions of the form (λ,x)->... performing a proximal map, where ⁠λ denotes the proximal parameter, for each of the summands of F.\nx0 – initial data x_0  mathcal M\n\nOptional values\n\nthe default parameter is given in brackets\n\nλ – ((iter) -> 1.0) function to provide the value for the proximal parameter during the calls\nα – ((iter) -> 0.9) relaxation of the step from old to new iterate, i.e. t_k+1 = g(α_k t_k s_k), where s_k is the result of the double reflection involved in the DR algorithm\nR – (reflect) method employed in the iteration to perform the reflection of x at the prox p.\nstopping_criterion – (StopWhenAny(StopAfterIteration(200),StopWhenChangeLess(10.0^-5))) a StoppingCriterion.\nparallel – (false) clarify that we are doing a parallel DR, i.e. on a PowerManifold manifold with two proxes. This can be used to trigger parallel Douglas–Rachford if you enter with two proxes. Keep in mind, that a parallel Douglas–Rachford implicitly works on a PowerManifold manifold and its first argument is the result then (assuming all are equal after the second prox.\nreturn_options – (false) – if actiavated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/DouglasRachford.html#Manopt.DouglasRachford!","page":"Douglas–Rachford","title":"Manopt.DouglasRachford!","text":" DouglasRachford(M, F, proxMaps, x)\n\nComputes the Douglas-Rachford algorithm on the manifold mathcal M, initial data x_0 and the (two) proximal maps proxMaps in place of x.\n\nFor k2 proximal maps the problem is reformulated using the parallelDouglasRachford: a vectorial proximal map on the power manifold mathcal M^k and the proximal map of the set that identifies all entries again, i.e. the Karcher mean. This hence also boils down to two proximal maps, though each evauates proximal maps in parallel, i.e. component wise in a vector.\n\nInput\n\nM – a Riemannian Manifold mathcal M\nF – a cost function consisting of a sum of cost functions\nproxes – functions of the form (λ,x)->... performing a proximal map, where ⁠λ denotes the proximal parameter, for each of the summands of F.\nx0 – initial data x_0  mathcal M\n\nFor more options, see DouglasRachford.\n\n\n\n\n\n","category":"function"},{"location":"solvers/DouglasRachford.html#Options-1","page":"Douglas–Rachford","title":"Options","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"DouglasRachfordOptions","category":"page"},{"location":"solvers/DouglasRachford.html#Manopt.DouglasRachfordOptions","page":"Douglas–Rachford","title":"Manopt.DouglasRachfordOptions","text":"DouglasRachfordOptions <: Options\n\nStore all options required for the DouglasRachford algorithm,\n\nFields\n\nx - the current iterate (result) For the parallel Douglas-Rachford, this is not a value from the PowerManifold manifold but the mean.\ns – the last result of the double reflection at the proxes relaxed by α.\nλ – ((iter)->1.0) function to provide the value for the proximal parameter during the calls\nα – ((iter)->0.9) relaxation of the step from old to new iterate, i.e. x^(k+1) = g(α(k) x^(k) t^(k)), where t^(k) is the result of the double reflection involved in the DR algorithm\nR – (reflect) method employed in the iteration to perform the reflection of x at the prox p.\nstop – (StopAfterIteration(300)) a StoppingCriterion\nparallel – (false) inducate whether we are running a pallel Douglas-Rachford or not.\n\n\n\n\n\n","category":"type"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"For specific DebugActions and RecordActions see also Cyclic Proximal Point.","category":"page"},{"location":"solvers/DouglasRachford.html#Literature-1","page":"Douglas–Rachford","title":"Literature","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"<ul>\n<li id=\"BergmannPerschSteidl2016\">[<a>Bergmann, Persch, Steidl, 2016</a>]\n  Bergmann, R; Persch, J.; Steidl, G.: <emph>A Parallel Douglas–Rachford\n  Algorithm for Minimizing ROF-like Functionals on Images with Values in\n  Symmetric Hadamard Manifolds.</emph>\n  SIAM Journal on Imaging Sciences, Volume 9, Number 3, pp. 901–937, 2016.\n  doi: <a href=\"https://doi.org/10.1137/15M1052858\">10.1137/15M1052858</a>,\n  arXiv: <a href=\"https://arxiv.org/abs/1512.02814\">1512.02814</a>.\n</li>\n</ul>","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#CGSolver-1","page":"Conjugate gradient descent","title":"Conjugate Gradient Descent","text":"","category":"section"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"conjugate_gradient_descent\nconjugate_gradient_descent!","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.conjugate_gradient_descent","page":"Conjugate gradient descent","title":"Manopt.conjugate_gradient_descent","text":"conjugate_gradient_descent(M, F, ∇F, x)\n\nperform a conjugate gradient based descent\n\nx_k+1 = operatornameretr_x_k bigl( s_kdelta_k bigr)\n\nwhere operatornameretr denotes a retraction on the Manifold M and one can employ different rules to update the descent direction delta_k based on the last direction delta_k-1 and both gradients nabla f(x_k),nabla f(x_k-1). The Stepsize s_k may be determined by a Linesearch.\n\nAvailable update rules are SteepestDirectionUpdateRule, which yields a gradient_descent, ConjugateDescentCoefficient, DaiYuanCoefficient, FletcherReevesCoefficient, HagerZhangCoefficient, HeestenesStiefelCoefficient, LiuStoreyCoefficient, and PolakRibiereCoefficient.\n\nThey all compute beta_k such that this algorithm updates the search direction as\n\ndelta_k=nabla f(x_k) + beta_k delta_k-1\n\nInput\n\nM : a manifold mathcal M\nF : a cost function Fcolonmathcal Mtomathbb R to minimize\n∇F: the gradient  Fcolonmathcal Mto Tmathcal M of F\nx : an initial value xinmathcal M\n\nOptional\n\ncoefficient : (SteepestDirectionUpdateRule <: DirectionUpdateRule rule to compute the descent direction update coefficient beta_k, as a functor, i.e. the resulting function maps (p,o,i) -> β, where p is the current GradientProblem, o are the ConjugateGradientDescentOptions o and i is the current iterate.\nretraction_method - (ExponentialRetraction) a retraction method to use, by default the exponntial map\nreturn_options – (false) – if actiavated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt if returned\nstepsize - (Constant(1.)) A Stepsize function applied to the search direction. The default is a constant step size 1.\nstopping_criterion : (stopWhenAny( stopAtIteration(200), stopGradientNormLess(10.0^-8))) a function indicating when to stop.\nvector_transport_method – (ParallelTransport()) vector transport method to transport the old descent direction when computing the new descent direction.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.conjugate_gradient_descent!","page":"Conjugate gradient descent","title":"Manopt.conjugate_gradient_descent!","text":"conjugate_gradient_descent!(M, F, ∇F, x)\n\nperform a conjugate gradient based descent in place of x, i.e.\n\nx_k+1 = operatornameretr_x_k bigl( s_kdelta_k bigr)\n\nwhere operatornameretr denotes a retraction on the Manifold M\n\nInput\n\nM : a manifold mathcal M\nF : a cost function Fcolonmathcal Mtomathbb R to minimize\n∇F: the gradient  Fcolonmathcal Mto Tmathcal M of F\nx : an initial value xinmathcal M\n\nfor more details and options, especially the DirectionUpdateRules,  see conjugate_gradient_descent.\n\n\n\n\n\n","category":"function"},{"location":"solvers/conjugate_gradient_descent.html#Options-1","page":"Conjugate gradient descent","title":"Options","text":"","category":"section"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"ConjugateGradientDescentOptions","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.ConjugateGradientDescentOptions","page":"Conjugate gradient descent","title":"Manopt.ConjugateGradientDescentOptions","text":"ConjugateGradientOptions <: Options\n\nspecify options for a conjugate gradient descent algoritm, that solves a [GradientProblem].\n\nFields\n\nx – the current iterate, a point on a manifold\n∇ – the current gradient\nδ – the current descent direction, i.e. also tangent vector\nβ – the current update coefficient rule, see .\ncoefficient – a DirectionUpdateRule function to determine the new β\nstepsize – a Stepsize function\nstop – a StoppingCriterion\nretraction_method – (ExponentialRetraction()) a type of retraction\n\nSee also\n\nconjugate_gradient_descent, GradientProblem, ArmijoLinesearch\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Available-Coefficients-1","page":"Conjugate gradient descent","title":"Available Coefficients","text":"","category":"section"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"The update rules act as DirectionUpdateRule, which internally always first evaluate the gradient itself.","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"ConjugateDescentCoefficient\nDaiYuanCoefficient\nFletcherReevesCoefficient\nHagerZhangCoefficient\nHeestenesStiefelCoefficient\nLiuStoreyCoefficient\nPolakRibiereCoefficient\nSteepestDirectionUpdateRule","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.ConjugateDescentCoefficient","page":"Conjugate gradient descent","title":"Manopt.ConjugateDescentCoefficient","text":"ConjugateDescentCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kxi_k, the current iterates x_k+1xi_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [Flethcer1987] adapted to manifolds:\n\nbeta_k =\nfrac lVert xi_k+1 rVert_x_k+1^2 \nlangle -delta_kxi_k rangle_x_k\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nConjugateDescentCoefficient(a::StoreOptionsAction=())\n\nConstruct the conjugate descnt coefficient update rule, a new storage is created by default.\n\n[Flethcer1987]: R. Fletcher, Practical Methods of Optimization vol. 1: Unconstrained Optimization John Wiley & Sons, New York, 1987. doi 10.1137/1024028\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.DaiYuanCoefficient","page":"Conjugate gradient descent","title":"Manopt.DaiYuanCoefficient","text":"DaiYuanCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kxi_k, the current iterates x_k+1xi_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [DaiYuan1999]\n\nadapted to manifolds: let nu_k = xi_k+1 - P_x_k+1gets x_kxi_k, where P_agets b(cdot) denotes a vector transport from the tangent space at a to b.\n\nThen the coefficient reads\n\nbeta_k =\nfrac lVert xi_k+1 rVert_x_k+1^2 \nlangle P_x_k+1gets x_kdelta_k nu_k rangle_x_k+1\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nDaiYuanCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=(),\n)\n\nConstruct the Dai Yuan coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\n[DaiYuan1999]: [Y. H. Dai and Y. Yuan, A nonlinear conjugate gradient method with a strong global convergence property, SIAM J. Optim., 10 (1999), pp. 177–182. doi: 10.1137/S1052623497318992\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.FletcherReevesCoefficient","page":"Conjugate gradient descent","title":"Manopt.FletcherReevesCoefficient","text":"FletcherReevesCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kxi_k, the current iterates x_k+1xi_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [FletcherReeves1964] adapted to manifolds:\n\nbeta_k =\nfraclVert xi_k+1rVert_x_k+1^2lVert xi_krVert_x_k^2\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nFletcherReevesCoefficient(a::StoreOptionsAction=())\n\nConstruct the Fletcher Reeves coefficient update rule, a new storage is created by default.\n\n[FletcherReeves1964]: R. Fletcher and C. Reeves, Function minimization by conjugate gradients, Comput. J., 7 (1964), pp. 149–154. doi: 10.1093/comjnl/7.2.149\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.HagerZhangCoefficient","page":"Conjugate gradient descent","title":"Manopt.HagerZhangCoefficient","text":"HagerZhangCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kxi_k, the current iterates x_k+1xi_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [HagerZhang2005] adapted to manifolds: let nu_k = xi_k+1 - P_x_k+1gets x_kxi_k, where P_agets b(cdot) denotes a vector transport from the tangent space at a to b.\n\nbeta_k = Bigllanglenu_k -\nfrac 2lVert nu_krVert_x_k+1^2  langle P_x_k+1gets x_kdelta_k nu_k rangle_x_k+1 \nP_x_k+1gets x_kdelta_k\nfracxi_k+1 langle P_x_k+1gets x_kdelta_k nu_k rangle_x_k+1 \nBigrrangle_x_k+1\n\nThis method includes a numerical stability proposed by those authors.\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nHagerZhangCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=(),\n)\n\nConstruct the Hager Zhang coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\n[HagerZhang2005]: [W. W. Hager and H. Zhang, A new conjugate gradient method with guaranteed descent and an efficient line search, SIAM J. Optim, (16), pp. 170-192, 2005. doi: 10.1137/030601880\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.HeestenesStiefelCoefficient","page":"Conjugate gradient descent","title":"Manopt.HeestenesStiefelCoefficient","text":"HeestenesStiefelCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kxi_k, the current iterates x_k+1xi_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [HeestensStiefel1952]\n\nadapted to manifolds as follows: let nu_k = xi_k+1 - P_x_k+1gets x_kxi_k. Then the update reads\n\nbeta_k = fraclangle xi_k+1 nu_k rangle_x_k+1 \n     langle P_x_k+1gets x_k delta_k nu_krangle_x_k+1 \n\nwhere P_agets b(cdot) denotes a vector transport from the tangent space at a to b.\n\nConstructor\n\nHeestenesStiefelCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=()\n)\n\nConstruct the Heestens Stiefel coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\nSee also conjugate_gradient_descent\n\n[HeestensStiefel1952]: M.R. Hestenes, E.L. Stiefel, Methods of conjugate gradients for solving linear systems, J. Research Nat. Bur. Standards, 49 (1952), pp. 409–436. doi: 10.6028/jres.049.044\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.LiuStoreyCoefficient","page":"Conjugate gradient descent","title":"Manopt.LiuStoreyCoefficient","text":"LiuStoreyCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kxi_k, the current iterates x_k+1xi_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [LuiStorey1991] adapted to manifolds: let nu_k = xi_k+1 - P_x_k+1gets x_kxi_k, where P_agets b(cdot) denotes a vector transport from the tangent space at a to b.\n\nThen the coefficient reads\n\nbeta_k = -\nfrac langle xi_k+1nu_k rangle_x_k+1 \nlangle delta_kxi_k rangle_x_k\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nLiuStoreyCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=()\n)\n\nConstruct the Lui Storey coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\n[LuiStorey1991]: [Y. Liu and C. Storey, Efficient generalized conjugate gradient algorithms, Part 1: Theory J. Optim. Theory Appl., 69 (1991), pp. 129–137. doi: 10.1007/BF00940464\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.PolakRibiereCoefficient","page":"Conjugate gradient descent","title":"Manopt.PolakRibiereCoefficient","text":"PolakRibiereCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kxi_k, the current iterates x_k+1xi_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [PolakRibiere1969][Polyak1969]\n\nadapted to manifolds: let nu_k = xi_k+1 - P_x_k+1gets x_kxi_k, where P_agets b(cdot) denotes a vector transport from the tangent space at a to b.\n\nThen the update reads\n\nbeta_k =\nfrac langle xi_k+1 nu_k rangle_x_k+1 \nlVert xi_k rVert_x_k^2 \n\nConstructor\n\nPolakRibiereCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=()\n)\n\nConstruct the PolakRibiere coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\nSee also conjugate_gradient_descent\n\n[PolakRibiere1969]: E. Polak, G. Ribiere, Note sur la convergence de méthodes de directions conjuguées ESAIM: Mathematical Modelling and Numerical Analysis - Modélisation Mathématique et Analyse Numérique, Tome 3 (1969) no. R1, p. 35-43, url: http://www.numdam.org/item/?id=M2AN1969__31350\n\n[Polyak1969]: B. T. Polyak, The conjugate gradient method in extreme problems, USSR Comp. Math. Math. Phys., 9 (1969), pp. 94–112. doi: 10.1016/0041-5553(69)90035-4\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.SteepestDirectionUpdateRule","page":"Conjugate gradient descent","title":"Manopt.SteepestDirectionUpdateRule","text":"SteepestDirectionUpdateRule <: DirectionUpdateRule\n\nThe simplest rule to update is to have no influence of the last direction and hence return an update beta = 0 for all ConjugateGradientDescentOptionso\n\nSee also conjugate_gradient_descent\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Literature-1","page":"Conjugate gradient descent","title":"Literature","text":"","category":"section"},{"location":"about.html#About-1","page":"About","title":"About","text":"","category":"section"},{"location":"about.html#","page":"About","title":"About","text":"Manopt.jl inherited its name from Manopt, a Matlab toolbox. It is currently Maintained by Ronny Bergmann (manopt@ronnybergmann.net) with contributions from Tom Christian Riemer, who implemented the trust regions solver.","category":"page"},{"location":"about.html#","page":"About","title":"About","text":"If you want to contribute a manifold or algorithm or have any questions, visit the GitHub repository to clone/fork the repository or open an issue.","category":"page"},{"location":"functions/bezier.html#BezierCurves-1","page":"Bézier curves","title":"Bézier curves","text":"","category":"section"},{"location":"functions/bezier.html#","page":"Bézier curves","title":"Bézier curves","text":"Modules = [Manopt]\nPages   = [\"bezier_curves.jl\"]","category":"page"},{"location":"functions/bezier.html#Manopt.BezierSegment","page":"Bézier curves","title":"Manopt.BezierSegment","text":"BezierSegment\n\nA type to capture a Bezier segment. With n points, a Beziér segment of degree n-1 is stored. On the Euclidean manifold, this yields a polynomial of degree n-1.\n\nThis type is mainly used to encapsulate the points within a composite Bezier curve, which consist of an AbstractVector of BezierSegments where each of the points might be a nested array on a PowerManifold already.\n\nNot that this can also be used to represent tangent vectors on the control points of a segment.\n\nSee also: de_casteljau.\n\nConstructor\n\nBezierSegment(pts::AbstractVector)\n\nGiven an abstract vector of pts generate the corresponding Bézier segment.\n\n\n\n","category":"type"},{"location":"functions/bezier.html#Manopt.de_casteljau-Tuple{ManifoldsBase.Manifold,Vararg{Any,N} where N}","page":"Bézier curves","title":"Manopt.de_casteljau","text":"de_casteljau(M::Manifold, b::BezierSegment NTuple{N,P}) -> Function\n\nreturn the Bézier curve beta(cdotp_0ldotsb_n)colon 01 to mathcal M defined by the control points b_0ldotsb_ninmathcal M, nin mathbb N, as a BezierSegment. This function implements de Casteljau's algorithm[Casteljau1959][Casteljau1963] gneralized to manifolds[PopielNoakes2007]: Let gamma_ab(t) denote the shortest geodesic connecting abinmathcal M. Then the curve is defined by the recursion\n\nbeginaligned\n    beta(tb_0b_1) = gamma_b_0b_1(t)\n    beta(tb_0ldotsb_n) = gamma_beta(tb_0ldotsb_n-1) beta(tb_1ldotsb_n)(t)\nendaligned\n\nand P is the type of a point on the Manifold M.\n\n    de_casteljau(M::Manifold, B::AbstractVector{<:BezierSegment}) -> Function\n\nGiven a vector of Bézier segments, i.e. a vector of control points B=bigl( (b_00ldotsb_n_00)ldots(b_0mldots b_n_mm) bigr), where the different segments might be of different degree(s) n_0ldotsn_m. The resulting composite Bézier curve c_Bcolon0m to mathcal M consists of m segments which are Bézier curves.\n\nc_B(t) =\n    begincases\n        beta(t b_00ldotsb_n_00)  text if  t in 01\n        beta(t-i b_0ildotsb_n_ii)  text if \n            tin (ii+1 quad iin1ldotsm-1\n    endcases\n\n    de_casteljau(M::Manifold, b::BezierSegment, t::Real)\n    de_casteljau(M::Manifold, B::AbstractVector{<:BezierSegment}, t::Real)\n    de_casteljau(M::Manifold, b::BezierSegment, T::AbstractVector) -> AbstractVector\n    de_casteljau(\n        M::Manifold,\n        B::AbstractVector{<:BezierSegment},\n        T::AbstractVector\n    ) -> AbstractVector\n\nEvaluate the Bézier curve at time t or at times t in T.\n\n[Casteljau1959]: de Casteljau, P.: Outillage methodes calcul, Enveloppe Soleau 40.040 (1959), Institute National de la Propriété Industrielle, Paris.\n\n[Casteljau1963]: de Casteljau, P.: Courbes et surfaces à pôles, Microfiche P 4147-1, André Citroën Automobile SA, Paris, (1963).\n\n[PopielNoakes2007]: Popiel, T. and Noakes, L.: Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory (2007), 148(2), pp. 111–127.- doi: 10.1016/j.jat.2007.03.002.\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_degree-Tuple{ManifoldsBase.Manifold,BezierSegment}","page":"Bézier curves","title":"Manopt.get_bezier_degree","text":"get_bezier_degree(M::Manifold, b::BezierSegment)\n\nreturn the degree of the Bézier curve represented by the tuple b of control points on the manifold M, i.e. the number of points minus 1.\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_degrees-Tuple{ManifoldsBase.Manifold,AbstractArray{#s32,1} where #s32<:BezierSegment}","page":"Bézier curves","title":"Manopt.get_bezier_degrees","text":"get_bezier_degrees(M::Manidold, B::AbstractVector{<:BezierSegment})\n\nreturn the degrees of the components of a composite Bézier curve represented by tuples in B containing points on the manifold M.\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_inner_points-Tuple{ManifoldsBase.Manifold,AbstractArray{#s32,1} where #s32<:BezierSegment}","page":"Bézier curves","title":"Manopt.get_bezier_inner_points","text":"get_bezier_inner_points(M::Manifold, B::AbstractVector{<:BezierSegment} )\nget_bezier_inner_points(M::Manifold, b::BezierSegment)\n\nreturns the inner (i.e. despite start and end) points of the segments of the composite Bézier curve specified by the control points B. For a single segment b, its inner points are returned\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_junction_tangent_vectors-Union{Tuple{P}, Tuple{ManifoldsBase.Manifold,AbstractArray{#s32,1} where #s32<:BezierSegment}} where P","page":"Bézier curves","title":"Manopt.get_bezier_junction_tangent_vectors","text":"get_bezier_junction_tangent_vectors(M::Manifold, B::AbstractVector{<:BezierSegment})\nget_bezier_junction_tangent_vectors(M::Manifold, b::BezierSegment)\n\nreturns the tangent vectors at start and end points of the composite Bézier curve pointing from a junction point to the first and last inner control points for each segment of the composite Bezier curve specified by the control points B, either a vector of segments of controlpoints.\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_junctions","page":"Bézier curves","title":"Manopt.get_bezier_junctions","text":"get_bezier_junctions(M::Manifold, B::AbstractVector{<:BezierSegment})\nget_bezier_junctions(M::Manifold, b::BezierSegment)\n\nreturns the start and end point(s) of the segments of the composite Bézier curve specified by the control points B. For just one segment b, its start and end points are returned.\n\n\n\n\n\n","category":"function"},{"location":"functions/bezier.html#Manopt.get_bezier_points","page":"Bézier curves","title":"Manopt.get_bezier_points","text":"get_bezier_points(\n    M::MAnifold,\n    B::AbstractVector{<:BezierSegment},\n    reduce::Symbol=:default\n)\nget_bezier_points(M::Manifold, b::BezierSegment, reduce::Symbol=:default)\n\nreturns the control points of the segments of the composite Bézier curve specified by the control points B, either a vector of segments of controlpoints or a.\n\nThis method reduces the points depending on the optional reduce symbol\n\n:default – no reduction is performed\n:continuous – for a continuous function, the junction points are doubled at b_0i=b_n_i-1i-1, so only b_0i is in the vector.\n:differentiable – for a differentiable function additionally log_b_0ib_1i = -log_b_n_i-1i-1b_n_i-1-1i-1 holds. hence b_n_i-1-1i-1 is ommited.\n\nIf only one segment is given, all points of b – i.e. b.pts is returned.\n\n\n\n\n\n","category":"function"},{"location":"functions/bezier.html#Manopt.get_bezier_segments-Union{Tuple{P}, Tuple{ManifoldsBase.Manifold,Array{P,1},Any}, Tuple{ManifoldsBase.Manifold,Array{P,1},Any,Symbol}} where P","page":"Bézier curves","title":"Manopt.get_bezier_segments","text":"get_bezier_segments(M::Manifold, c::AbstractArray{P}, d[, s::Symbol=:default])\n\nreturns the array of BezierSegments B of a composite Bézier curve reconstructed from an array c of points on the manifold M and an array of degrees d.\n\nThere are a few (reduced) representations that can get extended; see also get_bezier_points. For ease of the following, let c=(c_1ldotsc_k) and d=(d_1ldotsd_m), where m denotes the number of components the composite Bézier curve consists of. Then\n\n:default – k = m + sum_i=1^m d_i since each component requires one point more than its degree. The points are then orderen in tuples, i.e.\nB = bigl c_1ldotsc_d_1+1 (c_d_1+2ldotsc_d_1+d_2+2ldots c_k-m+1+d_mldotsc_k bigr\n:continuous – k = 1+ sum_i=1m d_i, since for a continuous curve start and end point of sccessive components are the same, so the very first start point and the end points are stored.\nB = bigl c_1ldotsc_d_1+1 c_d_1+1ldotsc_d_1+d_2+1ldots c_k-1+d_mldotsb_k) bigr\n:differentiable – for a differentiable function additionally to the last explanation, also the second point of any segment was not stored except for the first segment. Hence k = 2 - m + sum_i=1m d_i and at a junction point b_n with its given prior point c_n-1, i.e. this is the last inner point of a segment, the first inner point in the next segment the junction is computed as b = exp_c_n(-log_c_n c_n-1) such that the assumed differentiability holds\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Literature-1","page":"Bézier curves","title":"Literature","text":"","category":"section"},{"location":"solvers/ChambollePock.html#ChambollePockSolver-1","page":"Chambolle-Pock","title":"The Riemannian Chambolle-Pock Algorithm","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"The Riemannian Chambolle–Pock is a generalization of the Chambolle–Pock algorithm[ChambollePock2011]. It is also known as primal dual hybrig gradient (PDHG) or primal dual proximal splitting (PDPS) algorithm.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"In order to minimize over p\\in\\mathcal M§ the cost function consisting of","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"F(p) + G(Lambda(p))","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"where Fmathcal M to overlinemathbb R, Gmathcal N to overlinemathbb R, and Lambdamathcal M tomathcal N. If the manifolds mathcal M or mathcal N are not Hadamard, it has to be considered locally, i.e. on geodesically convex sets mathcal C subset mathcal M and mathcal D subsetmathcal N such that Lambda(mathcal C) subset mathcal D.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"The algorithm is available in four variants: exact versus linearized (see variant) as well as with primal versus dual relaxation (see relax). For more details, see [BergmannHerzogSilvaLouzeiroTenbrinckVidalNunez2020]. In the following we note the case of the exact, primal relaxed Riemannian Chambolle–Pock algorithm.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"Given base points minmathcal C, n=Lambda(m)inmathcal D, initial primal and dual values p^(0) in mathcal C, xi_n^(0) in T_n^*mathcal N, and primal and dual step sizes sigma_0, tau_0, relaxation theta_0, as well as acceleration gamma.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"As an initialization, perform bar p^(0) gets p^(0).","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"The algorithms performs the steps k=1ldots (until a StoppingCriterion is fulfilled with)","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"xi^(k+1)_n = operatornameprox_tau_k G_n^*Bigl(xi_n^(k) + tau_k bigl(log_n Lambda (bar p^(k))bigr)^flatBigr)\np^(k+1) = operatornameprox_sigma_k Fbiggl(exp_p^(k)Bigl( operatornamePT_p^(k)gets mbigl(-sigma_k DLambda(m)^*xi_n^(k+1)bigr)^sharpBigr)biggr)\nUpdate\ntheta_k = (1+2gammasigma_k)^-frac12\nsigma_k+1 = sigma_ktheta_k\ntau_k+1 =  fractau_ktheta_k\nbar p^(k+1)  = exp_p^(k+1)bigl(-theta_k log_p^(k+1) p^(k)bigr)","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"Furthermore you can exchange the exponential map, the logarithmic map, and the parallel transport by a retraction, an in verse retraction and a vector transport.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"Finally you can also update the base points m and n during the iterations. This introduces a few additional vector transports. The same holds for the case that Lambda(m^(k))neq n^(k) at some point. All these cases are covered in the algorithm.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"ChambollePock\nChambollePock!","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.ChambollePock","page":"Chambolle-Pock","title":"Manopt.ChambollePock","text":"ChambollePock(M, N, cost, x0, ξ0, m, n, prox_F, prox_G_dual, forward_operator, adjoint_DΛ)\n\nPerform the Riemannian Chambolle–Pock algorithm.\n\nGiven a cost function mathcal Ecolonmathcal M to ℝ of the form\n\nmathcal E(x) = F(x) + G( Λ(x) )\n\nwhere Fcolonmathcal M to ℝ, Gcolonmathcal N to ℝ, and Lambdacolonmathcal M to mathcal N. The remaining input parameters are\n\nx,ξ primal and dual start points xinmathcal M and xiin T_nmathcal N\nm,n base points on mathcal M and mathcal N, respectively.\nforward_operator the operator Λ() or its linearization DΛ(), depending on whether :exact or :linearized is chosen.\nadjoint_linearized_operator the adjoint DΛ^* of the linearized operator DΛ(m)colon T_mmathcal M to T_Λ(m)mathcal N\nprox_F, prox_G_Dual the proximal maps of F and G^ast_n\n\nBy default, this performs the exact Riemannian Chambolle Pock algorithm, see the opional parameter DΛ for ther linearized variant.\n\nFor more details on the algorithm, see[BergmannHerzogSilvaLouzeiroTenbrinckVidalNunez2020].\n\nOptional Parameters\n\nacceleration – (0.05)\ndual_stepsize – (1/sqrt(8)) proximnal parameter of the primal prox\nΛ (missing) the exact operator, that is required if the forward operator is linearized; missing indicates, that the forward operator is exact.\nprimal_stepsize – (1/sqrt(8)) proximnal parameter of the dual prox\nrelaxation – (1.)\nrelax – (:primal) whether to relax the primal or dual\nvariant - (:exact if Λ is missing, otherwise :linearized) variant to use. Note that this changes the arguments the forward_operator will be called.\nstopping_criterion – (stopAtIteration(100)) a StoppingCriterion\nupdate_primal_base – (missing) function to update m (identity by default/missing)\nupdate_dual_base – (missing) function to update n (identity by default/missing)\nretraction_method – (ExponentialRetraction()) the rectraction to use\ninverse_retraction_method - (LogarithmicInverseRetraction()) an inverse retraction to use.\nvector_transport_method - (ParallelTransport()) a vector transport to use\n\n[BergmannHerzogSilvaLouzeiroTenbrinckVidalNunez2020]: R. Bergmann, R. Herzog, M. Silva Louzeiro, D. Tenbrinck, J. Vidal-Núñez: Fenchel Duality Theory and a Primal-Dual Algorithm on Riemannian Manifolds, arXiv: 1908.02022 accepted for publication in Foundations of Computational Mathematics\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.ChambollePock!","page":"Chambolle-Pock","title":"Manopt.ChambollePock!","text":"ChambollePock(M, N, cost, x, ξ, m, n, prox_F, prox_G_dual, forward_operator, adjoint_DΛ)\n\nPerform the Riemannian Chambolle–Pock algorithm in place of x, ξ, and potenitally m, n if they are not fixed. See ChambollePock for details and optional parameters.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Problem-and-Options-1","page":"Chambolle-Pock","title":"Problem & Options","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"PrimalDualProblem\nPrimalDualOptions\nChambollePockOptions","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.PrimalDualProblem","page":"Chambolle-Pock","title":"Manopt.PrimalDualProblem","text":"PrimalDualProblem {mT <: Manifold, nT <: Manifold} <: PrimalDualProblem} <: Problem\n\nDescribes a Problem for the linearized Chambolle-Pock algorithm.\n\nFields\n\nM, N – two manifolds mathcal M, mathcal N\ncost F + G(Λ()) to evaluate interims cost function values\nforward_oprator the operator for the forward operation in the algorthm, either Λ (exact) or DΛ (linearized).\nlinearized_adjoint_operator The adjoint differential (DΛ)^* colon mathcal N to Tmathcal M\nprox_F the proximal map belonging to f\nprox_G_dual the proximal map belonging to g_n^*\nΛ – (fordward_operator) for the linearized variant, this has to be set to the exact forward operator. This operator is required in several variants of the linearized algorithm. Since the exact variant is the default, Λ is by default set to forward_operator.\n\nConstructor\n\nLinearizedPrimalDualProblem(M, N, cost, prox_F, prox_G_dual, forward_operator, adjoint_linearized_operator,Λ=forward_operator)\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Manopt.PrimalDualOptions","page":"Chambolle-Pock","title":"Manopt.PrimalDualOptions","text":"PrimalDualOptions\n\nA general type for all primal dual based options to be used within primal dual based algorithms\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Manopt.ChambollePockOptions","page":"Chambolle-Pock","title":"Manopt.ChambollePockOptions","text":"ChambollePockOptions <: PrimalDualOptions\n\nstores all options and variables within a linearized or exact Chambolle Pock. The following list provides the order for the constructor, where the previous iterates are initialized automatically and values with a default may be left out.\n\nm - base point on $ \\mathcal M $\nn - base point on $ \\mathcal N $\nx - an initial point on x^(0) in mathcal M (and its previous iterate)\nξ - an initial tangent vector xi^(0)in T^*mathcal N (and its previous iterate)\nxbar - the relaxed iterate used in the next dual update step (when using :primal relaxation)\nξbar - the relaxed iterate used in the next primal update step (when using :dual relaxation)\nΘ – factor to damp the helping tilde x\nprimal_stepsize – (1/sqrt(8)) proximal parameter of the primal prox\ndual_stepsize – (1/sqrt(8)) proximnal parameter of the dual prox\nacceleration – (0.) acceleration factor due to Chambolle & Pock\nrelaxation – (1.) relaxation in the primal relaxation step (to compute xbar)\nrelax – (_primal) which variable to relax (:primal or :dual)\nstop - a StoppingCriterion\ntype – (exact) whether to perform an :exact or :linearized Chambolle-Pock\nupdate_primal_base ((p,o,i) -> o.m) function to update the primal base\nupdate_dual_base ((p,o,i) -> o.n) function to update the dual base\nretraction_method – (ExponentialRetraction()) the rectraction to use\ninverse_retraction_method - (LogarithmicInverseRetraction()) an inverse retraction to use.\nvector_transport_method - (ParallelTransport()) a vector transport to use\n\nwhere for the last two the functions a Problemp, Optionso and the current iterate i are the arguments. If you activate these to be different from the default identity, you have to provide p.Λ for the algorithm to work (which might be missing in the linearized case).\n\nConstructor\n\nChambollePockOptions(m::P, n::Q, x::P, ξ::T, primal_stepsize::Float64, dual_stepsize::Float64;\n    acceleration::Float64 = 0.0,\n    relaxation::Float64 = 1.0,\n    relax::Symbol = :primal,\n    stopping_criterion::StoppingCriterion = StopAfterIteration(300),\n    variant::Symbol = :exact,\n    update_primal_base::Union{Function,Missing} = missing,\n    update_dual_base::Union{Function,Missing} = missing,\n    retraction_method = ExponentialRetraction(),\n    inverse_retraction_method = LogarithmicInverseRetraction(),\n    vector_transport_method = ParallelTransport(),\n)\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Useful-Terms-1","page":"Chambolle-Pock","title":"Useful Terms","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"primal_residual\ndual_residual","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.primal_residual","page":"Chambolle-Pock","title":"Manopt.primal_residual","text":"primal_residual(p, o, x_old, ξ_old, n_old)\n\nCompute the primal residual at current iterate k given the necessary values x_k-1 ξ_k-1 and n_k-1 from the previous iterate.\n\nBigllVert\nfrac1σoperatornameretr^-1_x_kx_k-1 -\nV_x_kgets m_kbigl(DΛ^*(m_k)biglV_n_kgets n_k-1ξ_k-1 - ξ_k bigr\nBigrrVert\n\nwhere V_cdotgetscdot is the vector transport used in the ChambollePockOptions\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.dual_residual","page":"Chambolle-Pock","title":"Manopt.dual_residual","text":"dual_residual(p, o, x_old, ξ_old, n_old)\n\nCompute the dual residual at current iterate k given the necessary values x_k-1 ξ_k-1 and n_k-1 from the previous iterate. The formula is slightly different depending on the o.variant used:\n\nFor the :lineaized it reads\n\nBigllVert\nfrac1τbigl(\nV_n_kgets n_k-1(ξ_k-1)\n- ξ_k\nbigr)\n-\nDΛ(m_k)bigl\nV_m_kgets x_koperatornameretr^-1_x_kx_k-1\nbigr\nBigrrVert\n\nand for the :exact variant\n\nBigllVert\nfrac1τ V_n_kgets n_k-1(ξ_k-1)\n-\noperatornameretr^-1_n_kbigl(\nΛ(operatornameretr_m_k(V_m_kgets x_koperatornameretr^-1_x_kx_k-1))\nbigr)\nBigrrVert\n\nwhere in both cases V_cdotgetscdot is the vector transport used in the ChambollePockOptions.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Debug-1","page":"Chambolle-Pock","title":"Debug","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"DebugDualBaseIterate\nDebugDualBaseChange\nDebugPrimalBaseIterate\nDebugPrimalBaseChange\nDebugDualChange\nDebugDualIterate\nDebugDualResidual\nDebugPrimalChange\nDebugPrimalIterate\nDebugPrimalResidual\nDebugPrimalDualResidual","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualBaseIterate","page":"Chambolle-Pock","title":"Manopt.DebugDualBaseIterate","text":"DebugDualBaseIterate(io::IO=stdout)\n\nPrint the dual base variable by using DebugEntry, see their constructors for detail. This method is further set display o.n.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualBaseChange","page":"Chambolle-Pock","title":"Manopt.DebugDualBaseChange","text":"DebugDualChange(a=StoreOptionsAction((:ξ)),io::IO=stdout)\n\nPrint the change of the dual base variable by using DebugEntryChange, see their constructors for detail, on o.n.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalBaseIterate","page":"Chambolle-Pock","title":"Manopt.DebugPrimalBaseIterate","text":"DebugPrimalBaseIterate(io::IO=stdout)\n\nPrint the primal base variable by using DebugEntry, see their constructors for detail. This method is further set display o.m.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalBaseChange","page":"Chambolle-Pock","title":"Manopt.DebugPrimalBaseChange","text":"DebugPrimalBaseChange(a::StoreOptionsAction=StoreOptionsAction((:m)),io::IO=stdout)\n\nPrint the change of the primal base variable by using DebugEntryChange, see their constructors for detail, on o.n.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualChange","page":"Chambolle-Pock","title":"Manopt.DebugDualChange","text":"DebugDualChange(opts...)\n\nPrint the change of the dual variable, similar to DebugChange, see their constructors for detail, but with a different calculation of the change, since the dual variable lives in (possibly different) tangent spaces.\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualIterate","page":"Chambolle-Pock","title":"Manopt.DebugDualIterate","text":"DebugDualIterate(e)\n\nPrint the dual variable by using DebugEntry, see their constructors for detail. This method is further set display o.ξ.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualResidual","page":"Chambolle-Pock","title":"Manopt.DebugDualResidual","text":"DebugDualResidual <: DebugAction\n\nA Debug action to print the dual residual. The constructor accepts a printing function and some (shared) storage, which should at least record :x, :ξ and :n.\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalChange","page":"Chambolle-Pock","title":"Manopt.DebugPrimalChange","text":"DebugPrimalChange(opts...)\n\nPrint the change of the primal variable by using DebugChange, see their constructors for detail.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalIterate","page":"Chambolle-Pock","title":"Manopt.DebugPrimalIterate","text":"DebugPrimalIterate(opts...)\n\nPrint the change of the primal variable by using DebugIterate, see their constructors for detail.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalResidual","page":"Chambolle-Pock","title":"Manopt.DebugPrimalResidual","text":"DebugPrimalResidual <: DebugAction\n\nA Debug action to print the primal residual. The constructor accepts a printing function and some (shared) storage, which should at least record :x, :ξ and :n.\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalDualResidual","page":"Chambolle-Pock","title":"Manopt.DebugPrimalDualResidual","text":"DebugPrimalDualResidual <: DebugAction\n\nA Debug action to print the primaldual residual. The constructor accepts a printing function and some (shared) storage, which should at least record :x, :ξ and :n.\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Record-1","page":"Chambolle-Pock","title":"Record","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"RecordDualBaseIterate\nRecordDualBaseChange\nRecordDualChange\nRecordDualIterate\nRecordPrimalBaseIterate\nRecordPrimalBaseChange\nRecordPrimalChange\nRecordPrimalIterate","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.RecordDualBaseIterate","page":"Chambolle-Pock","title":"Manopt.RecordDualBaseIterate","text":"RecordDualBaseIterate(n)\n\nCreate an RecordAction that records the dual base point, i.e. RecordEntry of o.n.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordDualBaseChange","page":"Chambolle-Pock","title":"Manopt.RecordDualBaseChange","text":"RecordDualBaseChange(e)\n\nCreate an RecordAction that records the dual base point change, i.e. RecordEntryChange of o.n with distance to the last value to store a value.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordDualChange","page":"Chambolle-Pock","title":"Manopt.RecordDualChange","text":"RecordDualChange()\n\nCreate the action either with a given (shared) Storage, which can be set to the values Tuple, if that is provided).\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordDualIterate","page":"Chambolle-Pock","title":"Manopt.RecordDualIterate","text":"RecordDualIterate(ξ)\n\nCreate an RecordAction that records the dual base point, i.e. RecordEntry of o.ξ, so .\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordPrimalBaseIterate","page":"Chambolle-Pock","title":"Manopt.RecordPrimalBaseIterate","text":"RecordPrimalBaseIterate(x)\n\nCreate an RecordAction that records the primal base point, i.e. RecordEntry of o.m.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordPrimalBaseChange","page":"Chambolle-Pock","title":"Manopt.RecordPrimalBaseChange","text":"RecordPrimalBaseChange()\n\nCreate an RecordAction that records the primal base point change, i.e. RecordEntryChange of o.m with distance to the last value to store a value.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordPrimalChange","page":"Chambolle-Pock","title":"Manopt.RecordPrimalChange","text":"RecordPrimalChange(a)\n\nCreate an RecordAction that records the primal value change, i.e. RecordChange, since we just redord the change of o.x.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordPrimalIterate","page":"Chambolle-Pock","title":"Manopt.RecordPrimalIterate","text":"RecordDualBaseIterate(x)\n\nCreate an RecordAction that records the dual base point, i.e. RecordIterate, i.e. o.x.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Internals-1","page":"Chambolle-Pock","title":"Internals","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"Manopt.update_prox_parameters!","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.update_prox_parameters!","page":"Chambolle-Pock","title":"Manopt.update_prox_parameters!","text":"update_prox_parameters!(o)\n\nupdate the prox parameters as described in Algorithm 2 of Chambolle, Pock, 2010, i.e.\n\ntheta_n = frac1sqrt1+2gammatau_n\ntau_n+1 = theta_ntau_n\nsigma_n+1 = fracsigma_ntheta_n\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"[ChambollePock2011]: A. Chambolle, T. Pock: A first-order primal-dual algorithm for convex problems with applications to imaging, Journal of Mathematical Imaging and Vision 40(1), 120–145, 2011. doi: 10.1007/s10851-010-0251-1","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/MeanAndMedian.jl\"","category":"page"},{"location":"tutorials/MeanAndMedian.html#Optimize-1","page":"get Started: Optimize!","title":"Get started: Optimize!","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"This example illustrates how to set up and solve optimization problems and how to further get data from the algorithm using DebugOptions and RecordOptions","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"To start from the quite general case: A Solver is an algorithm that aims to solve","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"operatorname*argmin_xmathcal M f(x)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"where mathcal M is a Manifold and fcolonmathcal M to mathbb R is the cost function.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"In Manopt.jl a Solver is an algorithm that requires a Problem p and Options o. While former contains static data, most prominently the manifold mathcal M (usually as p.M) and the cost function f (usually as p.cost), the latter contains dynamic data, i.e. things that usually change during the algorithm, are allowed to change, or specify the details of the algorithm to use. Together they form a plan. A plan uniquely determines the algorithm to use and provide all necessary information to run the algorithm.","category":"page"},{"location":"tutorials/MeanAndMedian.html#Example-1","page":"get Started: Optimize!","title":"Example","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"A gradient plan consists of a GradientProblem with the fields M, cost function f as well as gradient storing the gradient function corresponding to f. Accessing both functions can be done directly but should be encapsulated using get_cost(p,x) and get_gradient(p,x), where in both cases x is a point on the Manifold M. Second, the GradientDescentOptions specify that the algorithm to solve the GradientProblem will be the gradient descent algorithm. It requires an initial value o.x0, a StoppingCriterion o.stop, a Stepsize o.stepsize and a retraction o.retraction and it internally stores the last evaluation of the gradient at o.∇ for convenience. The only mandatory parameter is the initial value x0, though the defaults for both the stopping criterion (StopAfterIteration(100)) as well as the stepsize (ConstantStepsize(1.) are quite conservative, but are chosen to be as simple as possible.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"With these two at hand, running the algorithm just requires to call x_opt = solve(p,o).","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"In the following two examples we will see, how to use a higher level interface that allows to more easily activate for example a debug output or record values during the iterations","category":"page"},{"location":"tutorials/MeanAndMedian.html#The-given-Dataset-1","page":"get Started: Optimize!","title":"The given Dataset","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"using Manopt, Manifolds\nusing Random, Colors","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"For a persistent random set we use","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"n = 100\nσ = π / 8\nM = Sphere(2)\nx = 1 / sqrt(2) * [1.0, 0.0, 1.0]\nRandom.seed!(42)\ndata = [exp(M, x, random_tangent(M, x, Val(:Gaussian), σ)) for i in 1:n]\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"black = RGBA{Float64}(colorant\"#000000\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\")\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\")\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\")\nTolVibrantMagenta = RGBA{Float64}(colorant\"#EE3377\")\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"Then our data rendered using asymptote_export_S2_signals looks like","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"asymptote_export_S2_signals(\"startDataAndCenter.asy\";\n    points = [ [x], data],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal]),\n    dot_size = 3.5, camera_position = (1.,.5,.5)\n)\nrender_asymptote(\"startDataAndCenter.asy\"; render = 2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"(Image: The data of noisy versions of $x$)","category":"page"},{"location":"tutorials/MeanAndMedian.html#Computing-the-Mean-1","page":"get Started: Optimize!","title":"Computing the Mean","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"To compute the mean on the manifold we use the characterization, that the Euclidean mean minimizes the sum of squared distances, and end up with the following cost function. Its minimizer is called Riemannian Center of Mass.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"F = y -> sum(1 / (2 * n) * distance.(Ref(M), Ref(y), data) .^ 2)\n∇F = y -> sum(1 / n * ∇distance.(Ref(M), data, Ref(y)))\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"note that the ∇distance defaults to the case p=2, i.e. the gradient of the squared distance. For details on convergence of the gradient descent for this problem, see [Afsari, Tron, Vidal, 2013]","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"The easiest way to call the gradient descent is now to call gradient_descent","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"xMean = gradient_descent(M, F, ∇F, data[1])\nnothing; #hide\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"but in order to get more details, we further add the debug= options, which act as a decorator pattern using the DebugOptions and DebugActions. The latter store values if that's necessary, for example for the DebugChange that prints the change during the last iteration. The following debug prints","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"# i | x: | Last Change: | F(x):`","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"as well as the reason why the algorithm stopped at the end. Here, the format shorthand and the [DebugFactory] are used, which returns a DebugGroup of DebugAction performed each iteration and the stop, respectively.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"xMean = gradient_descent(\n    M,\n    F,\n    ∇F,\n    data[1];\n    debug=[:Iteration, \" | \", :x, \" | \", :Change, \" | \", :Cost, \"\\n\", :Stop],\n)\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"asymptote_export_S2_signals(\"startDataCenterMean.asy\";\n    points = [ [x], data, [xMean] ],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal, TolVibrantOrange]),\n    dot_size = 3.5, camera_position = (1.,.5,.5)\n)\nrender_asymptote(\"startDataCenterMean.asy\"; render = 2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"(Image: The resulting mean (orange))","category":"page"},{"location":"tutorials/MeanAndMedian.html#Computing-the-Median-1","page":"get Started: Optimize!","title":"Computing the Median","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"Similar to the mean you can also define the median as the minimizer of the distances, see for example [Bačák, 2014], but since this problem is not differentiable, we employ the Cyclic Proximal Point (CPP) algorithm, described in the same reference. We define","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"F2 = y -> sum(1 / (2 * n) * distance.(Ref(M), Ref(y), data))\nproxes = Function[(λ, y) -> prox_distance(M, λ / n, di, y, 1) for di in data]\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"where the Function is a helper for global scope to infer the correct type.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"We then call the cyclic_proximal_point as","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"o = cyclic_proximal_point(\n    M,\n    F2,\n    proxes,\n    data[1];\n    debug=[:Iteration, \" | \", :x, \" | \", :Change, \" | \", :Cost, \"\\n\", 50, :Stop],\n    record=[:Iteration, :Change, :Cost],\n    return_options=true,\n)\nxMedian = get_solver_result(o)\nvalues = get_record(o)\nnothing # hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"where the differences to gradient_descent are as follows","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"the third parameter is now an Array of proximal maps\ndebug is reduces to only every 50th iteration\nwe further activated a RecordAction using the record= optional parameter. These work very similar to those in debug, but they collect their data in an array. The high level interface then returns two variables; the values do contain an array of recorded datum per iteration. Here a Tuple containing the iteration, last change and cost respectively; see RecordGroup, RecordIteration, RecordChange, RecordCost as well as the RecordFactory for details. The values contains hence a tuple per iteration, that itself consists of (by order of specification) the iteration number, the last change and the cost function value.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"These recorded entries read","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"values","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"The resulting median and mean for the data hence are","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"asymptote_export_S2_signals(\"startDataCenterMean.asy\";\n    points = [ [x], data, [xMean], [xMedian] ],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal, TolVibrantOrange, TolVibrantMagenta]),\n    dot_size = 3.5, camera_position = (1.,.5,.5)\n)\nrender_asymptote(\"startDataCenterMedianAndMean.asy\"; render = 2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"(Image: The resulting mean (orange) and median (magenta))","category":"page"},{"location":"tutorials/MeanAndMedian.html#Literature-1","page":"get Started: Optimize!","title":"Literature","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"<ul>\n<li id=\"Bačák2014\">[<a>Bačák, 2014</a>]\n  Bačák, M: <emph>Computing Medians and Means in Hadamard Spaces.</emph>,\n  SIAM Journal on Optimization, Volume 24, Number 3, pp. 1542–1566,\n  doi: <a href=\"https://doi.org/10.1137/140953393\">10.1137/140953393</a>,\n  arxiv: <a href=\"https://arxiv.org/abs/1210.2145\">1210.2145</a>.</li>\n  <li id=\"AfsariTronVidal2013\">[<a>Afsari, Tron, Vidal, 2013</a>]\n   Afsari, B; Tron, R.; Vidal, R.: <emph>On the Convergence of Gradient\n   Descent for Finding the Riemannian Center of Mass</emph>,\n   SIAM Journal on Control and Optimization, Volume 51, Issue 3,\n   pp. 2230–2260.\n   doi: <a href=\"https://doi.org/10.1137/12086282X\">10.1137/12086282X</a>,\n   arxiv: <a href=\"https://arxiv.org/abs/1201.0925\">1201.0925</a></li>\n</ul>","category":"page"},{"location":"helpers/data.html#Data-1","page":"Data","title":"Data","text":"","category":"section"},{"location":"helpers/data.html#","page":"Data","title":"Data","text":"For some manifolds there are artificial or real application data available that can be loaded using the following data functions","category":"page"},{"location":"helpers/data.html#","page":"Data","title":"Data","text":"Modules = [Manopt]\nPages   = [\"artificialDataFunctions.jl\"]","category":"page"},{"location":"helpers/data.html#Manopt.artificialIn_SAR_image-Tuple{Integer}","page":"Data","title":"Manopt.artificialIn_SAR_image","text":"artificialIn_SAR_image([pts=500])\n\ngenerate an artificial InSAR image, i.e. phase valued data, of size pts x pts points.\n\nThis data set was introduced for the numerical examples in\n\nBergmann, R., Laus, F., Steidl, G., Weinmann, A.: Second Order Differences of Cyclic Data and Applications in Variational Denoising SIAM J. Imaging Sci., 7(4), 2916–2953, 2014. doi: 10.1137/140969993 arxiv: 1405.5349\n\n\n\n\n\n","category":"method"},{"location":"helpers/data.html#Manopt.artificial_S1_signal","page":"Data","title":"Manopt.artificial_S1_signal","text":"artificial_S1_signal([pts=500])\n\ngenerate a real-valued signal having piecewise constant, linear and quadratic intervals with jumps in between. If the resulting manifold the data lives on, is the Circle the data is also wrapped to -pipi).\n\nOptional\n\npts – (500) number of points to sample the function\n\nBergmann, R., Laus, F., Steidl, G., Weinmann, A.: Second Order Differences of Cyclic Data and Applications in Variational Denoising SIAM J. Imaging Sci., 7(4), 2916–2953, 2014. doi: 10.1137/140969993 arxiv: 1405.5349\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S1_signal-Tuple{Real}","page":"Data","title":"Manopt.artificial_S1_signal","text":"artificial_S1_signal(x)\n\nevaluate the example signal f(x) x   01, of phase-valued data introduces in Sec. 5.1 of\n\nBergmann, R., Laus, F., Steidl, G., Weinmann, A.: Second Order Differences of Cyclic Data and Applications in Variational Denoising SIAM J. Imaging Sci., 7(4), 2916–2953, 2014. doi: 10.1137/140969993 arxiv: 1405.5349\n\nfor values outside that intervall, this Signal is missing.\n\n\n\n\n\n","category":"method"},{"location":"helpers/data.html#Manopt.artificial_S1_slope_signal","page":"Data","title":"Manopt.artificial_S1_slope_signal","text":"artificial_S1_slope_signal([pts=500, slope=4.])\n\nCreates a Signal of (phase-valued) data represented on the CircleManifold with increasing slope.\n\nOptional\n\npts – (500) number of points to sample the function.\nslope – (4.0) initial slope that gets increased afterwards\n\nThis data set was introduced for the numerical examples in\n\nBergmann, R., Laus, F., Steidl, G., Weinmann, A.: Second Order Differences of Cyclic Data and Applications in Variational Denoising SIAM J. Imaging Sci., 7(4), 2916–2953, 2014. doi: 10.1137/140969993 arxiv: 1405.5349\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_composite_bezier_curve-Tuple{}","page":"Data","title":"Manopt.artificial_S2_composite_bezier_curve","text":"artificial_S2_composite_bezier_curve()\n\nCreate the artificial curve in the Sphere(2) consisting of 3 segments between the four points\n\np_0 = beginbmatrix001endbmatrix^mathrmT\np_1 = beginbmatrix0-10endbmatrix^mathrmT\np_2 = beginbmatrix-100endbmatrix^mathrmT\np_3 = beginbmatrix00-1endbmatrix^mathrmT\n\nwhere each segment is a cubic Bezér curve, i.e. each point, except p_3 has a first point within the following segment b_i^+, i=012 and a last point within the previous segment, except for p_0, which are denoted by b_i^-, i=123. This curve is differentiable by the conditions b_i^- = gamma_b_i^+p_i(2), i=12, where gamma_ab is the shortest_geodesic connecting a and b. The remaining points are defined as\n\nbeginaligned\n    b_0^+ = exp_p_0fracpi8sqrt2beginpmatrix1-10endpmatrix^mathrmT\n    b_1^+ = exp_p_1-fracpi4sqrt2beginpmatrix-101endpmatrix^mathrmT\n    b_2^+ = exp_p_2fracpi4sqrt2beginpmatrix01-1endpmatrix^mathrmT\n    b_3^- = exp_p_3-fracpi8sqrt2beginpmatrix-110endpmatrix^mathrmT\nendaligned\n\nThis example was used within minimization of acceleration of the paper\n\nBergmann, R., Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve, Front. Appl. Math. Stat. 12, 2018. doi: 10.3389/fams.2018.00059 arxiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"helpers/data.html#Manopt.artificial_S2_lemniscate","page":"Data","title":"Manopt.artificial_S2_lemniscate","text":"artificial_S2_lemniscate(p,t; a=π/2)\n\ngenerate a point from the signal on the Sphere mathbb S^2 by creating the Lemniscate of Bernoulli in the tangent space of p sampled at t and use èxp` to obtain a point on the Sphere.\n\nInput\n\np – the tangent space the Lemniscate is created in\nt – value to sample the Lemniscate at\n\nOptional Values\n\na – (π/2) defines a half axis of the Lemniscate to cover a half sphere.\n\nThis dataset was used in the numerical example of Section 5.1 of\n\nBačák, M., Bergmann, R., Steidl, G., Weinmann, A.: A Second Order Non-Smooth Variational Model for Restoring Manifold-Valued Images SIAM J. Sci. Comput. 38(1), A567–A597, 2016. doi: 10.1137/15M101988X arxiv: 1506.02409\n\n\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_lemniscate","page":"Data","title":"Manopt.artificial_S2_lemniscate","text":"artificial_S2_lemniscate(p [,pts=128,a=π/2,interval=[0,2π])\n\ngenerate a Signal on the Sphere mathbb S^2 by creating the Lemniscate of Bernoulli in the tangent space of p sampled at pts points and use exp to get a signal on the Sphere.\n\nInput\n\np – the tangent space the Lemniscate is created in\npts – (128) number of points to sample the Lemniscate\na – (π/2) defines a half axis of the Lemniscate to cover a  half sphere.\ninterval – ([0,2*π]) range to sample the lemniscate at, the default value refers to one closed curve\n\nThis dataset was used in the numerical example of Section 5.1 of\n\nBačák, M., Bergmann, R., Steidl, G., Weinmann, A.: A Second Order Non-Smooth Variational Model for Restoring Manifold-Valued Images SIAM J. Sci. Comput. 38(1), A567–A597, 2016. doi: 10.1137/15M101988X arxiv: 1506.02409\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_rotation_image","page":"Data","title":"Manopt.artificial_S2_rotation_image","text":"artificial_S2_rotation_image([pts=64, rotations=(.5,.5)])\n\ncreates an image with a rotation on each axis as a parametrization.\n\nOptional Parameters\n\npts – (64) number of pixels along one dimension\nrotations – ((.5,.5)) number of total rotations performed on the axes.\n\nThis dataset was used in the numerical example of Section 5.1 of\n\nBačák, M., Bergmann, R., Steidl, G., Weinmann, A.: A Second Order Non-Smooth Variational Model for Restoring Manifold-Valued Images SIAM J. Sci. Comput. 38(1), A567–A597, 2016. doi: 10.1137/15M101988X arxiv: 1506.02409\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_whirl_image","page":"Data","title":"Manopt.artificial_S2_whirl_image","text":"artificial_S2_whirl_image([pts=64])\n\ngenerate an artificial image of data on the 2 sphere,\n\nArguments\n\npts – (64) size of the image in ptstimespts pixel.\n\nThis example dataset was used in the numerical example in Section 5.5 of\n\nLaus, F., Nikolova, M., Persch, J., Steidl, G.: A Nonlocal Denoising Algorithm for Manifold-Valued Images Using Second Order Statistics, SIAM J. Imaging Sci., 10(1), 416–448, 2017. doi:  10.1137/16M1087114 arxiv: 1607.08481\n\nIt is based on artificial_S2_rotation_image extended by small whirl patches.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_whirl_patch","page":"Data","title":"Manopt.artificial_S2_whirl_patch","text":"artificial_S2_whirl_patch([pts=5])\n\ncreate a whirl within the ptstimespts patch of Sphere(@ref)(2)-valued image data.\n\nThese patches are used within artificial_S2_whirl_image.\n\nOptional Parameters\n\npts – (5) size of the patch. If the number is odd, the center is the north pole.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_SPD_image","page":"Data","title":"Manopt.artificial_SPD_image","text":"artificial_SPD_image([pts=64, stepsize=1.5])\n\ncreate an artificial image of symmetric positive definite matrices of size ptstimespts pixel with a jump of size stepsize.\n\nThis dataset was used in the numerical example of Section 5.2 of\n\nBačák, M., Bergmann, R., Steidl, G., Weinmann, A.: A Second Order Non-Smooth Variational Model for Restoring Manifold-Valued Images SIAM J. Sci. Comput. 38(1), A567–A597, 2016. doi: 10.1137/15M101988X arxiv: 1506.02409\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_SPD_image2","page":"Data","title":"Manopt.artificial_SPD_image2","text":"artificial_SPD_image2([pts=64, fraction=.66])\n\ncreate an artificial image of symmetric positive definite matrices of size ptstimespts pixel with right hand side fraction is moved upwards.\n\nThis data set was introduced in the numerical examples of Section of\n\nBergmann, R., Persch, J., Steidl, G.: A Parallel Douglas Rachford Algorithm for Minimizing ROF-like Functionals on Images with Values in Symmetric Hadamard Manifolds SIAM J. Imaging. Sci. 9(3), pp. 901-937, 2016. doi: 10.1137/15M1052858 arxiv: 1512.02814\n\n\n\n\n\n","category":"function"},{"location":"solvers/subgradient.html#SubgradientSolver-1","page":"Subgradient method","title":"Subgradient Method","text":"","category":"section"},{"location":"solvers/subgradient.html#","page":"Subgradient method","title":"Subgradient method","text":"subgradient_method\nsubgradient_method!","category":"page"},{"location":"solvers/subgradient.html#Manopt.subgradient_method","page":"Subgradient method","title":"Manopt.subgradient_method","text":"subgradient_method(M, F, ∂F, x)\n\nperform a subgradient method x_k+1 = mathrmretr(x_k s_kF(x_k)),\n\nwhere mathrmretr is a retraction, s_k can be specified as a function but is usually set to a constant value. Though the subgradient might be set valued, the argument ∂F should always return one element from the subgradient, but not necessarily determistic.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n∂F: the (sub)gradient partial Fcolonmathcal Mto Tmathcal M of F restricted to always only returning one value/element from the subgradient\nx – an initial value x  mathcal M\n\nOptional\n\nstepsize – (ConstantStepsize(1.)) specify a Stepsize\nretraction – (exp) a retraction(M,x,ξ) to use.\nstopping_criterion – (StopAfterIteration(5000)) a functor, seeStoppingCriterion, indicating when to stop.\nreturn_options – (false) – if activated, the extended result, i.e. the complete Options re returned. This can be used to access recorded values. If set to false (default) just the optimal value x_opt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of the subgradient method\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/subgradient.html#Manopt.subgradient_method!","page":"Subgradient method","title":"Manopt.subgradient_method!","text":"subgradient_method!(M, F, ∂F, x)\n\nperform a subgradient method x_k+1 = mathrmretr(x_k s_kF(x_k)) in place of x\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n∂F: the (sub)gradient partial Fcolonmathcal Mto Tmathcal M of F restricted to always only returning one value/element from the subgradient\nx – an initial value x  mathcal M\n\nfor more details and all optional parameters, see subgradient_method.\n\n\n\n\n\n","category":"function"},{"location":"solvers/subgradient.html#Options-1","page":"Subgradient method","title":"Options","text":"","category":"section"},{"location":"solvers/subgradient.html#","page":"Subgradient method","title":"Subgradient method","text":"SubGradientMethodOptions","category":"page"},{"location":"solvers/subgradient.html#Manopt.SubGradientMethodOptions","page":"Subgradient method","title":"Manopt.SubGradientMethodOptions","text":"SubGradientMethodOptions <: Options\n\nstories option values for a subgradient_method solver\n\nFields\n\nretraction_method – the retration to use within\nstepsize – a Stepsize\nstop – a StoppingCriterion\nx – (initial or current) value the algorithm is at\nx_optimal – optimal value\n∂ the current element from the possivle subgradients at x that is used\n\n\n\n\n\n","category":"type"},{"location":"solvers/subgradient.html#","page":"Subgradient method","title":"Subgradient method","text":"For DebugActions and RecordActions to record (sub)gradient, its norm and the step sizes, see the steepest Descent actions.","category":"page"},{"location":"functions/manifold.html#Specific-manifold-functions-1","page":"Specific Manifold Functions","title":"Specific manifold functions","text":"","category":"section"},{"location":"functions/manifold.html#","page":"Specific Manifold Functions","title":"Specific Manifold Functions","text":"This small section extends the functions available from ManifoldsBase.jl and Manifolds.jl, espcially a few random generators, that are simpler than the functions available.","category":"page"},{"location":"functions/manifold.html#","page":"Specific Manifold Functions","title":"Specific Manifold Functions","text":"Modules = [Manopt]\nPages   = [\"manifold.jl\"]","category":"page"},{"location":"functions/manifold.html#ManifoldsBase.mid_point-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Specific Manifold Functions","title":"ManifoldsBase.mid_point","text":"mid_point(M, p, q, x)\n\nCompute the mid point between p and q. If there is more than one mid point of (not neccessarily minimizing) geodesics (i.e. on the sphere), the one nearest to x is returned.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.reflect-Tuple{ManifoldsBase.Manifold,Any,Any}","page":"Specific Manifold Functions","title":"Manopt.reflect","text":"reflect(M, p, x)\n\nreflect the point x from the manifold M at point x, i.e.\n\n    operatornamerefl_p(x) = exp_p(-log_p x)\n\nwhere exp and log denote the exponential and logarithmic map on M.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.sym_rem-Union{Tuple{N}, Tuple{N}, Tuple{N,Any}} where N<:Number","page":"Specific Manifold Functions","title":"Manopt.sym_rem","text":"sym_rem(x,[T=π])\n\nCompute symmetric remainder of x with respect to the interall 2*T, i.e. (x+T)%2T, where the default for T is π\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Simplified-random-functions-1","page":"Specific Manifold Functions","title":"Simplified random functions","text":"","category":"section"},{"location":"functions/manifold.html#","page":"Specific Manifold Functions","title":"Specific Manifold Functions","text":"While statistics are available in Manifolds.jl, the following functions provide default random points and vectors on manifolds.","category":"page"},{"location":"functions/manifold.html#","page":"Specific Manifold Functions","title":"Specific Manifold Functions","text":"Modules = [Manopt]\nPages   = [\"random.jl\"]","category":"page"},{"location":"functions/manifold.html#Manopt.random_point","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Sphere, :Gaussian[, σ=1.0])\n\nreturn a random point on the Sphere by projecting a normal distirbuted vector from within the embedding to the sphere.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_point","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Rotations, :Gaussian [, σ=1.0])\n\nreturn a random point p on the manifold Rotations by generating a (Gaussian) random orthogonal matrix with determinant +1. Let QR = A be the QR decomposition of a random matrix A, then the formula reads p = QD where D is a diagonal matrix with the signs of the diagonal entries of R, i.e.\n\nD_ij=begincases\noperatornamesgn(R_ij)  textif  i=j \n0   textotherwise\nendcases\n\nIt can happen that the matrix gets -1 as a determinant. In this case, the first and second columns are swapped.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{Manifolds.Circle,Val{:Uniform}}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Circle, :Uniform)\n\nreturn a random point on the Circle mathbb S^1 by picking a random element from -pipi) uniformly.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{Manifolds.Euclidean}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Euclidean[,:Gaussian, σ::Float64=1.0])\n\ngenerate a random point on the Euclidean manifold M, where the optional parameter determines the type of the entries of the resulting point on the Euclidean space d.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{Manifolds.ProductManifold,Vararg{Any,N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::ProductManifold, options...)\n\nreturn a random point x on Grassmannian manifold M by generating a random (Gaussian) matrix with standard deviation σ in matching size, which is orthonormal.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{ManifoldsBase.Manifold,Symbol,Vararg{Any,N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Manidold, s::Symbol, options...)\n\ngenerate a random point using a noise model given by s with its additional options just passed on.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{ManifoldsBase.Manifold}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Manidold)\n\ngenerate a random point on a manifold. By default it uses random_point(M,:Gaussian).\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{Mt}, Tuple{𝔽}, Tuple{ManifoldsBase.AbstractPowerManifold{𝔽,Mt,ManifoldsBase.NestedPowerRepresentation},Vararg{Any,N} where N}} where Mt where 𝔽","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::AbstractPowerManifold, options...)\n\ngenerate a random point on the AbstractPowerManfold M given options that are passed on.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{N}, Tuple{Manifolds.SymmetricPositiveDefinite{N},Val{:Gaussian}}, Tuple{Manifolds.SymmetricPositiveDefinite{N},Val{:Gaussian},Float64}} where N","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::SymmetricPositiveDefinite, :Gaussian[, σ=1.0])\n\ngerenate a random symmetric positive definite matrix on the SymmetricPositiveDefinite manifold M.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{𝔽}, Tuple{k}, Tuple{n}, Tuple{Manifolds.Grassmann{n,k,𝔽},Val{:Gaussian}}, Tuple{Manifolds.Grassmann{n,k,𝔽},Val{:Gaussian},Float64}} where 𝔽 where k where n","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Grassmannian, :Gaussian [, σ=1.0])\n\nreturn a random point x on Grassmannian manifold M by generating a random (Gaussian) matrix with standard deviation σ in matching size, which is orthonormal.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{𝔽}, Tuple{k}, Tuple{n}, Tuple{Manifolds.Stiefel{n,k,𝔽},Val{:Gaussian}}, Tuple{Manifolds.Stiefel{n,k,𝔽},Val{:Gaussian},Float64}} where 𝔽 where k where n","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Stiefel, :Gaussian[, σ=1.0])\n\nreturn a random (Gaussian) point x on the Stiefel manifold M by generating a (Gaussian) matrix with standard deviation σ and return the orthogonalized version, i.e. return ​​the Q component of the QR decomposition of the random matrix of size nk.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M, p[, :Gaussian, σ = 1.0])\n\ngenerate a random tangent vector in the tangent space of the point p on the SymmetricPositiveDefinite manifold M by using a Gaussian distribution with standard deviation σ on an ONB of the tangent space.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Hyperpolic, p, :Gaussian [, σ=1.0])\n\ngenerate a random point on the Hyperbolic manifold by projecting a point from the embedding with respect to the Minkowsky metric.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Sphere, p[, :Gaussian, σ=1.0])\n\nreturn a random tangent vector in the tangent space of p on the Sphere M.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Grassmann, p[,type=:Gaussian, σ=1.0])\n\nreturn a (Gaussian) random vector from the tangent space T_xmathrmGr(nk) with mean zero and standard deviation σ by projecting a random Matrix onto the  x.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Circle, p [, :Gaussian, σ=1.0])\n\nreturn a random tangent vector from the tangent space of the point p on the Circle mathbb S^1 by using a normal distribution with mean 0 and standard deviation 1.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::SymmetricPositiveDefinite, p, :Rician [,σ = 0.01])\n\ngenerate a random tangent vector in the tangent space of p on the SymmetricPositiveDefinite manifold M by using a Rician distribution with standard deviation σ.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Rotations, p[, type=:Gaussian, σ=1.0])\n\nreturn a random tangent vector in the tangent space T_xmathrmSO(n) of the point x on the Rotations manifold M by generating a random skew-symmetric matrix. The function takes the real upper triangular matrix of a (Gaussian) random matrix A with dimension ntimes n and subtracts its transposed matrix. Finally, the matrix is ​​normalized.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{Manifolds.ProductManifold,Any,Vararg{Any,N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::ProductManifold, p)\n\ngenerate a random tangent vector in the tangent space of the point p on the ProductManifold M.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{ManifoldsBase.Manifold,Any,Vararg{Any,N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M, p, options...)\n\ngenerate a random tangent vector in the tangent space of p on M. By default this is a :Gaussian distribution.\n\n\n\n\n\n","category":"method"},{"location":"solvers/truncated_conjugate_gradient_descent.html#tCG-1","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint Truncated Conjugate-Gradient Method","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"The aim is to solve the trust-region subproblem","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"operatorname*argmin_eta    T_xmathcalM m_x(eta) = F(x) +\nlangle nabla F(x) eta rangle_x + frac12 langle\noperatornameHessF(eta)_ x eta rangle_x","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"textst  langle eta eta rangle_x leq Delta^2","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"on a manifold by using the Steihaug-Toint truncated conjugate-gradient method. All terms involving the trust-region radius use an inner product w.r.t. the preconditioner; this is because the iterates grow in length w.r.t. the preconditioner, guaranteeing that we do not re-enter the trust-region.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Initialization-1","page":"Steihaug-Toint TCG Method","title":"Initialization","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"Initialize eta_0 = eta if using randomized approach and eta the zero tangent vector otherwise, r_0 = nabla F(x), z_0 = operatornameP(r_0), delta_0 = z_0 and k=0","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Iteration-1","page":"Steihaug-Toint TCG Method","title":"Iteration","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"Repeat until a convergence criterion is reached","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"Set kappa = langle delta_k operatornameHessF (delta_k)_x rangle_x,  alpha =fraclangle r_k z_k rangle_xkappa and  langle eta_k eta_k rangle_x^*  = langle eta_k operatornameP(eta_k) rangle_x +  2alpha langle eta_k operatornameP(delta_k) rangle_x +  alpha^2  langle delta_k operatornameP(delta_k) rangle_x.\nIf kappa leqq 0 or langle eta_k eta_k rangle_x^*  geqq Delta^2  return eta_k+1 = eta_k + tau delta_k and stop.\nSet eta_k^* = eta_k + alpha delta_k, if  langle eta_k eta_k rangle_x + frac12 langle eta_k  operatornameHessF (eta_k)_x rangle_x leqq langle eta_k^*   eta_k^* rangle_x + frac12 langle eta_k^*   operatornameHessF (eta_k)_ x rangle_x  set eta_k+1 = eta_k else set eta_k+1 = eta_k^* .\nSet r_k+1 = r_k + alpha operatornameHessF (delta_k)_ x,   z_k+1 = operatornameP(r_k+1),  beta = fraclangle r_k+1  z_k+1 rangle_xlangle r_k z_k  rangle_x and delta_k+1 = -z_k+1 + beta delta_k.\nSet k=k+1.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Result-1","page":"Steihaug-Toint TCG Method","title":"Result","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"The result is given by the last computed η_k.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Remarks-1","page":"Steihaug-Toint TCG Method","title":"Remarks","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"The operatornameP(cdot) denotes the symmetric, positive deﬁnite preconditioner. It is required if a randomized approach is used i.e. using a random tangent vector eta as initial vector. The idea behind it is to avoid saddle points. Preconditioning is simply a rescaling of the variables and thus a redeﬁnition of the shape of the trust region. Ideally operatornameP(cdot) is a cheap, positive approximation of the inverse of the Hessian of F at x. On default, the preconditioner is just the identity.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"To step number 2: Obtain tau from the positive root of leftlVert eta_k + tau delta_k rightrVert_operatornameP x = Delta what becomes after the conversion of the equation to","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":" tau = frac-langle eta_k operatornameP(delta_k) rangle_x +\n sqrtlangle eta_k operatornameP(delta_k) rangle_x^2 +\n langle delta_k operatornameP(delta_k) rangle_x ( Delta^2 -\n langle eta_k operatornameP(eta_k) rangle_x)\n langle delta_k operatornameP(delta_k) rangle_x","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"It can occur that langle delta_k operatornameHessF (delta_k)_x rangle_x = kappa leqq 0 at iteration k. In this case, the model is not strictly convex, and the stepsize alpha =fraclangle r_k z_k rangle_x kappa computed in step 1. does not give a reduction in the modelfunction m_x(cdot). Indeed, m_x(cdot) is unbounded from below along the line eta_k + alpha delta_k. If our aim is to minimize the model within the trust-region, it makes far more sense to reduce m_x(cdot) along eta_k + alpha delta_k as much as we can while staying within the trust-region, and this means moving to the trust-region boundary along this line. Thus when kappa leqq 0 at iteration k, we replace alpha = fraclangle r_k z_k rangle_xkappa with tau described as above. The other possibility is that eta_k+1 would lie outside the trust-region at iteration k (i.e. langle eta_k eta_k rangle_x^*  geqq Delta^2 what can be identified with the norm of eta_k+1). In particular, when operatornameHessF (cdot)_x is positive deﬁnite and eta_k+1 lies outside the trust region, the solution to the trust-region problem must lie on the trust-region boundary. Thus, there is no reason to continue with the conjugate gradient iteration, as it stands, as subsequent iterates will move further outside the trust-region boundary. A sensible strategy, just as in the case considered above, is to move to the trust-region boundary by ﬁnding tau.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Interface-1","page":"Steihaug-Toint TCG Method","title":"Interface","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"  truncated_conjugate_gradient_descent\n  truncated_conjugate_gradient_descent!","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent","page":"Steihaug-Toint TCG Method","title":"Manopt.truncated_conjugate_gradient_descent","text":"truncated_conjugate_gradient_descent(M, F, ∇F, x, η, H, Δ)\n\nsolve the trust-region subproblem\n\noperatorname*argmin_eta    T_xM m_x(eta) = F(x) + langle nabla F(x) eta rangle_x + frac12 langle operatornameHessF(eta)_ x eta rangle_x\n\ntextst  langle eta eta rangle_x leqq Delta^2\n\nwith the Steihaug-Toint truncated conjugate-gradient method. For a description of the algorithm and theorems offering convergence guarantees, see the reference:\n\nP.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: 10.1007/s10208-005-0179-9\nA. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: 10.1137/1.9780898719857\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n∇F – the gradient nabla Fcolonmathcal Mto Tmathcal M of F\nx – a point on the manifold x  mathcal M\nη – an update tangential vector eta  mathcalT_xM\nH – the hessian H( mathcal M x xi) of F\nΔ – a trust-region radius\n\nOptional\n\npreconditioner – a preconditioner for the hessian H\nθ – 1+θ is the superlinear convergence target rate. The algorithm will   terminate early if the residual was reduced by a power of 1+theta.\nκ – the linear convergence target rate: algorithm will terminate   early if the residual was reduced by a factor of kappa.\nuseRandom – set to true if the trust-region solve is to be initiated with a   random tangent vector. If set to true, no preconditioner will be   used. This option is set to true in some scenarios to escape saddle   points, but is otherwise seldom activated.\nstopping_criterion – (StopWhenAny, StopAfterIteration,   StopIfResidualIsReducedByFactor, StopIfResidualIsReducedByPower,   StopWhenCurvatureIsNegative, StopWhenTrustRegionIsExceeded )   a functor inheriting from StoppingCriterion indicating when to stop,   where for the default, the maximal number of iterations is set to the dimension of the   manifold, the power factor is θ, the reduction factor is κ.   .\nreturn_options – (false) – if actiavated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt is returned\n\nand the ones that are passed to decorate_options for decorators.\n\nOutput\n\nη – an approximate solution of the trust-region subproblem in   mathcalT_xM.\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\nsee also\n\ntrust_regions\n\n\n\n\n\n","category":"function"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent!","page":"Steihaug-Toint TCG Method","title":"Manopt.truncated_conjugate_gradient_descent!","text":"truncated_conjugate_gradient_descent!(M, F, ∇F, x, η, H, Δ; kwargs...)\n\nsolve the trust-region subproblem in place of x.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n∇F – the gradient nabla Fcolonmathcal Mto Tmathcal M of F\nx – a point on the manifold x  mathcal M\nη – an update tangential vector eta  mathcalT_xM\nH – the hessian H( mathcal M x xi) of F\nΔ – a trust-region radius\n\nFor more details and all optional arguments, see truncated_conjugate_gradient_descent.\n\n\n\n\n\n","category":"function"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Options-1","page":"Steihaug-Toint TCG Method","title":"Options","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"TruncatedConjugateGradientOptions","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.TruncatedConjugateGradientOptions","page":"Steihaug-Toint TCG Method","title":"Manopt.TruncatedConjugateGradientOptions","text":"TruncatedConjugateGradientOptions <: HessianOptions\n\ndescribe the Steihaug-Toint truncated conjugate-gradient method, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx : a point, where the trust-region subproblem needs   to be solved\nstop : a function s,r = @(o,iter,ξ,x,xnew) returning a stop   indicator and a reason based on an iteration number, the gradient and the   last and current iterates\nη : a tangent vector (called update vector), which solves the   trust-region subproblem after successful calculation by the algorithm\nδ : search direction\nΔ : the trust-region radius\nresidual : the gradient\nuseRand : indicates if the trust-region solve and so the algorithm is to be       initiated with a random tangent vector. If set to true, no       preconditioner will be used. This option is set to true in some       scenarios to escape saddle points, but is otherwise seldom activated.\n\nConstructor\n\nTruncatedConjugateGradientOptions(x, stop, eta, delta, Delta, res, uR)\n\nconstruct a truncated conjugate-gradient Option with the fields as above.\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Additional-Stopping-Criteria-1","page":"Steihaug-Toint TCG Method","title":"Additional Stopping Criteria","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"StopIfResidualIsReducedByPower\nStopIfResidualIsReducedByFactor\nStopWhenTrustRegionIsExceeded\nStopWhenCurvatureIsNegative","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.StopIfResidualIsReducedByPower","page":"Steihaug-Toint TCG Method","title":"Manopt.StopIfResidualIsReducedByPower","text":"StopIfResidualIsReducedByPower <: StoppingCriterion\n\nA functor for testing if the norm of residual at the current iterate is reduced by a power of 1+θ compared to the norm of the initial residual, i.e. Vert r_k Vert_x leqq  Vert r_0 Vert_x^1+theta. In this case the algorithm reached superlinear convergence.\n\nFields\n\nθ – part of the reduction power\ninitialResidualNorm - stores the norm of the residual at the initial vector   eta of the Steihaug-Toint tcg mehtod truncated_conjugate_gradient_descent\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\n\nConstructor\n\nStopIfResidualIsReducedByPower(iRN, θ)\n\ninitialize the StopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN to the power of 1+θ.\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.StopIfResidualIsReducedByFactor","page":"Steihaug-Toint TCG Method","title":"Manopt.StopIfResidualIsReducedByFactor","text":"StopIfResidualIsReducedByFactor <: StoppingCriterion\n\nA functor for testing if the norm of residual at the current iterate is reduced by a factor compared to the norm of the initial residual, i.e. Vert r_k Vert_x leqq kappa Vert r_0 Vert_x. In this case the algorithm reached linear convergence.\n\nFields\n\nκ – the reduction factor\ninitialResidualNorm - stores the norm of the residual at the initial vector   eta of the Steihaug-Toint tcg mehtod truncated_conjugate_gradient_descent\nreason – stores a reason of stopping if the stopping criterion has one be reached, see get_reason.\n\nConstructor\n\nStopIfResidualIsReducedByFactor(iRN, κ)\n\ninitialize the StopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN times κ.\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.StopWhenTrustRegionIsExceeded","page":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenTrustRegionIsExceeded","text":"StopWhenTrustRegionIsExceeded <: StoppingCriterion\n\nA functor for testing if the norm of the next iterate in the  Steihaug-Toint tcg mehtod is larger than the trust-region radius, i.e. Vert η_k^* Vert_x  Δ. terminate the algorithm when the trust region has been left.\n\nFields\n\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\nstorage – stores the necessary parameters η, δ, residual to check the   criterion.\n\nConstructor\n\nStopWhenTrustRegionIsExceeded([a])\n\ninitialize the StopWhenTrustRegionIsExceeded functor to indicate to stop after the norm of the next iterate is greater than the trust-region radius using the StoreOptionsAction a, which is initialized to store :η, :δ, :residual by default.\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.StopWhenCurvatureIsNegative","page":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenCurvatureIsNegative","text":"StopWhenCurvatureIsNegative <: StoppingCriterion\n\nA functor for testing if the curvature of the model is negative, i.e. langle delta_k operatornameHessF(delta_k)rangle_x leqq 0. In this case, the model is not strictly convex, and the stepsize as computed does not give a reduction of the model.\n\nFields\n\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\nstorage – stores the necessary parameter δ to check the   criterion.\n\nConstructor\n\nStopWhenCurvatureIsNegative([a])\n\ninitialize the StopWhenCurvatureIsNegative functor to indicate to stop after the inner product of the search direction and the hessian applied on the search dircetion is less than zero using the StoreOptionsAction a, which is initialized to just store :δ by default.\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/particle_swarm.html#ParticleSwarmSolver-1","page":"Particle Swarm Optimization","title":"Particle Swarm Optimization","text":"","category":"section"},{"location":"solvers/particle_swarm.html#","page":"Particle Swarm Optimization","title":"Particle Swarm Optimization","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/particle_swarm.html#","page":"Particle Swarm Optimization","title":"Particle Swarm Optimization","text":"  particle_swarm\n  particle_swarm!","category":"page"},{"location":"solvers/particle_swarm.html#Manopt.particle_swarm","page":"Particle Swarm Optimization","title":"Manopt.particle_swarm","text":"patricle_swarm(M, F)\n\nperform the particle swarm optimization algorithm (PSO), starting with the initial particle positions x_0[Borckmans2010]. The aim of PSO is to find the particle position g on the Manifold M that solves\n\nmin_x in mathcalM F(x)\n\nTo this end, a swarm of particles is moved around the Manifold M in the following manner. For every particle k we compute the new particle velocities v_k^(i) in every step i of the algorithm by\n\nv_k^(i) = omega  operatornameT_x_k^(i)gets x_k^(i-1)v_k^(i-1) + c   r_1  operatornameretr_x_k^(i)^-1(p_k^(i)) + s   r_2 operatornameretr_x_k^(i)^-1(g)\n\nwhere x_k^(i) is the current particle position, omega denotes the inertia, c and s are a cognitive and a social weight, respectively, r_j, j=12 are random factors which are computed new for each particle and step, operatornameretr^-1 denotes an inverse retraction on the Manifold M, and operatornameT is a vector transport.\n\nThen the position of the particle is updated as\n\nx_k^(i+1) = operatornameretr_x_k^(i)(v_k^(i))\n\nwhere operatornameretr denotes a retraction on the Manifold M. At the end of each step for every particle, we set\n\np_k^(i+1) = begincases\nx_k^(i+1)   textif  F(x_k^(i+1))F(p_k^(i))\np_k^(i)  textelse\nendcases\n\nand\n\ng_k^(i+1) =begincases\np_k^(i+1)   textif  F(p_k^(i+1))F(g_k^(i))\ng_k^(i)  textelse\nendcases\n\ni.e. p_k^(i) is the best known position for the particle k and g^(i) is the global best known position ever visited up to step i.\n\n[Borckmans2010]: P. B. Borckmans, M. Ishteva, P.-A. Absil, A Modified Particle Swarm Optimization Algorithm for the Best Low Multilinear Rank Approximation of Higher-Order Tensors, In: Dorigo M. et al. (eds) Swarm Intelligence. ANTS 2010. Lecture Notes in Computer Science, vol 6234. Springer, Berlin, Heidelberg, doi 10.1007/978-3-642-15461-4_2\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n\nOptional\n\nn - (100) number of random initial positions of x0\nx0 – the initial positions of each particle in the swarm x_k^(0)  mathcal M for k = 1 dots n, per default these are n random_points\nvelocity – a set of tangent vectors (of type AbstractVector{T}) representing the velocities of the particles, per default a random_tangent per inital position\ninertia – (0.65) the inertia of the patricles\nsocial_weight – (1.4) a social weight factor\ncognitive_weight – (1.4) a cognitive weight factor\nretraction_method – (ExponentialRetraction()) a retraction(M,x,ξ) to use.\ninverse_retraction_method - (LogarithmicInverseRetraction()) an inverse_retraction(M,x,y) to use.\nvector_transport_mthod - (ParallelTransport()) a vector transport method to use.\nstopping_criterion – (StopWhenAny(StopAfterIteration(500), StopWhenChangeLess(10^{-4}))) a functor inheriting from StoppingCriterion indicating when to stop.\nreturn_options – (false) – if activated, the extended result, i.e. the   complete Options are returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\ng – the resulting point of PSO\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/particle_swarm.html#Manopt.particle_swarm!","page":"Particle Swarm Optimization","title":"Manopt.particle_swarm!","text":"patricle_swarm!(M, F; n=100, x0::AbstractVector=[random_point(M) for i in 1:n], kwargs...)\n\nperform the particle swarm optimization algorithm (PSO), starting with the initial particle positions x_0[Borckmans2010] in place of x0.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n\nOptional\n\nn - (100) number of random initial positions of x0\nx0 – the initial positions of each particle in the swarm x_k^(0)  mathcal M for k = 1 dots n, per default these are n random_points\n\nfor more optional arguments, see particle_swarm.\n\n\n\n\n\n","category":"function"},{"location":"solvers/particle_swarm.html#Options-1","page":"Particle Swarm Optimization","title":"Options","text":"","category":"section"},{"location":"solvers/particle_swarm.html#","page":"Particle Swarm Optimization","title":"Particle Swarm Optimization","text":"ParticleSwarmOptions","category":"page"},{"location":"solvers/particle_swarm.html#Manopt.ParticleSwarmOptions","page":"Particle Swarm Optimization","title":"Manopt.ParticleSwarmOptions","text":"ParticleSwarmOptions{P,T} <: Options\n\nDescribes a particle swarm optimizing algorithm, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx0 – a set of points (of type AbstractVector{P}) on a manifold as initial particle positions\nvelocity – a set of tangent vectors (of type AbstractVector{T}) representing the velocities of the particles\ninertia – (0.65) the inertia of the patricles\nsocial_weight – (1.4) a social weight factor\ncognitive_weight – (1.4) a cognitive weight factor\nstopping_criterion – (StopWhenAny(StopAfterIteration(500), StopWhenChangeLess(10^{-4}))) a functor inheriting from StoppingCriterion indicating when to stop.\nretraction_method – (ExponentialRetraction) the rectraction to use\ninverse_retraction_method - (LogarithmicInverseRetraction) an inverse retraction to use.\n\nConstructor\n\nParticleSwarmOptions(x0, velocity, inertia, social_weight, cognitive_weight, stopping_criterion[, retraction_method=ExponentialRetraction(), inverse_retraction_method=LogarithmicInverseRetraction()])\n\nconstruct a particle swarm Option with the fields and defaults as above.\n\nSee also\n\nparticle_swarm\n\n\n\n\n\n","category":"type"},{"location":"solvers/particle_swarm.html#Literature-1","page":"Particle Swarm Optimization","title":"Literature","text":"","category":"section"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/JacobiFields.jl\"","category":"page"},{"location":"tutorials/JacobiFields.html#Illustration-of-Jacobi-Fields-1","page":"use Jacobi Fields","title":"Illustration of Jacobi Fields","text":"","category":"section"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"This tutorial illustrates the usage of Jacobi Fields within Manopt.jl. For this tutorial you should be familiar with the basic terminology on a manifold like the exponential and logarithmic map as well as shortest geodesics.","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"We first initialize the manifold","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"using Manopt, Manifolds","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"using Colors\nblack = RGBA{Float64}(colorant\"#000000\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\")\nTolVibrantCyan = RGBA{Float64}(colorant\"#33BBEE\")\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\")\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Assume we have two points on the equator of the Sphere mathcal M = mathbb S^2","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"M = Sphere(2)\np, q = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"their connecting shortest geodesic (sampled at 100 points)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"geodesicCurve = shortest_geodesic(M, p, q, [0:0.1:1.0...]);\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"looks as follows using the asymptote_export_S2_signals export","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"asymptote_export_S2_signals(\"jacobiGeodesic.asy\";\n    render = asyResolution,\n    curves=[geodesicCurve], points = [ [x,y] ],\n    colors=Dict(:curves => [black], :points => [TolVibrantOrange]),\n    dot_size = 3.5, line_width = 0.75, camera_position = (1.,1.,.5)\n)\nrender_asymptote(\"jacobiGeodesic.asy\"; render = 2)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"(Image: A geodesic connecting two points on the equator)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"where x is on the left. Then this tutorial solves the following task:","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Given a direction X_p T_xmathcal M, for example","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"X = [0.0, 0.4, 0.5]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"we move the start point x into, how does any point on the geodesic move?","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Or mathematically: Compute D_p g(t pq) for some fixed t01 and a given direction X_p. Of course two cases are quite easy: For t=0 we are in x and how x “moves” is already known, so D_x g(0pq) = X. On the other side, for t=1, g(1 pq) = q which is fixed, so D_p g(1 pq) is the zero tangent vector (in T_qmathcal M).","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"For all other cases we employ a jacobi_field, which is a (tangent) vector field along the shortest geodesic given as follows: The geodesic variation Gamma_gX(st) is defined for some varepsilon  0 as","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Gamma_gX(st)=expgamma_pX(s)tlog_g(spX)pqquad s(-varepsilonvarepsilon) t01","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Intuitively we make a small step s into direction xi using the geodesic g(cdot pX) and from r=g(s pX) we follow (in t) the geodesic g(cdot rq). The corresponding Jacobi field~(J_{g,X}) along~(g(\\cdot; p,q)) is given","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"J_gX(t)=fracDpartial sGamma_gX(st)Biglrvert_s=0","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"which is an ODE and we know the boundary conditions J_gX(0)=X and J_gX(t) = 0. In symmetric spaces we can compute the solution, since the system of ODEs decouples, see for example do Carmo, Chapter 4.2. Within Manopt.jl this is implemented as jacobi_field(M,p,q,t,X[,β]), where the optional parameter (function) β specifies, which Jacobi field we want to evaluate and the one used here is the default.","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"We can hence evaluate that on the points on the geodesic at","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"T = [0:0.1:1.0...]\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"namely","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"r = shortest_geodesic(M, p, q, T)\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"the geodesic moves as","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"W = jacobi_field.(Ref(M), Ref(p), Ref(q), T, Ref(X))","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"which can also be called using differential_geodesic_startpoint. We can add to the image above by creating extended tangent vectors the include their base points","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"V = [Tuple([a, b]) for (a, b) in zip(r, W)]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"and add that as one further set to the Asymptote export.","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"asymptote_export_S2_signals(\"jacobiGeodesicdifferential_geodesic_startpoint.asy\";\n    render = asyResolution,\n    curves=[geodesicCurve], points = [ [x,y], Z], tangent_vectors = [Vx],\n    colors=Dict(\n        :curves => [black],\n        :points => [TolVibrantOrange,TolVibrantCyan],\n        :tvectors => [TolVibrantCyan]\n    ),\n    dot_sizes = [3.5,2.], line_width = 0.75, camera_position = (1.,1.,.5)\n)\nrender_asymptote(\"jacobiGeodesicdifferential_geodesic_startpoint.asy\"; render = 2)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"(Image: A Jacobi field for $D_xg(t,x,y)[\\eta]$)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"If we further move the end point, too, we can derive that Differential in direction","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Xq = [0.2, 0.0, -0.5]\nW2 = differential_geodesic_endpoint.(Ref(M), Ref(p), Ref(q), T, Ref(Xq))\nV2 = [Tuple([a, b]) for (a, b) in zip(r, W2)]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"and we can combine both keeping the base point","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"V3 = [Tuple([a, b]) for (a, b) in zip(r, W2 + W)]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"asymptote_export_S2_signals(\"jacobiGeodesicResult.asy\";\n   render = asyResolution,\n   curves=[geodesicCurve], points = [ [x,y], Z], tangent_vectors = [Vx,Vy,Vb],\n   colors=Dict(\n       :curves => [black],\n       :points => [TolVibrantOrange,TolVibrantCyan],\n       :tvectors => [TolVibrantCyan,TolVibrantCyan,TolVibrantTeal]\n  ),\n  dot_sizes = [3.5,2.], line_width = 0.75, camera_position = (1.,1.,0.)\n)\nrender_asymptote(\"jacobiGeodesicResult.asy\"; render = 2)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"(Image: A Jacobi field for the effect of two differentials (blue) in sum (teal))","category":"page"},{"location":"tutorials/JacobiFields.html#Literature-1","page":"use Jacobi Fields","title":"Literature","text":"","category":"section"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"<ul><li id=\"doCarmo1992\">[<a>doCarmo1992</a>] do Carmo, M. P.:\n   <emph>Riemannian Geometry</emph>, Mathematics: Theory & Applications,\n   Birkhäuser Basel, 1992, ISBN: 0-8176-3490-8</li>\n<li id=\"BergmannGousenbourger2018\">[<a>BergmannGousenbourger2018</a>]\n  Bergmann, R.; Gousenbourger, P.-Y.: <emph>A variational model for data\n  fitting on manifolds by minimizing the acceleration of a Bézier curve</emph>,\n  Frontiers in Applied Mathematics and Statistics, 2018.\n  doi: <a href=\"https://dx.doi.org/10.3389/fams.2018.00059\">10.3389/fams.2018.00059</a></li>\n</ul>","category":"page"},{"location":"functions/index.html#Functions-1","page":"Introduction","title":"Functions","text":"","category":"section"},{"location":"functions/index.html#","page":"Introduction","title":"Introduction","text":"There are several functions required within optimization, most prominently costFunctions and gradients. This package includes several cost functions and corresponding gradients, but also corresponding proximal maps for variational methods manifold-valued data. Most of these functions require the evaluation of Differentials or their AdjointDifferentials as well as JacobiFields (e.g. easily to evaluate for symmetric manifolds).","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#StochasticGradientDescentSolver-1","page":"Stochastic Gradient Descent","title":"Gradient Descent","text":"","category":"section"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"stochastic_gradient_descent\nstochastic_gradient_descent!","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.stochastic_gradient_descent","page":"Stochastic Gradient Descent","title":"Manopt.stochastic_gradient_descent","text":"stochastic_gradient_descent(M, ∇F, x)\n\nperform a stochastic gradient descent\n\nInput\n\nM a manifold mathcal M\n∇F – a gradient function, that either returns a vector of the subgradients or is a vector of gradients\nx – an initial value x  mathcal M\n\nOptional\n\ncost – (missing) you can provide a cost function for example to track the function value\nevaluation_order – (:Random) – whether to use a randomly permuted sequence (:FixedRandom), a per cycle permuted sequence (:Linear) or the default :Random one.\nstopping_criterion (StopAfterIteration(1000))– a StoppingCriterion\nstepsize (ConstantStepsize(1.0)) a Stepsize\norder_type (:RandomOder) a type of ordering of gradient evaluations. values are :RandomOrder, a :FixedPermutation, :LinearOrder\norder - ([1:n]) the initial permutation, where n is the number of gradients in ∇F.\nretraction_method – (ExponentialRetraction()) a retraction(M,x,ξ) to use.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.stochastic_gradient_descent!","page":"Stochastic Gradient Descent","title":"Manopt.stochastic_gradient_descent!","text":"stochastic_gradient_descent!(M, ∇F, x)\n\nperform a stochastic gradient descent inplace of x.\n\nInput\n\nM a manifold mathcal M\n∇F – a gradient function, that either returns a vector of the subgradients or is a vector of gradients\nx – an initial value x  mathcal M\n\nfor all optional parameters, see stochastic_gradient_descent.\n\n\n\n\n\n","category":"function"},{"location":"solvers/stochastic_gradient_descent.html#Options-1","page":"Stochastic Gradient Descent","title":"Options","text":"","category":"section"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"StochasticGradientDescentOptions","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.StochasticGradientDescentOptions","page":"Stochastic Gradient Descent","title":"Manopt.StochasticGradientDescentOptions","text":"StochasticGradientDescentOptions <: AbstractStochasticGradientDescentOptions\n\nStore the following fields for a default stochastic gradient descent algorithm, see also StochasticGradientProblem and stochastic_gradient_descent.\n\nfields\n\nFields\n\nx the current iterate\nstopping_criterion (StopAfterIteration(1000))– a StoppingCriterion\nstepsize (ConstantStepsize(1.0)) a Stepsize\nevaluation_order – (:Random) – whether to use a randomly permuted sequence (:FixedRandom), a per cycle permuted sequence (:Linear) or the default :Random one.\norder the current permutation\nretraction_method – (ExponentialRetraction()) a retraction(M,x,ξ) to use.\n\nConstructor\n\nStochasticGradientDescentOptions(x)\n\nCreate a StochasticGradientDescentOptions with start point x. all other fields are optional keyword arguments.\n\n\n\n\n\n","category":"type"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"Additionally, the options share a DirectionUpdateRule, so you can also apply MomentumGradient and AverageGradient here. The most inner one should always be.","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"AbstractStochasticGradientProcessor\nStochasticGradient","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.AbstractStochasticGradientProcessor","page":"Stochastic Gradient Descent","title":"Manopt.AbstractStochasticGradientProcessor","text":"AbstractStochasticGradientDescentOptions <: Options\n\nA generic type for all options related to stochastic gradient descent methods\n\n\n\n\n\n","category":"type"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.StochasticGradient","page":"Stochastic Gradient Descent","title":"Manopt.StochasticGradient","text":"StochasticGradient <: DirectionUpdateRule\n\nThe default gradient processor, which just evaluates the (stochastic) gradient or a subset thereof.\n\n\n\n\n\n","category":"type"},{"location":"functions/differentials.html#DifferentialFunctions-1","page":"Differentials","title":"Differentials","text":"","category":"section"},{"location":"functions/differentials.html#","page":"Differentials","title":"Differentials","text":"Modules = [Manopt]\nPages   = [\"functions/differentials.jl\"]","category":"page"},{"location":"functions/differentials.html#Manopt.differential_bezier_control-Tuple{ManifoldsBase.Manifold,AbstractArray{#s31,1} where #s31<:BezierSegment,Array{Float64,1},AbstractArray{#s30,1} where #s30<:BezierSegment}","page":"Differentials","title":"Manopt.differential_bezier_control","text":"differential_bezier_control(\n    M::Manifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector{Float}\n    X::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors X in the tangent spaces of the control points. The result is the “change” of the curve at pts, which are elementwise in 0N, and each depending the corresponding segment(s). Here, N is the length of B.\n\nSee de_casteljau for more details on the curve and [BergmannGousenbourger2018].\n\n[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics, 2018. doi: 10.3389/fams.2018.00059, arXiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_bezier_control-Tuple{ManifoldsBase.Manifold,AbstractArray{#s31,1} where #s31<:BezierSegment,Float64,AbstractArray{#s30,1} where #s30<:BezierSegment}","page":"Differentials","title":"Manopt.differential_bezier_control","text":"differential_bezier_control(\n    M::Manifold,\n    B::AbstractVector{<:BezierSegment},\n    t::Float64,\n    X::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors Ξ in the tangent spaces of the control points. The result is the “change” of the curve at t in0N, which depends only on the corresponding segment. Here, N is the length of B.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_bezier_control-Tuple{ManifoldsBase.Manifold,BezierSegment,Array{Float64,1},BezierSegment}","page":"Differentials","title":"Manopt.differential_bezier_control","text":"differential_bezier_control(\n    M::Manifold,\n    b::NTuple{N,P},\n    T::Array{Float64,1},\n    X::BezierSegment,\n)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X in the tangent spaces of the control points. The result is the “change” of the curve at the points T, elementwise in in01.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_bezier_control-Union{Tuple{Q}, Tuple{ManifoldsBase.Manifold,BezierSegment,Float64,BezierSegment}} where Q","page":"Differentials","title":"Manopt.differential_bezier_control","text":"differential_bezier(M::Manifold, b::BezierSegment, t::Float, X::BezierSegment)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X given in the tangent spaces of the control points. The result is the “change” of the curve at tin01.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_exp_argument-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Differentials","title":"Manopt.differential_exp_argument","text":"differential_exp_argument(M, p, X, Y)\n\ncomputes D_Xexp_pXY. Note that X   T_X(T_pmathcal M) = T_pmathcal M is still a tangent vector.\n\nSee also\n\ndifferential_exp_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_exp_basepoint-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Differentials","title":"Manopt.differential_exp_basepoint","text":"differential_exp_basepoint(M, p, X, Y)\n\nCompute D_pexp_p XY.\n\nSee also\n\ndifferential_exp_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_forward_logs-Tuple{ManifoldsBase.PowerManifold,Any,Any}","page":"Differentials","title":"Manopt.differential_forward_logs","text":"Y = differential_forward_logs(M, p, X)\n\ncompute the differenital of forward_logs F on the PowerManifold manifold M at p and direction X , in the power manifold array, the differential of the function\n\nF_i(x) = sum_j  mathcal I_i log_p_i p_j quad i    mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM     – a PowerManifold manifold\np     – a point.\nX     – a tangent vector.\n\nOuput\n\nY – resulting tangent vector in T_xmathcal N representing the differentials of the logs, where mathcal N is thw power manifold with the number of dimensions added to size(x).\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_geodesic_endpoint-Union{Tuple{mT}, Tuple{mT,Any,Any,Any,Any}} where mT<:ManifoldsBase.Manifold","page":"Differentials","title":"Manopt.differential_geodesic_endpoint","text":"differential_geodesic_endpoint(M,x,y,t,η)\n\ncomputes D_qg(tpq)eta.\n\nSee also\n\ndifferential_geodesic_startpoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_geodesic_startpoint-Union{Tuple{mT}, Tuple{mT,Any,Any,Any,Any}} where mT<:ManifoldsBase.Manifold","page":"Differentials","title":"Manopt.differential_geodesic_startpoint","text":"differential_geodesic_startpoint(M, p, q, t, X)\n\ncomputes D_p g(tpq)eta.\n\nSee also\n\ndifferential_geodesic_endpoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_log_argument-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Differentials","title":"Manopt.differential_log_argument","text":"differential_log_argument(M,p,q,X)\n\ncomputes D_qlog_pqX.\n\nSee also\n\ndifferential_log_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_log_basepoint-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Differentials","title":"Manopt.differential_log_basepoint","text":"differential_log_basepoint(M, p, q, X)\n\ncomputes D_plog_pqX.\n\nSee also\n\ndifferential_log_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"helpers/errorMeasures.html#ErrorMeasures-1","page":"Error Measures","title":"Error Measures","text":"","category":"section"},{"location":"helpers/errorMeasures.html#","page":"Error Measures","title":"Error Measures","text":"meanSquaredError\nmeanAverageError","category":"page"},{"location":"helpers/errorMeasures.html#Manopt.meanSquaredError","page":"Error Measures","title":"Manopt.meanSquaredError","text":"meanSquaredError(M, p, q)\n\nCompute the (mean) squared error between the two points p and q on the (power) manifold M.\n\n\n\n\n\n","category":"function"},{"location":"helpers/errorMeasures.html#Manopt.meanAverageError","page":"Error Measures","title":"Manopt.meanAverageError","text":"meanSquaredError(M,x,y)\n\nComputes the (mean) squared error between the two points x and y on the PowerManifold manifold M.\n\n\n\n\n\n","category":"function"},{"location":"functions/proximal_maps.html#proximalMapFunctions-1","page":"Proximal Maps","title":"Proximal Maps","text":"","category":"section"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"For a function varphicolonmathcal M tomathbb R the proximal map is defined as","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"displaystyleoperatornameprox_lambdavarphi(x)\n= operatorname*argmin_y  mathcal M d_mathcal M^2(xy) + varphi(y)\nquad lambda  0","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"where d_mathcal Mcolon mathcal M times mathcal M to mathbb R denotes the geodesic distance on (\\mathcal M). While it might still be difficult to compute the minimizer, there are several proximal maps known (locally) in closed form. Furthermore if x^star  mathcal M is a minimizer of varphi, then","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"displaystyleoperatornameprox_lambdavarphi(x^star) = x^star","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"i.e. a minimizer is a fixed point of the proximal map.","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"This page lists all proximal maps available within Manopt. To add you own, just extend the functions/proximal_maps.jl file.","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"Modules = [Manopt]\nPages   = [\"proximal_maps.jl\"]","category":"page"},{"location":"functions/proximal_maps.html#Manopt.project_collaborative_TV","page":"Proximal Maps","title":"Manopt.project_collaborative_TV","text":"project_collaborative_TV(M,λ,x [,p=2,q=1])\n\ncompute the projection onto collaborative Norm unit (or α-) ball, i.e. of the function\n\nF^q(x) = sum_iinmathcal G\n  Bigl( sum_jinmathcal I_i\n    sum_k=1^d lVert X_ijrVert_x^pBigr)^fracqp\n\nwhere mathcal G is the set of indices for xinmathcal M and mathcal I_i is the set of its forward neighbors. This is adopted from the paper by Duran, Möller, Sbert, Cremers: Collaborative Total Variation: A General Framework for Vectorial TV Models (arxiv: 1508.01308), where the most inner norm is not on a manifold but on a vector space, see their Example 3 for details.\n\n\n\n\n\n","category":"function"},{"location":"functions/proximal_maps.html#Manopt.prox_TV-Union{Tuple{T}, Tuple{N}, Tuple{𝔽}, Tuple{ManifoldsBase.PowerManifold{𝔽,N,T,TPR} where TPR<:ManifoldsBase.AbstractPowerRepresentation,Any,Any}, Tuple{ManifoldsBase.PowerManifold{𝔽,N,T,TPR} where TPR<:ManifoldsBase.AbstractPowerRepresentation,Any,Any,Int64}} where T where N<:ManifoldsBase.Manifold where 𝔽","page":"Proximal Maps","title":"Manopt.prox_TV","text":"ξ = prox_TV(M,λ,x [,p=1])\n\ncompute the proximal maps operatornameprox_lambdavarphi of all forward differences orrucirng in the power manifold array, i.e. varphi(xixj) = d_mathcal M^p(xixj) with xi and xj are array elemets of x and j = i+e_k, where e_k is the kth unitvector. The parameter λ is the prox parameter.\n\nInput\n\nM – a Manifold\nλ – a real value, parameter of the proximal map\nx – a point.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\ny – resulting  point containinf with all mentioned proximal points evaluated (in a cylic order).\n\n\n\n\n\n","category":"method"},{"location":"functions/proximal_maps.html#Manopt.prox_TV-Union{Tuple{T}, Tuple{mT}, Tuple{mT,Number,Tuple{T,T}}, Tuple{mT,Number,Tuple{T,T},Int64}} where T where mT<:ManifoldsBase.Manifold","page":"Proximal Maps","title":"Manopt.prox_TV","text":"(y1,y2) = prox_TV(M,λ,(x1,x2) [,p=1])\n\nCompute the proximal map operatornameprox_lambdavarphi of varphi(xy) = d_mathcal M^p(xy) with parameter λ.\n\nInput\n\nM – a Manifold\nλ – a real value, parameter of the proximal map\n(x1,x2) – a tuple of two points,\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\n(y1,y2) – resulting tuple of points of the operatornameprox_lambdavarphi( (x1,x2) )\n\n\n\n\n\n","category":"method"},{"location":"functions/proximal_maps.html#Manopt.prox_TV2-Union{Tuple{T}, Tuple{ManifoldsBase.Manifold,Any,Tuple{T,T,T}}, Tuple{ManifoldsBase.Manifold,Any,Tuple{T,T,T},Int64}} where T","page":"Proximal Maps","title":"Manopt.prox_TV2","text":"(y1,y2,y3) = prox_TV2(M,λ,(x1,x2,x3),[p=1], kwargs...)\n\nCompute the proximal map operatornameprox_lambdavarphi of varphi(x_1x_2x_3) = d_mathcal M^p(c(x_1x_3)x_2) with parameter λ>0, where c(xz) denotes the mid point of a shortest geodesic from x1 to x3 that is closest to x2.\n\nInput\n\nM          – a manifold\nλ          – a real value, parameter of the proximal map\n(x1,x2,x3) – a tuple of three points\np – (1) exponent of the distance of the TV term\n\nOptional\n\nkwargs... – parameters for the internal subgradient_method     (if M is neither Euclidean nor Circle, since for these a closed form     is given)\n\nOutput\n\n(y1,y2,y3) – resulting tuple of points of the proximal map\n\n\n\n\n\n","category":"method"},{"location":"functions/proximal_maps.html#Manopt.prox_TV2-Union{Tuple{T}, Tuple{N}, Tuple{ManifoldsBase.PowerManifold{N,T,TSize,TPR} where TPR<:ManifoldsBase.AbstractPowerRepresentation where TSize,Any,Any}, Tuple{ManifoldsBase.PowerManifold{N,T,TSize,TPR} where TPR<:ManifoldsBase.AbstractPowerRepresentation where TSize,Any,Any,Int64}} where T where N","page":"Proximal Maps","title":"Manopt.prox_TV2","text":"ξ = prox_TV2(M,λ,x,[p])\n\ncompute the proximal maps operatornameprox_lambdavarphi of all centered second order differences orrucirng in the power manifold array, i.e. varphi(x_kx_ix_j) = d_2(x_kx_ix_j), where kj are backward and forward neighbors (along any dimension in the array of x). The parameter λ is the prox parameter.\n\nInput\n\nM – a Manifold\nλ – a real value, parameter of the proximal map\nx – a points.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\ny – resulting point with all mentioned proximal points evaluated (in a cylic order).\n\n\n\n\n\n","category":"method"},{"location":"functions/proximal_maps.html#Manopt.prox_distance","page":"Proximal Maps","title":"Manopt.prox_distance","text":"y = prox_distance(M,λ,f,x [,p=2])\n\ncompute the proximal map operatornameprox_lambdavarphi with parameter λ of varphi(x) = frac1pd_mathcal M^p(fx).\n\nInput\n\nM – a Manifold mathcal M\nλ – the prox parameter\nf – a point f  mathcal M (the data)\nx – the argument of the proximal map\n\nOptional argument\n\np – (2) exponent of the distance.\n\nOuput\n\ny – the result of the proximal map of varphi\n\n\n\n\n\n","category":"function"},{"location":"functions/proximal_maps.html#Manopt.prox_parallel_TV-Union{Tuple{T}, Tuple{ManifoldsBase.PowerManifold,Any,Array{T,1}}, Tuple{ManifoldsBase.PowerManifold,Any,Array{T,1},Int64}} where T","page":"Proximal Maps","title":"Manopt.prox_parallel_TV","text":"ξ = prox_parallel_TV(M,λ,x [,p=1])\n\ncompute the proximal maps operatornameprox_lambdavarphi of all forward differences orrucirng in the power manifold array, i.e. varphi(xixj) = d_mathcal M^p(xixj) with xi and xj are array elemets of x and j = i+e_k, where e_k is the kth unitvector. The parameter λ is the prox parameter.\n\nInput\n\nM     – a PowerManifold manifold\nλ     – a real value, parameter of the proximal map\nx     – a point\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\ny  – resulting Array of points with all mentioned proximal points evaluated (in a parallel within the arrays elements).\n\nSee also prox_TV\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#planSection-1","page":"Plans","title":"Plans for solvers","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"CurrentModule = Manopt","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"In order to start a solver, both a Problem and Options are required. Together they form a plan and these are stored in this folder. For sub-problems there are maybe also only Options, since they than refer to the same problem.","category":"page"},{"location":"plans/index.html#Options-1","page":"Plans","title":"Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"For most algorithms a certain set of options can either be generated beforehand of the function with keywords can be used. Generally the type","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Options\nget_options","category":"page"},{"location":"plans/index.html#Manopt.Options","page":"Plans","title":"Manopt.Options","text":"Options\n\nA general super type for all options.\n\nFields\n\nThe following fields are assumed to be default. If you use different ones, provide the access functions accordingly\n\nx a point with the current iterate\nstop a StoppingCriterion.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_options","page":"Plans","title":"Manopt.get_options","text":"get_options(o::Options)\n\nreturn the undecorated Options of the (possibly) decorated o. As long as your decorated options store the options within o.options and the dispatch_options_decorator is set to Val{true}, the internal options are extracted.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Since the Options directly relate to a solver, they are documented with the corresponding Solvers. You can always access the options (since they might be decorated) by calling get_options.","category":"page"},{"location":"plans/index.html#Decorators-for-Options-1","page":"Plans","title":"Decorators for Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Options can be decorated using the following trait and function to initialize","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"dispatch_options_decorator\nis_options_decorator\ndecorate_options","category":"page"},{"location":"plans/index.html#Manopt.dispatch_options_decorator","page":"Plans","title":"Manopt.dispatch_options_decorator","text":"dispatch_options_decorator(o::Options)\n\nIndicate internally, whether an Options o to be of decorating type, i.e. it stores (encapsulates) options in itself, by default in the field o. options.\n\nDecorators indicate this by returning Val{true} for further dispatch.\n\nThe default is Val{false}, i.e. by default an options is not decorated.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.is_options_decorator","page":"Plans","title":"Manopt.is_options_decorator","text":"is_options_decorator(o::Options)\n\nIndicate, whether Options o are of decorator type.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.decorate_options","page":"Plans","title":"Manopt.decorate_options","text":"decorate_options(o)\n\ndecorate the Optionso with specific decorators.\n\nOptional Arguments\n\noptional arguments provide necessary details on the decorators. A specific one is used to activate certain decorators.\n\ndebug – (Array{Union{Symbol,DebugAction,String,Int},1}()) a set of symbols representing DebugActions, Strings used as dividers and a subsampling integer. These are passed as a DebugGroup within :All to the DebugOptions decorator dictionary. Only excention is :Stop that is passed to :Stop.\nrecord – (Array{Union{Symbol,RecordAction,Int},1}()) specify recordings by using Symbols or RecordActions directly. The integer can again be used for only recording every ith iteration.\n\nSee also\n\nDebugOptions, RecordOptions\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"In general decorators often perform actions so we introduce","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"AbstractOptionsAction","category":"page"},{"location":"plans/index.html#Manopt.AbstractOptionsAction","page":"Plans","title":"Manopt.AbstractOptionsAction","text":"AbstractOptionsAction\n\na common Type for AbstractOptionsActions that might be triggered in decoraters, for example DebugOptions or RecordOptions.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"as well as a helper for storing values using keys, i.e.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"StoreOptionsAction\nget_storage\nhas_storage\nupdate_storage!","category":"page"},{"location":"plans/index.html#Manopt.StoreOptionsAction","page":"Plans","title":"Manopt.StoreOptionsAction","text":"StoreTupleAction <: AbstractOptionsAction\n\ninternal storage for AbstractOptionsActions to store a tuple of fields from an Optionss\n\nThis functor posesses the usual interface of functions called during an iteration, i.e. acts on (p,o,i), where p is a Problem, o is an Options and i is the current iteration.\n\nFields\n\nvalues – a dictionary to store interims values based on certain Symbols\nkeys – an NTuple of Symbols to refer to fields of Options\nonce – whether to update the internal values only once per iteration\nlastStored – last iterate, where this AbstractOptionsAction was called (to determine once\n\nConstructiors\n\nStoreOptionsAction([keys=(), once=true])\n\nInitialize the Functor to an (empty) set of keys, where once determines whether more that one update per iteration are effective\n\nStoreOptionsAction(keys, once=true])\n\nInitialize the Functor to a set of keys, where the dictionary is initialized to be empty. Further, once determines whether more that one update per iteration are effective, otherwise only the first update is stored, all others are ignored.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_storage","page":"Plans","title":"Manopt.get_storage","text":"get_storage(a,key)\n\nreturn the internal value of the StoreOptionsAction a at the Symbol key.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.has_storage","page":"Plans","title":"Manopt.has_storage","text":"get_storage(a,key)\n\nreturn whether the StoreOptionsAction a has a value stored at the Symbol key.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.update_storage!","page":"Plans","title":"Manopt.update_storage!","text":"update_storage!(a,o)\n\nupdate the StoreOptionsAction a internal values to the ones given on the Options o.\n\n\n\n\n\nupdate_storage!(a,o)\n\nupdate the StoreOptionsAction a internal values to the ones given in the dictionary d. The values are merged, where the values from d are preferred.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#DebugOptions-1","page":"Plans","title":"Debug Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Modules = [Manopt]\nPages = [\"plans/debug_options.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"plans/index.html#Manopt.DebugAction","page":"Plans","title":"Manopt.DebugAction","text":"DebugAction\n\nA DebugAction is a small functor to print/issue debug output. The usual call is given by (p,o,i) -> s that performs the debug based on a Problem p, Options o and the current iterate i.\n\nBy convention i=0 is interpreted as \"For Initialization only\", i.e. only debug info that prints initialization reacts, i<0 triggers updates of variables internally but does not trigger any output. Finally typemin(Int) is used to indicate a call from stop_solver! that returns true afterwards.\n\nFields (assumed by subtypes to exist)\n\nprint method to perform the actual print. Can for example be set to a file export,\n\nor to @info. The default is the print function on the default Base.stdout.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugChange","page":"Plans","title":"Manopt.DebugChange","text":"DebugChange(a,prefix,print)\n\ndebug for the amount of change of the iterate (stored in o.x of the Options) during the last iteration. See DebugEntryChange\n\nParameters\n\nx0 – an initial value to already get a Change after the first iterate. Can be left out\na – (StoreOptionsAction( (:x,) )) – the storage of the previous action\nprefix – (\"Last Change:\") prefix of the debug output\nprint – (print) default method to peform the print.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugCost","page":"Plans","title":"Manopt.DebugCost","text":"DebugCost <: DebugAction\n\nprint the current cost function value, see get_cost.\n\nConstructors\n\nDebugCost(long,print)\n\nwhere long indicated whether to print F(x): (default) or cost:\n\nDebugCost(prefix,print)\n\nset a prefix manually.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugDivider","page":"Plans","title":"Manopt.DebugDivider","text":"DebugDivider <: DebugAction\n\nprint a small divider (default \" | \").\n\nConstructor\n\nDebugDivider(div,print)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugEntry","page":"Plans","title":"Manopt.DebugEntry","text":"DebugEntry <: RecordAction\n\nprint a certain fields entry of type {T} during the iterates\n\nAddidtional Fields\n\nfield – Symbol the entry can be accessed with within Options\n\nConstructor\n\nDebugEntry(f[, prefix=\"$f:\", io=stdout])\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugEntryChange","page":"Plans","title":"Manopt.DebugEntryChange","text":"DebugEntryChange{T} <: DebugAction\n\nprint a certain entries change during iterates\n\nAdditional Fields\n\nprint – (print) function to print the result\nprefix – (\"Change of :x\") prefix to the print out\nfield – Symbol the field can be accessed with within Options\ndistance – function (p,o,x1,x2) to compute the change/distance between two values of the entry\nstorage – a StoreOptionsAction to store the previous value of :f\n\nConstructors\n\nDebugEntryChange(f,d[, a, prefix, io])\n\ninitialize the Debug to a field f and a distance d.\n\nDebugEntryChange(v,f,d[, a, prefix=\"Change of $f:\", io])\n\ninitialize the Debug to a field f and a distance d with initial value v for the history of o.field.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugEvery","page":"Plans","title":"Manopt.DebugEvery","text":"DebugEvery <: DebugAction\n\nevaluate and print debug only every ith iteration. Otherwise no print is performed. Whether internal variables are updates is determined by alwaysUpdate.\n\nThis method does not perform any print itself but relies on it's childrens print.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugGroup","page":"Plans","title":"Manopt.DebugGroup","text":"DebugGroup <: DebugAction\n\ngroup a set of DebugActions into one action, where the internal prints are removed by default and the resulting strings are concatenated\n\nConstructor\n\nDebugGroup(g)\n\nconstruct a group consisting of an Array of DebugActions g, that are evaluated en bloque; the method does not perform any print itself, but relies on the internal prints. It still concatenates the result and returns the complete string\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugIterate","page":"Plans","title":"Manopt.DebugIterate","text":"DebugIterate <: DebugAction\n\ndebug for the current iterate (stored in o.x).\n\nConstructor\n\nDebugIterate(io=stdout, long::Bool=false)\n\nParameters\n\nlong::Bool whether to print x: or current iterate\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugIteration","page":"Plans","title":"Manopt.DebugIteration","text":"DebugIteration <: DebugAction\n\ndebug for the current iteration (prefixed with #)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugOptions","page":"Plans","title":"Manopt.DebugOptions","text":"DebugOptions <: Options\n\nThe debug options append to any options a debug functionality, i.e. they act as a decorator pattern. Internally a Dictionary is kept that stores a DebugAction for several occasions using a Symbol as reference. The default occasion is :All and for example solvers join this field with :Start, :Step and :Stop at the beginning, every iteration or the end of the algorithm, respectively\n\nThe original options can still be accessed using the get_options function.\n\nFields (defaults in brackets)\n\noptions – the options that are extended by debug information\ndebugDictionary – a Dict{Symbol,DebugAction} to keep track of Debug for different actions\n\nConstructors\n\nDebugOptions(o,dA)\n\nconstruct debug decorated options, where dD can be\n\na DebugAction, then it is stored within the dictionary at :All\nan Array of DebugActions, then it is stored as a debugDictionary within :All.\na Dict{Symbol,DebugAction}.\nan Array of Symbols, String and an Int for the DebugFactory\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugStoppingCriterion","page":"Plans","title":"Manopt.DebugStoppingCriterion","text":"DebugStoppingCriterion <: DebugAction\n\nprint the Reason provided by the stopping criterion. Usually this should be empty, unless the algorithm stops.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugActionFactory-Tuple{String}","page":"Plans","title":"Manopt.DebugActionFactory","text":"DebugActionFactory(s)\n\ncreate a DebugAction where\n\na Stringyields the correspoinding divider\na DebugAction is passed through\na [Symbol] creates DebugEntry of that symbol, with the exceptions of :Change, :Iterate, :Iteration, and :Cost.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.DebugFactory-Tuple{Array{#s19,1} where #s19}","page":"Plans","title":"Manopt.DebugFactory","text":"DebugFactory(a)\n\ngiven an array of Symbols, Strings DebugActions and Ints\n\nThe symbol :Stop creates an entry of to display the stoping criterion at the end (:Stop => DebugStoppingCriterion())\nThe symbol :Cost creates a DebugCost\nThe symbol :iteration creates a DebugIteration\nThe symbol :Change creates a DebugChange\nany other symbol creates debug output of the corresponding field in Options\nany string creates a DebugDivider\nany DebugAction is directly included\nan Integer kintroduces that debug is only printed every kth iteration\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"see DebugSolver for details on the decorated solver.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Further specific DebugActions can be found at the specific Options.","category":"page"},{"location":"plans/index.html#RecordOptions-1","page":"Plans","title":"Record Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Modules = [Manopt]\nPages = [\"plans/record_options.jl\"]\nOrder = [:type, :function]\nPrivate = false","category":"page"},{"location":"plans/index.html#Manopt.RecordAction","page":"Plans","title":"Manopt.RecordAction","text":"RecordAction\n\nA RecordAction is a small functor to record values. The usual call is given by (p,o,i) -> s that performs the record based on a Problem p, Options o and the current iterate i.\n\nBy convention i<=0 is interpreted as \"For Initialization only\", i.e. only initialize internal values, but not trigger any record, the same holds for i=typemin(Inf) which is used to indicate stop, i.e. that the record is called from within stop_solver! which returns true afterwards.\n\nFields (assumed by subtypes to exist)\n\nrecorded_values an Array of the recorded values.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordChange","page":"Plans","title":"Manopt.RecordChange","text":"RecordChange <: RecordAction\n\ndebug for the amount of change of the iterate (stored in o.x of the Options) during the last iteration.\n\nAdditional Fields\n\nstorage a StoreOptionsAction to store (at least) o.x to use this as the last value (to compute the change)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordCost","page":"Plans","title":"Manopt.RecordCost","text":"RecordCost <: RecordAction\n\nrecord the current cost function value, see get_cost.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordEntry","page":"Plans","title":"Manopt.RecordEntry","text":"RecordEntry{T} <: RecordAction\n\nrecord a certain fields entry of type {T} during the iterates\n\nFields\n\nrecorded_values – the recorded Iterates\nfield – Symbol the entry can be accessed with within Options\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordEntryChange","page":"Plans","title":"Manopt.RecordEntryChange","text":"RecordEntryChange{T} <: RecordAction\n\nrecord a certain entries change during iterates\n\nAdditional Fields\n\nrecorded_values – the recorded Iterates\nfield – Symbol the field can be accessed with within Options\ndistance – function (p,o,x1,x2) to compute the change/distance between two values of the entry\nstorage – a StoreOptionsAction to store (at least) getproperty(o, d.field)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordEvery","page":"Plans","title":"Manopt.RecordEvery","text":"RecordEvery <: RecordAction\n\nrecord only every ith iteration. Otherwise (optionally, but activated by default) just update internal tracking values.\n\nThis method does not perform any record itself but relies on it's childrens methods\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordGroup","page":"Plans","title":"Manopt.RecordGroup","text":"RecordGroup <: RecordAction\n\ngroup a set of RecordActions into one action, where the internal prints are removed by default and the resulting strings are concatenated\n\nConstructor\n\nRecordGroup(g)\n\nconstruct a group consisting of an Array of RecordActions g, that are recording en bloque; the method does not perform any record itself, but keeps an array of records. Accessing these yields a Tuple of the recorded values per iteration\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordIterate","page":"Plans","title":"Manopt.RecordIterate","text":"RecordIterate <: RecordAction\n\nrecord the iterate\n\nConstructors\n\nRecordIterate(x0)\n\ninitialize the iterate record array to the type of x0, e.g. your initial data.\n\nRecordIterate(P)\n\ninitialize the iterate record array to the data type T.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordIteration","page":"Plans","title":"Manopt.RecordIteration","text":"RecordIteration <: RecordAction\n\nrecord the current iteration\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordOptions","page":"Plans","title":"Manopt.RecordOptions","text":"RecordOptions <: Options\n\nappend to any Options the decorator with record functionality, Internally a Dictionary is kept that stores a RecordAction for several occasions using a Symbol as reference. The default occasion is :All and for example solvers join this field with :Start, :Step and :Stop at the beginning, every iteration or the end of the algorithm, respectively\n\nThe original options can still be accessed using the get_options function.\n\nFields\n\noptions – the options that are extended by debug information\nrecordDictionary – a Dict{Symbol,RecordAction} to keep track of all different recorded values\n\nConstructors\n\nRecordOptions(o,dR)\n\nconstruct record decorated Options, where dR can be\n\na RecordAction, then it is stored within the dictionary at :All\nan Array of RecordActions, then it is stored as a recordDictionary(@ref) within the dictionary at :All.\na Dict{Symbol,RecordAction}.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordActionFactory-Union{Tuple{A}, Tuple{O}, Tuple{O,A}} where A<:RecordAction where O<:Options","page":"Plans","title":"Manopt.RecordActionFactory","text":"RecordActionFactory(s)\n\ncreate a RecordAction where\n\na RecordAction is passed through\na [Symbol] creates RecordEntry of that symbol, with the exceptions of :Change, :Iterate, :Iteration, and :Cost.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.RecordFactory-Union{Tuple{O}, Tuple{O,Array{#s12,1} where #s12}} where O<:Options","page":"Plans","title":"Manopt.RecordFactory","text":"RecordFactory(a)\n\ngiven an array of Symbols and RecordActions and Ints\n\nThe symbol :Cost creates a RecordCost\nThe symbol :iteration creates a RecordIteration\nThe symbol :Change creates a RecordChange\nany other symbol creates a RecordEntry of the corresponding field in Options\nany RecordAction is directly included\nan Integer k introduces that record is only performed every kth iteration\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.get_record","page":"Plans","title":"Manopt.get_record","text":"get_record(o[,s=:Step])\n\nreturn the recorded values from within the RecordOptions o that where recorded with respect to the Symbol s as an Array. The default refers to any recordings during an Iteration represented by the Symbol :Step\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.get_record-Union{Tuple{R}, Tuple{R}} where R<:RecordAction","page":"Plans","title":"Manopt.get_record","text":"get_record(r)\n\nreturn the recorded values stored within a RecordAction r.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.has_record-Tuple{RecordOptions}","page":"Plans","title":"Manopt.has_record","text":"has_record(o)\n\ncheck whether the Optionso are decorated with RecordOptions\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"see RecordSolver for details on the decorated solver.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Further specific RecordActions can be found at the specific Options.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"there's one internal helper that might be useful for you own actions, namely","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"record_or_reset!","category":"page"},{"location":"plans/index.html#Manopt.record_or_reset!","page":"Plans","title":"Manopt.record_or_reset!","text":"record_or_reset!(r,v,i)\n\neither record (i>0 and not Inf) the value v within the RecordAction r or reset (i<0) the internal storage, where v has to match the internal value type of the corresponding Recordaction.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Stepsize-1","page":"Plans","title":"Stepsize and Linesearch","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"The step size determination is implemented as a Functor based on","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Stepsize","category":"page"},{"location":"plans/index.html#Manopt.Stepsize","page":"Plans","title":"Manopt.Stepsize","text":"Stepsize\n\nAn abstract type for the functors representing step sizes, i.e. they are callable structurs. The naming scheme is TypeOfStepSize, e.g. ConstantStepsize.\n\nEvery Stepsize has to provide a constructor and its function has to have the interface (p,o,i) where a Problem as well as Options and the current number of iterations are the arguments and returns a number, namely the stepsize to use.\n\nSee also\n\nLinesearch\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"in general there are","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Modules = [Manopt]\nPages = [\"plans/stepsize.jl\"]\nOrder = [:type,:function]","category":"page"},{"location":"plans/index.html#Manopt.ArmijoLinesearch","page":"Plans","title":"Manopt.ArmijoLinesearch","text":"ArmijoLineseach <: Linesearch\n\nA functor representing Armijo line seach including the last runs state, i.e. a last step size.\n\nFields\n\ninitialStepsize – (1.0) and initial step size\nretraction_method – (ExponentialRetraction()) the rectraction to use, defaults to the exponential map\ncontractionFactor – (0.95) exponent for line search reduction\nsufficientDecrease – (0.1) gain within Armijo's rule\nlastStepSize – (initialstepsize) the last step size we start the search with\n\nConstructor\n\nArmijoLineSearch()\n\nwith the Fields above in their order as optional arguments.\n\nThis method returns the functor to perform Armijo line search, where two inter faces are available:\n\nbased on a tuple (p,o,i) of a GradientProblem p, Options o and a current iterate i.\nwith (M,x,F,∇Fx[,η=-∇Fx]) -> s where Manifold M, a current point x a function F, that maps from the manifold to the reals, its gradient (a tangent vector) ∇F=∇F(x) at x and an optional search direction tangent vector η=∇F are the arguments.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.ConstantStepsize","page":"Plans","title":"Manopt.ConstantStepsize","text":"ConstantStepsize <: Stepsize\n\nA functor that always returns a fixed step size.\n\nFields\n\nlength – constant value for the step size.\n\nConstructor\n\nConstantStepSize(s)\n\ninitialize the stepsie to a constant s\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DecreasingStepsize","page":"Plans","title":"Manopt.DecreasingStepsize","text":"DecreasingStepsize()\n\nA functor that represents several decreasing step sizes\n\nFields\n\nlength – (1) the initial step size l.\nfactor – (1) a value f to multiply the initial step size with every iteration\nsubtrahend – (0) a value a that is subtracted every iteration\nexponent – (1) a value e the current iteration numbers eth exponential is taken of\n\nIn total the complete formulae reads for the ith iterate as\n\n$ s_i = \\frac{(l-i\\cdot a)f^i}{i^e}$\n\nand hence the default simplifies to just $ s_i = \\frac{l}{i} $\n\nConstructor\n\nConstantStepSize(l,f,a,e)\n\ninitialiszes all fields above, where none of them is mandatory.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.Linesearch","page":"Plans","title":"Manopt.Linesearch","text":"Linesearch <: Stepsize\n\nAn abstract functor to represent line search type step size deteminations, see Stepsize for details. One example is the ArmijoLinesearch functor.\n\nCompared to simple step sizes, the linesearch functors provide an interface of the form (p,o,i,η) -> s with an additional (but optional) fourth parameter to proviade a search direction; this should default to something reasonable, e.g. the negative gradient.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.NonmonotoneLinesearch","page":"Plans","title":"Manopt.NonmonotoneLinesearch","text":"NonmonotoneLinesearch <: Linesearch\n\nA functor representing a nonmonotone line seach using the Barzilai-Borwein step size[Iannazzo2018]. Together with a gradient descent algorithm this line search represents the Riemannian Barzilai-Borwein with nonmonotone line-search (RBBNMLS) algorithm. We shifted the order of the algorithm steps from the paper by Iannazzo and Porcelli so that in each iteration we first find\n\ny_k = F(x_k) - operatornameT_x_k-1 to x_k(F(x_k-1))\n\nand\n\ns_k = - α_k-1 * operatornameT_x_k-1 to x_k(F(x_k-1))\n\nwhere α _k-1 is the step size computed in the last iteration and operatornameT is a vector transport. We then find the Barzilai–Borwein step size\n\nα_k^textBB = begincases\nmin(α_textmax max(α_textmin τ_k))   textif  s_k y_k_x_k  0\nα_textmax  textelse\nendcases\n\nwhere\n\nτ_k = fracs_k s_k_x_ks_k y_k_x_k\n\nif the direct strategy is chosen,\n\nτ_k = fracs_k y_k_x_ky_k y_k_x_k\n\nin case of the inverse strategy and an alternation between the two in case of the alternating strategy. Then we find the smallest h = 0 1 2  such that\n\nF(operatornameretr_x_k(- σ^h α_k^textBB F(x_k)))\nleq\nmax_1  j  min(k+1m) F(x_k+1-j) - γ σ^h α_k^textBB F(x_k) F(x_k)_x_k\n\nwhere σ is a step length reduction factor in (01), m is the number of iterations after which the function value has to be lower than the current one and γ is the sufficient decrease parameter in (01). We can then find the new stepsize by\n\nα_k = σ^h α_k^textBB\n\n[Iannazzo2018]: B. Iannazzo, M. Porcelli, The Riemannian Barzilai–Borwein Method with Nonmonotone Line Search and the Matrix Geometric Mean Computation, In: IMA Journal of Numerical Analysis. Volume 38, Issue 1, January 2018, Pages 495–517, doi 10.1093/imanum/drx015\n\nFields\n\ninitial_stepsize – (1.0) the step size we start the search with\nretraction_method – (ExponentialRetraction()) the rectraction to use\nvector_transport_method – (ParallelTransport()) the vector transport method to use\nstepsize_reduction – (0.5) step size reduction factor contained in the interval (0,1)\nsufficient_decrease – (1e-4) sufficient decrease parameter contained in the interval (0,1)\nmemory_size – (10) number of iterations after which the cost value needs to be lower than the current one\nmin_stepsize – (1e-3) lower bound for the Barzilai-Borwein step size greater than zero\nmax_stepsize – (1e3) upper bound for the Barzilai-Borwein step size greater than min_stepsize\nstrategy – (direct) defines if the new step size is computed using the direct, indirect or alternating strategy\nstorage – (x, ∇F) a StoreOptionsAction to store old_x and old_∇, the x-value and corresponding gradient of the previous iteration\n\nConstructor\n\nNonmonotoneLinesearch()\n\nwith the Fields above in their order as optional arguments.\n\nThis method returns the functor to perform nonmonotone line search.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.WolfePowellBinaryLinesearch","page":"Plans","title":"Manopt.WolfePowellBinaryLinesearch","text":"WolfePowellBinaryLinesearch <: Linesearch\n\nA Linesearch method that determines a step size t fulfilling the Wolfe conditions\n\nbased on a binary chop. Let η be a search direction and c_1c_20 be two constants. Then with\n\nA(t) = f(x_+)  c_1 t f(x) η_x\nquadtextandquad \nW(t) = f(x_+) textV_x_+gets xη_x_+  c_2 η f(x)_x\n\nwhere x_+ = operatornameretr_x(tη) is the current trial point, and textV is a vector transport, we perform the following Algorithm similar to Algorithm 7 from [Huang2014]\n\nset α=0, β= and t=1.\nWhile either A(t) does not hold or W(t) does not hold do steps 3-5.\nIf A(t) fails, set β=t.\nIf A(t) holds but W(t) fails, set α=t.\nIf β set t=fracα+β2, otherwise set t=2α.\n\nConstructor\n\nWolfePowellBinaryLinesearch(\n    retr::AbstractRetractionMethod=ExponentialRetraction(),\n    vtr::AbstractVectorTransportMethod=ParallelTransport(),\n    c_1::Float64=10^(-4),\n    c_2::Float64=0.999\n)\n\n[Huang2014]: Huang, W.: Optimization algorithms on Riemannian manifolds with applications, Dissertation, Flordia State University, 2014. pdf\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.WolfePowellLineseach","page":"Plans","title":"Manopt.WolfePowellLineseach","text":"WolfePowellLineseach <: Linesearch\n\nDo a backgtracking linesearch to find a step size α that fulfills the Wolfe conditions along a search direktion η starting ffrom x, i.e.\n\nfbigl( operatornameretr_x(αη) bigr)  f(x_k) + c_1 α_k f(x) η_x\nquadtextandquad\nfracmathrmdmathrmdt fbigr(operatornameretr_x(tη)bigr)\nBigvert_t=α\n c_2 fracmathrmdmathrmdt fbigl(operatornameretr_x(tη)bigr)Bigvert_t=0\n\nConstructor\n\nWolfePowellLinesearch(\n    retr::AbstractRetractionMethod=ExponentialRetraction(),\n    vtr::AbstractVectorTransportMethod=ParallelTransport(),\n    c_1::Float64=10^(-4),\n    c_2::Float64=0.999\n)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_stepsize-Tuple{Problem,Options,Vararg{Any,N} where N}","page":"Plans","title":"Manopt.get_stepsize","text":"get_stepsize(p::Problem, o::Options, vars...)\n\nreturn the stepsize stored within Options o when solving Problem p. This method also works for decorated options and the Stepsize function within the options, by default stored in o.stepsize.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.linesearch_backtrack-Union{Tuple{T}, Tuple{TF}, Tuple{ManifoldsBase.Manifold,TF,Any,T,Any,Any,Any}, Tuple{ManifoldsBase.Manifold,TF,Any,T,Any,Any,Any,ManifoldsBase.AbstractRetractionMethod}, Tuple{ManifoldsBase.Manifold,TF,Any,T,Any,Any,Any,ManifoldsBase.AbstractRetractionMethod,T}, Tuple{ManifoldsBase.Manifold,TF,Any,T,Any,Any,Any,ManifoldsBase.AbstractRetractionMethod,T,Any}} where T where TF","page":"Plans","title":"Manopt.linesearch_backtrack","text":"linesearch_backtrack(M, F, x, ∇F, s, decrease, contract, retr, η = -∇F, f0 = F(x))\n\nperform a linesearch for\n\na manifold M\na cost function F,\nan iterate x\nthe gradient F(x)\nan initial stepsize s usually called γ\na sufficient decrease\na contraction factor σ\na retraction, which defaults to the ExponentialRetraction()\na search direction η = -F(x)\nan offset, f_0 = F(x)\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Problems-1","page":"Plans","title":"Problems","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"A problem usually contains its cost function and provides and implementation to access the cost","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Problem\nget_cost","category":"page"},{"location":"plans/index.html#Manopt.Problem","page":"Plans","title":"Manopt.Problem","text":"Problem\n\nSpecify properties (values) and related functions for computing a certain optimization problem.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_cost","page":"Plans","title":"Manopt.get_cost","text":"get_cost(p,x)\n\nevaluate the cost function F stored within a Problem at the point x.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Cost-based-problem-1","page":"Plans","title":"Cost based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"CostProblem","category":"page"},{"location":"plans/index.html#Manopt.CostProblem","page":"Plans","title":"Manopt.CostProblem","text":"CostProblem <: Problem\n\nspeficy a problem for solvers just based on cost functions, i.e. gradient free ones.\n\nFields\n\nM            – a manifold mathcal M\ncost – a function Fcolonmathcal Mtomathbb R to minimize\n\nSee also\n\nNelderMead\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Gradient-based-problem-1","page":"Plans","title":"Gradient based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"GradientProblem\nStochasticGradientProblem\nget_gradient\nget_gradients","category":"page"},{"location":"plans/index.html#Manopt.GradientProblem","page":"Plans","title":"Manopt.GradientProblem","text":"GradientProblem <: Problem\n\nspecify a problem for gradient based algorithms.\n\nFields\n\nM            – a manifold mathcal M\ncost – a function Fcolonmathcal Mtomathbb R to minimize\ngradient     – the gradient nabla Fcolonmathcal M to mathcal Tmathcal M of the cost function F\n\nSee also\n\ngradient_descent GradientDescentOptions\n\n\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.StochasticGradientProblem","page":"Plans","title":"Manopt.StochasticGradientProblem","text":"StochasticGradientProblem <: Problem\n\nA stochastic gradient problem consists of\n\na Manifold M\na(n optional) cost function ``f(x) = \\displaystyle\\sum{i=1}^n fi(x)\nan array of gradients, i.e. a function that returns and array or an array of functions f_i_i=1^n.\n\nConstructors\n\nStochasticGradientProblem(M::Manifold, ∇::Function; cost=Missing())\nStochasticGradientProblem(M::Manifold, ∇::AbstractVector{<:Function}; cost=Missing())\n\nCreate a Stochastic gradient problem with an optional cost and the gradient either as one function (returning an array) or a vector of functions.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_gradient","page":"Plans","title":"Manopt.get_gradient","text":"get_gradient(p,x)\n\nevaluate the gradient of a GradientProblemp at the point x.\n\n\n\n\n\nget_gradient(p,x)\n\nevaluate the gradient of a HessianProblemp at the point x.\n\n\n\n\n\nget_gradient(P::StochasticGradientProblem, k, x)\n\nEvaluate one of the summands gradients f_k, kin 1n, at x.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.get_gradients","page":"Plans","title":"Manopt.get_gradients","text":"get_gradients(P::StochasticGradientProblem, x)\n\nEvaluate all summands gradients f_i_i=1^n at x.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Subgradient-based-problem-1","page":"Plans","title":"Subgradient based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"SubGradientProblem\nget_subgradient","category":"page"},{"location":"plans/index.html#Manopt.SubGradientProblem","page":"Plans","title":"Manopt.SubGradientProblem","text":"SubGradientProblem <: Problem\n\nA structure to store information about a subgradient based optimization problem\n\nFields\n\nmanifold – a Manifold\ncost – the function F to be minimized\nsubgradient – a function returning a subgradient partial F of F\n\nConstructor\n\nSubGradientProblem(M, f, ∂f)\n\nGenerate the [Problem] for a subgradient problem, i.e. a function f on the manifold M and a function ∂f that returns an element from the subdifferential at a point.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_subgradient","page":"Plans","title":"Manopt.get_subgradient","text":"get_subgradient(p,x)\n\nEvaluate the (sub)gradient of a SubGradientProblemp at the point x.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#ProximalProblem-1","page":"Plans","title":"Proximal Map(s) based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"ProximalProblem\nget_proximal_map","category":"page"},{"location":"plans/index.html#Manopt.ProximalProblem","page":"Plans","title":"Manopt.ProximalProblem","text":"ProximalProblem <: Problem\n\nspecify a problem for solvers based on the evaluation of proximal map(s).\n\nFields\n\nM - a Manifold mathcal M\ncost - a function Fcolonmathcal Mtomathbb R to minimize\nproxes - proximal maps operatornameprox_lambdavarphicolonmathcal Mtomathcal M as functions (λ,x) -> y, i.e. the prox parameter λ also belongs to the signature of the proximal map.\nnumber_of_proxes - (length(proxes)) number of proxmal Maps, e.g. if one of the maps is a combined one such that the proximal Maps functions return more than one entry per function\n\nSee also\n\ncyclic_proximal_point, get_cost, get_proximal_map\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_proximal_map","page":"Plans","title":"Manopt.get_proximal_map","text":"get_proximal_map(p,λ,x,i)\n\nevaluate the ith proximal map of ProximalProblem p at the point x of p.M with parameter λ0.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Further-planned-problems-1","page":"Plans","title":"Further planned problems","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"HessianProblem\ngetHessian\nget_preconditioner","category":"page"},{"location":"plans/index.html#Manopt.HessianProblem","page":"Plans","title":"Manopt.HessianProblem","text":"HessianProblem <: Problem\n\nspecify a problem for hessian based algorithms.\n\nFields\n\nM            : a manifold mathcal M\ncost : a function Fcolonmathcal Mtomathbb R to minimize\ngradient     : the gradient nabla Fcolonmathcal M to mathcal Tmathcal M of the cost function F\nhessian      : the hessian operatornameHessF (cdot)_ x colon mathcal T_x mathcal M to mathcal T_x mathcal M of the cost function F\nprecon       : the symmetric, positive deﬁnite   preconditioner (approximation of the inverse of the Hessian of F)\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.getHessian","page":"Plans","title":"Manopt.getHessian","text":"getHessian(p,x,ξ)\n\nevaluate the Hessian of a HessianProblem p at the point x applied to a tangent vector ξ.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.get_preconditioner","page":"Plans","title":"Manopt.get_preconditioner","text":"get_preconditioner(p,x,ξ)\n\nevaluate the symmetric, positive deﬁnite preconditioner (approximation of the inverse of the Hessian of the cost function F) of a HessianProblem p at the point xapplied to a tangent vector ξ.\n\n\n\n\n\n","category":"function"},{"location":"functions/Jacobi_fields.html#JacobiFieldFunctions-1","page":"Jacobi Fields","title":"Jacobi Fields","text":"","category":"section"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"A smooth tangent vector field Jcolon 01 to Tmathcal M along a geodesic g(cdotxy) is called Jacobi field if it fulfills the ODE","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"displaystyle 0 = fracDdtJ + R(Jdot g)dot g","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"where R is the Riemannian curvature tensor. Such Jacobi fields can be used to derive closed forms for the exponential map, the logarithmic map and the geodesic, all of them with respect to both arguments: Let Fcolonmathcal N to mathcal M be given (for the exp_xcdot   we have mathcal N = T_xmathcal M, otherwise mathcal N=mathcal M) and denote by Xi_1ldotsXi_d an orthonormal frame along g(cdotxy) that diagonalizes the curvature tensor with corresponding eigenvalues kappa_1ldotskappa_d. Note that on symmetric manifolds such a frame always exists.","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Then DF(x)eta = sum_k=1^d langle etaXi_k(0)rangle_xbeta(kappa_k)Xi_k(T) holds, where T also depends on the function F as the weights beta. The values stem from solving the corresponding system of (decoupled) ODEs.","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Note that in different references some factors might be a little different, for example when using unit speed geodesics.","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"The following weights functions are available","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Modules = [Manopt]\nPages   = [\"Jacobi_fields.jl\"]","category":"page"},{"location":"functions/Jacobi_fields.html#Manopt.adjoint_Jacobi_field","page":"Jacobi Fields","title":"Manopt.adjoint_Jacobi_field","text":"Y = adjoint_Jacobi_field(M, p, q, t, X, β)\n\nCompute the AdjointJacobiField J along the geodesic γ_pq on the manifold mathcal M with initial conditions (depending on the application) X  T_γ_pq(t)mathcal M and weights β. The result is a vector Y  T_pmathcal M. The main difference to jacobi_field is the, that the input X and the output Y switched tangent spaces. For detais see jacobi_field\n\n\n\n\n\n","category":"function"},{"location":"functions/Jacobi_fields.html#Manopt.jacobi_field","page":"Jacobi Fields","title":"Manopt.jacobi_field","text":"Y = jacobi_field(M, p, q, t, X, β)\n\ncompute the Jacobi jield J along the geodesic γ_pq on the manifold mathcal M with initial conditions (depending on the application) X  T_pmathcal M and weights β. The result is a tangent vector Y from T_γ_pq(t)mathcal M.\n\nSee also\n\nadjoint_Jacobi_field\n\n\n\n\n\n","category":"function"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_exp_argument-Tuple{Any,Number,Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_exp_argument","text":"βdifferential_exp_argument(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_X exp_p XY. They are\n\nbeta(kappa) = begincases\nfracsinh(dsqrt-kappa)dsqrt-kappatext if kappa  0\n1  text if  kappa = 0\nfracsin(dsqrtkappa)sqrtdkappatext if kappa  0\nendcases\n\nSee also\n\ndifferential_exp_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_exp_basepoint-Tuple{Any,Number,Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_exp_basepoint","text":"βdifferential_exp_basepoint(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_p exp_p X Y. They are\n\nbeta(kappa) = begincases\ncosh(sqrt-kappa)text if kappa  0\n1  text if  kappa = 0\ncos(sqrtkappa) text if kappa  0\nendcases\n\nSee also\n\ndifferential_exp_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_geodesic_startpoint-Tuple{Any,Any,Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_geodesic_startpoint","text":"βdifferential_geodesic_startpoint(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_x g(tpq)X. They are\n\nbeta(kappa) = begincases\nfracsinh(d(1-t)sqrt-kappa)sinh(dsqrt-kappa)\ntext if kappa  0\n1-t  text if  kappa = 0\nfracsin((1-t)dsqrtkappa)sinh(dsqrtkappa)\ntext if kappa  0\nendcases\n\nDue to a symmetry agrument, these are also used to compute D_q g(t pq)eta\n\nSee also\n\ndifferential_geodesic_endpoint, differential_geodesic_startpoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_log_argument-Tuple{Any,Number,Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_log_argument","text":"βdifferential_log_argument(κ,t,d)\n\nweights for the JacobiField corresponding to the differential of the logarithmic map with respect to its argument D_q log_p qX. They are\n\nbeta(kappa) = begincases\nfrac dsqrt-kappa sinh(dsqrt-kappa)text if kappa  0\n1  text if  kappa = 0\nfrac dsqrtkappa sin(dsqrtkappa)text if kappa  0\nendcases\n\nSee also\n\ndifferential_log_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_log_basepoint-Tuple{Any,Number,Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_log_basepoint","text":"βdifferential_log_basepoint(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_p log_p qX. They are\n\nbeta(kappa) = begincases\n-sqrt-kappadfraccosh(dsqrt-kappa)sinh(dsqrt-kappa)text if kappa  0\n-1  text if  kappa = 0\n-sqrtkappadfraccos(dsqrtkappa)sin(dsqrtkappa)text if kappa  0\nendcases\n\nSee also\n\ndifferential_log_argument, differential_log_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Exports-1","page":"Exports","title":"Exports","text":"","category":"section"},{"location":"helpers/exports.html#","page":"Exports","title":"Exports","text":"Exports aim to provide a consistent generation of images of your results. For example if you record the trace your algorithm walks on the Sphere, you yan easily export this trace to a rendered image using asymptote_export_S2_signals and render the result with Asymptote. Despite these, you can always record values during your iterations, and export these, for example to csv.","category":"page"},{"location":"helpers/exports.html#Asymptote-1","page":"Exports","title":"Asymptote","text":"","category":"section"},{"location":"helpers/exports.html#","page":"Exports","title":"Exports","text":"The following functions provide exports both in graphics and/or raw data using Asymptote.","category":"page"},{"location":"helpers/exports.html#","page":"Exports","title":"Exports","text":"Modules = [Manopt]\nPages   = [\"Asymptote.jl\"]","category":"page"},{"location":"helpers/exports.html#Manopt.asymptote_export_S2_data-Tuple{String}","page":"Exports","title":"Manopt.asymptote_export_S2_data","text":"asymptote_export_S2_data(filename)\n\nExport given data as an array of points on the sphere, i.e. one-, two- or three-dimensional data with points on the Sphere mathbb S^2.\n\nInput\n\nfilename – a file to store the Asymptote code in.\n\nOptional Arguments (Data)\n\ndata – a point representing the 1-,2-, or 3-D array of points\nelevation_color_scheme - A ColorScheme for elevation\nscale_axes - ((1/3,1/3,1/3)) move spheres closer to each other by a factor per direction\n\nOptional Arguments (Asymptote)\n\narrow_head_size - (1.8) size of the arrowheads of the vectors (in mm)\ncamera_position - position of the camrea (default: centered above xy-plane) szene\ntarget - position the camera points at (default: center of xy-plane within data).\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Manopt.asymptote_export_S2_signals-Tuple{String}","page":"Exports","title":"Manopt.asymptote_export_S2_signals","text":"asymptote_export_S2_signals(filename; points, curves, tangent_vectors, colors, options...)\n\nExport given points, curves, and tangent_vectors on the sphere mathbb S^2 to Asymptote.\n\nInput\n\nfilename – a file to store the Asymptote code in.\n\nOptional Arguments (Data)\n\ncolors - dictionary of color arrays (indexed by symbols :points, :curves and :tvector) where each entry has to provide as least as many colors as the length of the corresponding sets.\ncurves – an Array of Arrays of points on the sphere, where each inner array is interpreted as a curve and is accompanied by an entry within colors\npoints – an Array of Arrays of points on the sphere where each inner array is itnerpreted as a set of points and is accompanied by an entry within colors\ntangent_vectors – an Array of Arrays of tuples, where the first is a points, the second a tangent vector and each set of vectors is accompanied by an entry from within colors\n\nOptional Arguments (Asymptote)\n\narrow_head_size - (6.0) size of the arrowheads of the tangent vectors\narrow_head_sizes – overrides the previous value to specify a value per tVector set.\ncamera_position - ((1., 1., 0.)) position of the camera in the Asymptote szene\nline_width – (1.0) size of the lines used to draw the curves.\nline_widths – overrides the previous value to specify a value per curve and tVector set.\ndot_size – (1.0) size of the dots used to draw the points.\ndot_sizes – overrides the previous value to specify a value per point set.\nsphere_color – (RGBA{Float64}(0.85, 0.85, 0.85, 0.6)) color of the sphere the data is drawn on\nsphere_line_color –  (RGBA{Float64}(0.75, 0.75, 0.75, 0.6)) color of the lines on the sphere\nsphere_line_width – (0.5) line width of the lines on the sphere\ntarget – ((0.,0.,0.)) position the camera points at\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Manopt.asymptote_export_SPD-Tuple{String}","page":"Exports","title":"Manopt.asymptote_export_SPD","text":"asymptote_export_SPD(filename)\n\nexport given data as a point on a Power{SPDPoint} manifold, i.e. one-, two- or three-dimensional data with points on the manifold of symmetric positive definite matrices.\n\nInput\n\nfilename – a file to store the Asymptote code in.\n\nOptional Arguments (Data)\n\ndata – a point representing the 1-,2-, or 3-D array of SPDPoints\ncolor_scheme - A ColorScheme for Geometric Anisotropy Index\nscale_axes - ((1/3,1/3,1/3)) move symmetric positive definite matrices closer to each other by a factor per direction compared to the distance esimated by the maximal eigenvalue of all involved SPD points\n\nOptional Arguments (Asymptote)\n\ncamera_position - position of the camrea (default: centered above xy-plane) szene.\ntarget - position the camera points at (default: center of xy-plane within data).\n\nBoth values camera_position and target are scaled by scaledAxes*EW, where EW is the maximal eigenvalue in the data.\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Manopt.render_asymptote-Tuple{Any}","page":"Exports","title":"Manopt.render_asymptote","text":"render_asymptote(filename; render=4, format=\"png\", ...)\n\nrender an exported asymptote file specified in the filename, which can also be given as a relative or full path\n\nInput\n\nfilename – filename of the exported asy and rendered image\n\nKeyword Arguments\n\nthe default values are given in brackets\n\nrender – (4) render level of asymptote, i.e. its -render option\nformat – (\"png\") final rendered format, i.e. asymptote's -f option\nexport_file - (the filename with format as ending) specify the export filename\n\n\n\n\n\n","category":"method"},{"location":"solvers/gradient_descent.html#GradientDescentSolver-1","page":"Gradient Descent","title":"Gradient Descent","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"  gradient_descent\n  gradient_descent!","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.gradient_descent","page":"Gradient Descent","title":"Manopt.gradient_descent","text":"gradient_descent(M, F, ∇F, x)\n\nperform a gradientdescent ``x{k+1} = \\mathrm{retr}{xk} sk\\nabla f(xk)with different choices ofs_k`available (seestepsize` option below).\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n∇F – the gradient nabla Fcolonmathcal Mto Tmathcal M of F\nx – an initial value x  mathcal M\n\nOptional\n\nstepsize – (ConstantStepsize(1.)) specify a Stepsize functor.\nretraction_method – (ExponentialRetraction()) a retraction(M,x,ξ) to use.\nstopping_criterion – (StopWhenAny(StopAfterIteration(200),StopWhenGradientNormLess(10.0^-8))) a functor inheriting from StoppingCriterion indicating when to stop.\nreturn_options – (false) – if activated, the extended result, i.e. the   complete Options are returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/gradient_descent.html#Manopt.gradient_descent!","page":"Gradient Descent","title":"Manopt.gradient_descent!","text":"gradient_descent!(M, F, ∇F, x)\n\nperform a gradientdescent ``x{k+1} = \\mathrm{retr}{xk} sk\\nabla f(xk)inplace of x with different choices ofs_k`` available.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\n∇F – the gradient nabla Fcolonmathcal Mto Tmathcal M of F\nx – an initial value x  mathcal M\n\nFor more options, especially Stepsizes for s_k, see gradient_descent\n\n\n\n\n\n","category":"function"},{"location":"solvers/gradient_descent.html#Options-1","page":"Gradient Descent","title":"Options","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"AbstractGradientDescentOptions\nGradientDescentOptions","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.AbstractGradientDescentOptions","page":"Gradient Descent","title":"Manopt.AbstractGradientDescentOptions","text":"AbstractGradientDescentOptions <: Options\n\nA generic Options type for gradient based options data.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.GradientDescentOptions","page":"Gradient Descent","title":"Manopt.GradientDescentOptions","text":"GradientDescentOptions{P,T} <: AbstractGradientDescentOptions\n\nDescribes a Gradient based descent algorithm, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx0 – an a point (of type P) on a manifold as starting point\nstopping_criterion – (StopAfterIteration(100)) a StoppingCriterion\nstepsize – (ConstantStepsize(1.))a Stepsize\ndirection - (IdentityUpdateRule) a processor to compute the gradient\nretraction_method – (ExponentialRetraction()) the rectraction to use, defaults to the exponential map\n\nConstructor\n\nGradientDescentOptions(x, stop, s [, retr=ExponentialRetraction()])\n\nconstruct a Gradient Descent Option with the fields and defaults as above\n\nSee also\n\ngradient_descent, GradientProblem\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Direction-Update-Rules-1","page":"Gradient Descent","title":"Direction Update Rules","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"A field of the options is the direction, a DirectionUpdateRule, which by default IdentityUpdateRule just evaluates the gradient but can be enhanced for example to","category":"page"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"DirectionUpdateRule\nIdentityUpdateRule\nMomentumGradient\nAverageGradient\nNesterov","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.DirectionUpdateRule","page":"Gradient Descent","title":"Manopt.DirectionUpdateRule","text":"DirectionUpdateRule\n\nA general functor, that handles direction update rules. It's field(s) is usually only a StoreOptionsAction by default initialized to the fields required for the specific coefficient, but can also be replaced by a (common, global) individual one that provides these values.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.IdentityUpdateRule","page":"Gradient Descent","title":"Manopt.IdentityUpdateRule","text":"IdentityUpdateRule <: DirectionUpdateRule\n\nThe default gradient direction update is the identity, i.e. it just evaluates the gradient.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.MomentumGradient","page":"Gradient Descent","title":"Manopt.MomentumGradient","text":"MomentumGradient <: DirectionUpdateRule\n\nAppend a momentum to a gradient processor, where the last direction and last iterate are stored and the new is composed as abla_i = m* abla_i-1 - s d_i, where sd_i is the current (inner) direction and abla_i-1 is the vector transported last direction multiplied by momentum m.\n\nFields\n\n∇ – (zero_tangent_vector(M,x0)) the last gradient/direction update added as momentum\nlast_iterate - remember the last iterate for parallel transporting the last direction\nmomentum – (0.2) factor for momentum\ndirection – internal DirectionUpdateRule to determine directions to add the momentum to.\nvector_transport_method vector transport method to use\n\nConstructors\n\nMomentumGradient(\n    p::GradientProlem,\n    x0,\n    s::DirectionUpdateRule=Gradient();\n    ∇=zero_tangent_vector(p.M, o.x), momentum=0.2\n   vector_transport_method=ParallelTransport(),\n)\n\nAdd momentum to a gradient problem, where by default just a gradient evaluation is used Equivalently you can also use a Manifold M instead of the GradientProblem p.\n\nMomentumGradient(\n    p::StochasticGradientProblem\n    x0\n    s::DirectionUpdateRule=StochasticGradient();\n    ∇=zero_tangent_vector(p.M, x0), momentum=0.2\n   vector_transport_method=ParallelTransport(),\n)\n\nAdd momentum to a stochastic gradient problem, where by default just a stochastic gradient evaluation is used\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.AverageGradient","page":"Gradient Descent","title":"Manopt.AverageGradient","text":"AverageGradient <: DirectionUpdateRule\n\nAdd an average of gradients to a gradient processor. A set of previous directions (from the inner processor) and the last iterate are stored, average is taken after vector transporting them to the current iterates tangent space.\n\nFields\n\ngradients – (fill(zero_tangent_vector(M,x0),n)) the last n gradient/direction updates\nlast_iterate – last iterate (needed to transport the gradients)\ndirection – internal DirectionUpdateRule to determine directions to apply the averaging to\nvector_transport_method - vector transport method to use\n\nConstructors\n\nAverageGradient(\n    p::GradientProlem,\n    x0,\n    n::Int=10\n    s::DirectionUpdateRule=IdentityUpdateRule();\n    gradients = fill(zero_tangent_vector(p.M, o.x),n),\n    last_iterate = deepcopy(x0),\n    vector_transport_method = ParallelTransport()\n)\n\nAdd average to a gradient problem, n determines the size of averaging and gradients can be prefilled with some history Equivalently you can also use a Manifold M instead of the GradientProblem p.\n\nAverageGradient(\n    p::StochasticGradientProblem\n    x0\n    n::Int=10\n    s::DirectionUpdateRule=StochasticGradient();\n    gradients = fill(zero_tangent_vector(p.M, o.x),n),\n    last_iterate = deepcopy(x0),\n    vector_transport_method = ParallelTransport()\n)\n\nAdd average to a stochastic gradient problem, n determines the size of averaging and gradients can be prefilled with some history\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.Nesterov","page":"Gradient Descent","title":"Manopt.Nesterov","text":"Nesterov <: DirectionUpdateRule\n\nFields\n\nγ\nμ the strong convexity coefficient\nv (==v_k, v_0=x_0) an interims point to compute the next gradient evaluation point y_k\nshrinkage (= i -> 0.8) a function to compute the shrinkage β_k per iterate.\n\nLet's assume f is L-Lipschitz and μ-strongly convex. Given\n\na step size h_kfrac1L (from the GradientDescentOptions\na shrinkage parameter β_k\nand a current iterate x_k\nas well as the interims values γ_kandv_k`` from the previous iterate.\n\nThis compute a Nesterov type update using the following steps, see [ZhangSra2018]\n\nCopute the positive root, i.e. α_k(01) of α^2 = h_kbigl((1-α_k)γ_k+α_k μbigr).\nSet bar γ_k+1 = (1-α_k)γ_k + α_kμ\ny_k = operatornameretr_x_kBigl(fracα_kγ_kγ_k + α_kμoperatornameretr^-1_x_kv_k Bigr)\nx_k+1 = operatornameretr_y_k(-h_k f(y_k))\nv_k+1 = operatornameretr_y_kBigl(frac(1-α_k)γ_kbarγ_koperatornameretr_y_k^-1(v_k) - fracα_kbar γ_k+1f(y_k) Bigr)\nγ_k+1 = frac11+β_kbar γ_k+1\n\nThen the direction from x_k to x_k+1, i.e. d = operatornameretr^-1_x_kx_k+1 is returned.\n\nConstructor\n\nNesterov(x0::P, γ=0.001, μ=0.9, schrinkage = k -> 0.8;\n    inverse_retraction_method=LogarithmicInverseRetraction())\n\nInitialize the Nesterov acceleration, where x0 initializes v.\n\n[ZhangSra2018]: H. Zhang, S. Sra: Towards Riemannian Accelerated Gradient Methods, Preprint, 2018, arXiv: 1806.02812\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Debug-Actions-1","page":"Gradient Descent","title":"Debug Actions","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"DebugGradient\nDebugGradientNorm\nDebugStepsize","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.DebugGradient","page":"Gradient Descent","title":"Manopt.DebugGradient","text":"DebugGradient <: DebugAction\n\ndebug for the gradient evaluated at the current iterate\n\nConstructors\n\nDebugGradient([long=false,p=print])\n\ndisplay the short (false) or long (true) default text for the gradient.\n\nDebugGradient(prefix[, p=print])\n\ndisplay the a prefix in front of the gradient.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.DebugGradientNorm","page":"Gradient Descent","title":"Manopt.DebugGradientNorm","text":"DebugGradientNorm <: DebugAction\n\ndebug for gradient evaluated at the current iterate.\n\nConstructors\n\nDebugGradientNorm([long=false,p=print])\n\ndisplay the short (false) or long (true) default text for the gradient norm.\n\nDebugGradientNorm(prefix[, p=print])\n\ndisplay the a prefix in front of the gradientnorm.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.DebugStepsize","page":"Gradient Descent","title":"Manopt.DebugStepsize","text":"DebugStepsize <: DebugAction\n\ndebug for the current step size.\n\nConstructors\n\nDebugStepsize([long=false,p=print])\n\ndisplay the short (false) or long (true) default text for the step size.\n\nDebugStepsize(prefix[, p=print])\n\ndisplay the a prefix in front of the step size.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Record-Actions-1","page":"Gradient Descent","title":"Record Actions","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"RecordGradient\nRecordGradientNorm\nRecordStepsize","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.RecordGradient","page":"Gradient Descent","title":"Manopt.RecordGradient","text":"RecordGradient <: RecordAction\n\nrecord the gradient evaluated at the current iterate\n\nConstructors\n\nRecordGradient(ξ)\n\ninitialize the RecordAction to the corresponding type of the tangent vector.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.RecordGradientNorm","page":"Gradient Descent","title":"Manopt.RecordGradientNorm","text":"RecordGradientNorm <: RecordAction\n\nrecord the norm of the current gradient\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.RecordStepsize","page":"Gradient Descent","title":"Manopt.RecordStepsize","text":"RecordStepsize <: RecordAction\n\nrecord the step size\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Solvers-1","page":"Introduction","title":"Solvers","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Solvers can be applied to Problems with solver specific Options.","category":"page"},{"location":"solvers/index.html#List-of-Algorithms-1","page":"Introduction","title":"List of Algorithms","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The following algorithms are currently available","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Solver File Problem & Option\nCyclic Proximal Point cyclic_proximal_point.jl ProximalProblem, CyclicProximalPointOptions\nChambolle-Pock Chambolle-Pock PrimalDualProblem, ChambollePockOptions\nDouglas–Rachford DouglasRachford.jl ProximalProblem, DouglasRachfordOptions\nGradient Descent gradient_descent.jl GradientProblem, GradientDescentOptions\nNelder-Mead NelderMead.jl CostProblem, NelderMeadOptions\nParticle Swarm particle_swarm.jl CostProblem, ParticleSwarmOptions\nSubgradient Method subgradient_method.jl SubGradientProblem, SubGradientMethodOptions\nSteihaug-Toint Truncated Conjugate-Gradient Method truncated_conjugate_gradient_descent.jl HessianProblem,","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"TruncatedConjugateGradientOptions The Riemannian Trust-Regions Solver | trust_regions.jl | HessianProblem, TrustRegionsOptions","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Note that the Options can also be decorated to enhance your algorithm by general additional properties.","category":"page"},{"location":"solvers/index.html#StoppingCriteria-1","page":"Introduction","title":"StoppingCriteria","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Stopping criteria are implemented as a functor, i.e. inherit from the base type","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"StoppingCriterion\nStoppingCriterionSet","category":"page"},{"location":"solvers/index.html#Manopt.StoppingCriterion","page":"Introduction","title":"Manopt.StoppingCriterion","text":"StoppingCriterion\n\nAn abstract type for the functors representing stoping criteria, i.e. they are callable structures. The naming Scheme follows functions, see for example StopAfterIteration.\n\nEvery StoppingCriterion has to provide a constructor and its function has to have the interface (p,o,i) where a Problem as well as Options and the current number of iterations are the arguments and returns a Bool whether to stop or not.\n\nBy default each StoppingCriterion should provide a fiels reason to provide details when a criteion is met (and that is empty otherwise).\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StoppingCriterionSet","page":"Introduction","title":"Manopt.StoppingCriterionSet","text":"StoppingCriterionGroup <: StoppingCriterion\n\nAn abstract type for a Stopping Criterion that itself consists of a set of Stopping criteria. In total it acts as a stopping criterion itself. Examples are StopWhenAny and StopWhenAll that can be used to combine stopping criteria.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Modules = [Manopt]\nPages = [\"plans/stopping_criterion.jl\"]\nOrder = [:type]","category":"page"},{"location":"solvers/index.html#Manopt.StopAfter","page":"Introduction","title":"Manopt.StopAfter","text":"StopAfter <: StoppingCriterion\n\nstore a threshold when to stop looking at the complete runtime. It uses time_ns() to measure the time and you provide a Period as a time limit, i.e. Minute(15)\n\nConstructor\n\nStopAfter(t)\n\ninitialize the stopping criterion to a Period t to stop after.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopAfterIteration","page":"Introduction","title":"Manopt.StopAfterIteration","text":"StopAfterIteration <: StoppingCriterion\n\nA functor for an easy stopping criterion, i.e. to stop after a maximal number of iterations.\n\nFields\n\nmaxIter – stores the maximal iteration number where to stop at\nreason – stores a reason of stopping if the stopping criterion has one be reached, see get_reason.\n\nConstructor\n\nStopAfterIteration(maxIter)\n\ninitialize the stopafterIteration functor to indicate to stop after maxIter iterations.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenAll","page":"Introduction","title":"Manopt.StopWhenAll","text":"StopWhenAll <: StoppingCriterion\n\nstore an array of StoppingCriterion elements and indicates to stop, when all indicate to stop. The reseason is given by the concatenation of all reasons.\n\nConstructor\n\nStopWhenAll(c::NTuple{N,StoppingCriterion} where N)\nStopWhenAll(c::StoppingCriterion,...)\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenAny","page":"Introduction","title":"Manopt.StopWhenAny","text":"StopWhenAny <: StoppingCriterion\n\nstore an array of StoppingCriterion elements and indicates to stop, when any single one indicates to stop. The reseason is given by the concatenation of all reasons (assuming that all non-indicating return \"\").\n\nConstructor\n\nStopWhenAny(c::NTuple{N,StoppingCriterion} where N)\nStopWhenAny(c::StoppingCriterion...)\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenChangeLess","page":"Introduction","title":"Manopt.StopWhenChangeLess","text":"StopWhenChangeLess <: StoppingCriterion\n\nstores a threshold when to stop looking at the norm of the change of the optimization variable from within a Options, i.e o.x. For the storage a StoreOptionsAction is used\n\nConstructor\n\nStopWhenChangeLess(ε[, a])\n\ninitialize the stopping criterion to a threshold ε using the StoreOptionsAction a, which is initialized to just store :x by default.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenCostLess","page":"Introduction","title":"Manopt.StopWhenCostLess","text":"StopWhenCostLess <: StoppingCriterion\n\nstore a threshold when to stop looking at the cost function of the optimization problem from within a Problem, i.e get_cost(p,o.x).\n\nConstructor\n\nStopWhenCostLess(ε)\n\ninitialize the stopping criterion to a threshold ε.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenGradientNormLess","page":"Introduction","title":"Manopt.StopWhenGradientNormLess","text":"StopWhenGradientNormLess <: StoppingCriterion\n\nstores a threshold when to stop looking at the norm of the gradient from within a GradientProblem.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"as well as the functions","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"get_reason\nget_stopping_criteria\nget_active_stopping_criteria","category":"page"},{"location":"solvers/index.html#Manopt.get_reason","page":"Introduction","title":"Manopt.get_reason","text":"get_reason(o)\n\nreturn the current reason stored within the StoppingCriterion from within the Options This reason is empty if the criterion has never been met.\n\n\n\n\n\nget_reason(c)\n\nreturn the current reason stored within a StoppingCriterion c. This reason is empty if the criterion has never been met.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.get_stopping_criteria","page":"Introduction","title":"Manopt.get_stopping_criteria","text":"get_stopping_criteria(c)\n\nreturn the array of internally stored StoppingCriterions for a StoppingCriterionSet c.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.get_active_stopping_criteria","page":"Introduction","title":"Manopt.get_active_stopping_criteria","text":"get_active_stopping_criteria(c)\n\nreturns all active stopping criteria, if any, that are within a StoppingCriterion c, and indicated a stop, i.e. their reason is nonempty. To be precise for a simple stopping criterion, this returns either an empty array if no stop is incated or the stopping criterion as the only element of an array. For a StoppingCriterionSet all internal (even nested) criteria that indicate to stop are returned.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"further stopping criteria might be available for individual Solvers.","category":"page"},{"location":"solvers/index.html#DecoratedSolvers-1","page":"Introduction","title":"Decorated Solvers","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The following decorators are available.","category":"page"},{"location":"solvers/index.html#DebugSolver-1","page":"Introduction","title":"Debug Solver","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The decorator to print debug during the iterations can be activated by decorating the Options with DebugOptions and implementing your own DebugActions. For example printing a gradient from the GradientDescentOptions is automatically available, as explained in the gradient_descent solver.","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Modules = [Manopt]\nPages   = [\"debug_solver.jl\"]","category":"page"},{"location":"solvers/index.html#Manopt.get_solver_result-Tuple{DebugOptions}","page":"Introduction","title":"Manopt.get_solver_result","text":"get_solver_result(o)\n\nReturn the final result after all iterations that is stored within the (modified during the iterations) Options o.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.initialize_solver!-Tuple{Problem,DebugOptions}","page":"Introduction","title":"Manopt.initialize_solver!","text":"initialize_solver!(p,o)\n\nInitialize the solver to the optimization Problem by initializing all values in the DebugOptionso.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.step_solver!-Tuple{Problem,DebugOptions,Any}","page":"Introduction","title":"Manopt.step_solver!","text":"step_solver!(p,o,iter)\n\nDo one iteration step (the iterth) for Problemp by modifying the values in the Optionso.options and print Debug.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.stop_solver!-Tuple{Problem,DebugOptions,Int64}","page":"Introduction","title":"Manopt.stop_solver!","text":"stop_solver!(p,o,i)\n\ndetermine whether the solver for Problem p and the DebugOptions o should stop at iteration i. If so, print all debug from :All and :Final.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#RecordSolver-1","page":"Introduction","title":"Record Solver","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The decorator to record certain values during the iterations can be activated by decorating the Options with RecordOptions and implementing your own RecordActions. For example recording the gradient from the GradientDescentOptions is automatically available, as explained in the gradient_descent solver.","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Modules = [Manopt]\nPages   = [\"record_solver.jl\"]","category":"page"},{"location":"solvers/index.html#Manopt.get_solver_result-Tuple{RecordOptions}","page":"Introduction","title":"Manopt.get_solver_result","text":"get_solver_result(o)\n\nReturn the final result after all iterations that is stored within the (modified during the iterations) Optionso.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.initialize_solver!-Tuple{Problem,RecordOptions}","page":"Introduction","title":"Manopt.initialize_solver!","text":"initialize_solver!(p,o)\n\nInitialize the solver to the optimization Problem by initializing the encapsulated options from within the RecordOptionso.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.step_solver!-Tuple{Problem,RecordOptions,Any}","page":"Introduction","title":"Manopt.step_solver!","text":"step_solver!(p,o,iter)\n\nDo one iteration step (the iterth) for Problemp by modifying the values in the Optionso.options and record the result(s).\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.stop_solver!-Tuple{Problem,RecordOptions,Int64}","page":"Introduction","title":"Manopt.stop_solver!","text":"stop_solver!(p,o,i)\n\ndetermine whether the solver for Problem p and the RecordOptions o should stop at iteration i. If so, do a (final) record to :All and :Stop.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Technical-Details-1","page":"Introduction","title":"Technical Details","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The main function a solver calls is","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"solve(p::Problem, o::Options)","category":"page"},{"location":"solvers/index.html#Manopt.solve-Tuple{Problem,Options}","page":"Introduction","title":"Manopt.solve","text":"solve(p,o)\n\nrun the solver implemented for the Problemp and the Optionso employing initialize_solver!, step_solver!, as well as the stop_solver! of the solver.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"which is a framework, that you in general should not change or redefine. It uses the following methods, which also need to be implemented on your own algorithm, if you want to provide one.","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"initialize_solver!\nstep_solver!\nget_solver_result\nstop_solver!(p::Problem, o::Options, i::Int)","category":"page"},{"location":"solvers/index.html#Manopt.initialize_solver!","page":"Introduction","title":"Manopt.initialize_solver!","text":"initialize_solver!(p,o)\n\nInitialize the solver to the optimization Problem by initializing all values in the Optionso.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.step_solver!","page":"Introduction","title":"Manopt.step_solver!","text":"step_solver!(p,o,iter)\n\nDo one iteration step (the iterth) for Problemp by modifying the values in the Options o.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.get_solver_result","page":"Introduction","title":"Manopt.get_solver_result","text":"get_solver_result(o)\n\nReturn the final result after all iterations that is stored within the (modified during the iterations) Options o.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.stop_solver!-Tuple{Problem,Options,Int64}","page":"Introduction","title":"Manopt.stop_solver!","text":"stop_solver!(p,o,i)\n\ndepending on the current Problem p, the current state of the solver stored in Options o and the current iterate i this function determines whether to stop the solver by calling the StoppingCriterion.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#adjointDifferentialFunctions-1","page":"Adjoint Differentials","title":"Adjoint Differentials","text":"","category":"section"},{"location":"functions/adjointdifferentials.html#","page":"Adjoint Differentials","title":"Adjoint Differentials","text":"Modules = [Manopt]\nPages   = [\"adjoint_differentials.jl\"]","category":"page"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_bezier_control-Union{Tuple{Q}, Tuple{ManifoldsBase.Manifold,AbstractArray{#s32,1} where #s32<:BezierSegment,Float64,Q}} where Q","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_bezier_control","text":"adjoint_differential_bezier_control(\n    M::MAnifold,\n    B::AbstractVector{<:BezierSegment},\n    t::Float64,\n    X\n)\n\nevaluate the adjoint of the differential of a composite Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in t_iin01 on the curve and given corresponding tangential vectors X = (eta_i)_i=1^n, eta_iin T_beta(t_i)mathcal M\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_bezier_control-Union{Tuple{Q}, Tuple{ManifoldsBase.Manifold,BezierSegment,AbstractArray{Float64,1},AbstractArray{Q,1}}} where Q","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_bezier_control","text":"adjoint_differential_bezier_control(\n    M::Manifold,\n    b::BezierSegment,\n    t::Array{Float64,1},\n    X::Array{Q,1}\n)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in  t_iin01 on the curve and given corresponding tangential vectors X = (eta_i)_i=1^n, eta_iin T_beta(t_i)mathcal M\n\nSee de_casteljau for more details on the curve and[BergmannGousenbourger2018].\n\n[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics, 2018. doi: 10.3389/fams.2018.00059, arXiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_bezier_control-Union{Tuple{Q}, Tuple{ManifoldsBase.Manifold,BezierSegment,Float64,Q}} where Q","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_bezier_control","text":"adjoint_differential_bezier_control(\n    M::Manifold,\n    b::BezierSegment,\n    t::Float64,\n    η::Q)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a point t in01 on the curve and a tangent vector etain T_beta(t)mathcal M.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_bezier_control-Union{Tuple{Q}, Tuple{P}, Tuple{ManifoldsBase.Manifold,AbstractArray{#s30,1} where #s30<:BezierSegment,AbstractArray{Float64,1},AbstractArray{Q,1}}} where Q where P","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_bezier_control","text":"adjoint_differential_bezier_control(M,B,t,η)\n\nevaluate the adjoint of the differential with respect to the controlpoints.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_exp_argument-Union{Tuple{mT}, Tuple{mT,Any,Any,Any}} where mT<:ManifoldsBase.Manifold","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_exp_argument","text":"adjoint_differential_exp_argument(M, p, X, Y)\n\nCompute the adjoint of D_Xexp_p XY. Note that X   T_p(T_pmathcal M) = T_pmathcal M is still a tangent vector.\n\nSee also\n\ndifferential_exp_argument, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_exp_basepoint-Union{Tuple{MT}, Tuple{MT,Any,Any,Any}} where MT<:ManifoldsBase.Manifold","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_exp_basepoint","text":"adjoint_differential_exp_basepoint(M, p, X, Y)\n\nComputes the adjoint of D_p exp_p XY.\n\nSee also\n\ndifferential_exp_basepoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{ManifoldsBase.PowerManifold{𝔽,TM,TSize,TPR},Any,Any}} where TPR where TSize where TM where 𝔽","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_forward_logs","text":"Y = adjoint_differential_forward_logs(M, p, X)\n\nCompute the adjoint differential of forward_logs F orrucirng, in the power manifold array p, the differential of the function\n\nF_i(p) = sum_j  mathcal I_i log_p_i p_j\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i Let n be the number dimensions of the PowerManifold manifold (i.e. length(size(x))). Then the input tangent vector lies on the manifold mathcal M = mathcal M^n.\n\nInput\n\nM     – a PowerManifold manifold\np     – an array of points on a manifold\nX     – a tangent vector to from the n-fold power of p, where n is the ndims of p\n\nOuput\n\nY – resulting tangent vector in T_pmathcal M representing the adjoint   differentials of the logs.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_geodesic_endpoint-Union{Tuple{MT}, Tuple{MT,Any,Any,Any,Any}} where MT<:ManifoldsBase.Manifold","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_geodesic_endpoint","text":"adjoint_differential_geodesic_endpoint(M, p, q, t, X)\n\nCompute the adjoint of D_q γ(t p q)X.\n\nSee also\n\ndifferential_geodesic_endpoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_geodesic_startpoint-Union{Tuple{MT}, Tuple{MT,Any,Any,Any,Any}} where MT<:ManifoldsBase.Manifold","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_geodesic_startpoint","text":"adjoint_differential_geodesic_startpoint(M,p, q, t, X)\n\nCompute the adjoint of D_p γ(t p q)X.\n\nSee also\n\ndifferential_geodesic_startpoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_log_argument-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_log_argument","text":"adjoint_differential_log_argument(M, p, q, X)\n\nCompute the adjoint of D_q log_p qX.\n\nSee also\n\ndifferential_log_argument, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_log_basepoint-Tuple{ManifoldsBase.Manifold,Any,Any,Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_log_basepoint","text":"adjoint_differential_log_basepoint(M, p, q, X)\n\ncomputes the adjoint of D_p log_p qX.\n\nSee also\n\ndifferential_log_basepoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"solvers/quasi_Newton.html#quasiNewton-1","page":"Quasi-Newton","title":"Riemannian quasi-Newton methods","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"    CurrentModule = Manopt","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"    quasi_Newton\n    quasi_Newton!","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.quasi_Newton","page":"Quasi-Newton","title":"Manopt.quasi_Newton","text":"quasi_Newton(M, F, ∇F, x)\n\nPerform a quasi Newton iteration for F on the manifold M starting in the point x using a retration R and a vector transport T\n\nThe kth iteration consists of\n\nCompute the search direction η_kηk = -\\mathcal{B}k [∇f (xk)]or solve\\mathcal{H}k [ηk] = -∇f (xk)]``.\nDetermine a suitable stepsize α_k along the curve gamma(α) = R_x_k(α η_k) e.g. by using WolfePowellLineseach.\nCompute x_k+1 = R_x_k(α_k η_k).\nDefine s_k = T_x_k α_k η_k(α_k η_k) and y_k = f(x_k+1) - T_x_k α_k η_k(f(x_k)).\nCompute the new approximate Hessian H_k+1 or its inverse B_k.\n\nInput\n\nM – a manifold mathcalM.\nF – a cost function F colon mathcalM to ℝ to minimize.\n∇F– the gradient F colon mathcalM to T_xmathcal M of F.\nx – an initial value x in mathcalM.\nstoppingcriterion::StoppingCriterion=StopWhenAny(       StopAfterIteration(max(1000, memorysize)), StopWhenGradientNormLess(10^(-6))   ),   return_options=false,\n\nOptional\n\nbasis                   – (DefaultOrthonormalBasis()) basis within the tangent space(s) to represent the Hessian (inverse).\ncautious_update         – (false) – whether or not to use a QuasiNewtonCautiousDirectionUpdate\ncautious_function       – ((x) -> x*10^(-4)) – a monotone increasing function that is zero at 0 and strictly increasing at 0 for the cautious update.\ndirection_update        – (InverseBFGS()) the update rule to use.\ninitial_operator        – (Matrix{Float64}(I,n,n)) initial matrix to use die the approximation, where n=manifold_dimension(M), see also scale_initial_operator.\nmemory_size             – (20) limited memory, number of s_k y_k to store. Set to a negative value to use a full memory representation\nretraction_method       – (ExponentialRetraction()) a retraction method to use, by default the exponntial map.\nscale_initial_operator  - (true) scale initial operator with fracs_ky_k_x_klVert y_krVert_x_k in the computation\nstep_size – (WolfePowellLineseach(retraction_method, vector_transport_method)) specify a Stepsize.\nstopping_criterion      - (StopWhenAny(StopAfterIteration(max(1000, memory_size)), StopWhenGradientNormLess(10^(-6))) specify a StoppingCriterion\nvector_transport_method – (ParallelTransport()) a vector transport to use, by default the parallel transport.\nreturn_options – (false) – specify whether to return just the result x (default) or the complete Options, e.g. to access recorded values. if activated, the extended result, i.e. the\n\nOutput\n\nx_opt – the resulting (approximately critical) point of the quasi–Newton method\n\nOR\n\noptions – the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/quasi_Newton.html#Manopt.quasi_Newton!","page":"Quasi-Newton","title":"Manopt.quasi_Newton!","text":"quasi_Newton!(M, F, ∇F, x; options...)\n\nPerform a quasi Newton iteration for F on the manifold M starting in the point x using a retration R and a vector transport T.\n\nInput\n\nM – a manifold mathcalM.\nF – a cost function F colon mathcalM to ℝ to minimize.\n∇F– the gradient F colon mathcalM to T_xmathcal M of F.\nx – an initial value x in mathcalM.\n\nFor all optional parameters, see quasi_Newton.\n\n\n\n\n\n","category":"function"},{"location":"solvers/quasi_Newton.html#Background-1","page":"Quasi-Newton","title":"Background","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"The aim is to minimize a real-valued function on a Riemannian manifold, i.e.","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"min f(x) quad x in mathcalM","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"Riemannian quasi-Newtonian methods are as generalizations of their Euclidean counterparts Riemannian line search methods. These methods determine a search direction η_k  T_x_k mathcalM at the current iterate x_k and a suitable stepsize α_k along gamma(α) = R_x_k(α η_k), where R colon T mathcalM to mathcalM is a retraction. The next iterate is obtained by","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"x_k+1 = R_x_k(α_k η_k)","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"In quasi-Newton methods, the search direction is given by","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"η_k = -mathcalH_k^-1 f (x_k) = -mathcalB_k f (x_k)","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"where mathcalH_k colon T_x_k mathcalM to T_x_k mathcalM is a positive definite self-adjoint operator, which approximates the action of the Hessian operatornameHess f (x_k)cdot and mathcalB_k = mathcalH_k^-1. The idea of quasi-Newton methods is instead of creating a complete new approximation of the Hessian operator operatornameHess f(x_k+1) or its inverse at every iteration, the previous operator mathcalH_k or mathcalB_k is updated by a convenient formula using the obtained information about the curvature of the objective function during the iteration. The resulting operator mathcalH_k+1 or mathcalB_k+1 acts on the tangent space T_x_k+1 mathcalM of the freshly computed iterate x_k+1. In order to get a well-defined method, the following requirements are placed on the new operator mathcalH_k+1 or mathcalB_k+1 that is created by an update. Since the Hessian operatornameHess f(x_k+1) is a self-adjoint operator on the tangent space T_x_k+1 mathcalM, and mathcalH_k+1 approximates it, we require that mathcalH_k+1 or mathcalB_k+1 is also self-adjoint on T_x_k+1 mathcalM. In order to achieve a steady descent, we want η_k to be a descent direction in each iteration. Therefore we require, that mathcalH_k+1 or mathcalB_k+1 is a positive definite operator on T_x_k+1 mathcalM. In order to get information about the cruvature of the objective function into the new operator mathcalH_k+1 or mathcalB_k+1, we require that it satisfies a form of a Riemannian quasi-Newton equation:","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"mathcalH_k+1 T_x_k rightarrow x_k+1(R_x_k^-1(x_k+1)) = f(x_k+1) - T_x_k rightarrow x_k+1(f(x_k))","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"or","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"mathcalB_k+1 f(x_k+1) - T_x_k rightarrow x_k+1(f(x_k)) = T_x_k rightarrow x_k+1(R_x_k^-1(x_k+1))","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"where T_x_k rightarrow x_k+1 colon T_x_k mathcalM to T_x_k+1 mathcalM and the chosen retraction R is the associated retraction of T. We note that, of course, not all updates in all situations will meet these conditions in every iteration. For specific quasi-Newton updates, the fulfilment of the Riemannian curvature condition, which requires that","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"g_x_k+1(s_k y_k)  0","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"holds, is a requirement for the inheritance of the self-adjointness and positive definiteness of the mathcalH_k or mathcalB_k to the operator mathcalH_k+1 or mathcalB_k+1. Unfortunately, the fulfillment of the Riemannian curvature condition is not given by a step size `alpha_k  0 that satisfies the generalised Wolfe conditions. However, in order to create a positive definite operator mathcalH_k+1 or mathcalB_k+1 in each iteration, in [HuangGallivanAbsil2015] the so-called locking condition was introduced, which requires that the isometric vector transport T^S, which is used in the update formula, and its associate retraction R fulfill","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"T^Sx xi_x(xi_x) = beta T^Rx xi_x(xi_x) quad beta = fraclVert xi_x rVert_xlVert T^Rx xi_x(xi_x) rVert_R_x(xi_x)","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"where T^R is the vector transport by differentiated retraction. With the requirement that the isometric vector transport T^S and its associated retraction R satisfies the locking condition and using the tangent vector","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"y_k = beta_k^-1 f(x_k+1) - T^Sx_k α_k η_k(f(x_k))","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"where","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"beta_k = fraclVert α_k η_k rVert_x_klVert T^Rx_k α_k η_k(α_k η_k) rVert_x_k+1","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"in the update, it can be shown that choosing a stepsize α_k  0 that satisfies the Riemannian wolfe conditions leads to the fulfilment of the Riemannian curvature condition, which in turn implies that the operator generated by the updates is positive definite. In the following we denote the specific operators in matrix notation and hence use H_k and B_k, respectively.","category":"page"},{"location":"solvers/quasi_Newton.html#Direction-Updates-1","page":"Quasi-Newton","title":"Direction Updates","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"In general there are different ways to compute a fixed AbstractQuasiNewtonUpdateRule. In general these are represented by","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"AbstractQuasiNewtonDirectionUpdate\nQuasiNewtonMatrixDirectionUpdate\nQuasiNewtonLimitedMemoryDirectionUpdate\nQuasiNewtonCautiousDirectionUpdate","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.AbstractQuasiNewtonDirectionUpdate","page":"Quasi-Newton","title":"Manopt.AbstractQuasiNewtonDirectionUpdate","text":"AbstractQuasiNewtonDirectionUpdate\n\nAn abstract represresenation of an Quasi Newton Update rule to determine the next direction given current QuasiNewtonOptions.\n\nAll subtypes should be functors, i.e. one should be able to call them as H(M,x,d) to compute a new direction update.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.QuasiNewtonMatrixDirectionUpdate","page":"Quasi-Newton","title":"Manopt.QuasiNewtonMatrixDirectionUpdate","text":"QuasiNewtonMatrixDirectionUpdate <: AbstractQuasiNewtonDirectionUpdate\n\nThese AbstractQuasiNewtonDirectionUpdates represent any quasi-Newton update rule, where the operator is stored as a matrix. A distinction is made between the update of the approximation of the Hessian, H_k mapsto H_k+1, and the update of the approximation of the Hessian inverse, B_k mapsto B_k+1. For the first case, the coordinates of the search direction η_k with respect to a basis b_i^n_i=1 are determined by solving a linear system of equations, i.e.\n\ntextSolve quad hatη_k = - H_k widehatf(x_k)\n\nwhere H_k is the matrix representing the operator with respect to the basis b_i^n_i=1 and widehatf(x_k) represents the coordinates of the gradient of the objective function f in x_k with respect to the basis b_i^n_i=1. If a method is chosen where Hessian inverse is approximated, the coordinates of the search direction η_k with respect to a basis b_i^n_i=1 are obtained simply by matrix-vector multiplication, i.e.\n\nhatη_k = - B_k widehatf(x_k)\n\nwhere B_k is the matrix representing the operator with respect to the basis b_i^n_i=1 and widehatf(x_k) as above. In the end, the search direction η_k is generated from the coordinates hateta_k and the vectors of the basis b_i^n_i=1 in both variants. The AbstractQuasiNewtonUpdateRule indicates which quasi-Newton update rule is used. In all of them, the Euclidean update formula is used to generate the matrix H_k+1 and B_k+1, and the basis b_i^n_i=1 is transported into the upcoming tangent space T_x_k+1 mathcalM, preferably with an isometric vector transport, or generated there.\n\nFields\n\nbasis – the basis.\nmatrix – the matrix which represents the approximating operator.\nscale – indicates whether the initial matrix (= identity matrix) should be scaled before the first update.\nupdate – a AbstractQuasiNewtonUpdateRule.\nvector_transport_method – an AbstractVectorTransportMethod\n\nSee also\n\nQuasiNewtonLimitedMemoryDirectionUpdate QuasiNewtonCautiousDirectionUpdate AbstractQuasiNewtonDirectionUpdate\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.QuasiNewtonLimitedMemoryDirectionUpdate","page":"Quasi-Newton","title":"Manopt.QuasiNewtonLimitedMemoryDirectionUpdate","text":"QuasiNewtonLimitedMemoryDirectionUpdate <: AbstractQuasiNewtonDirectionUpdate\n\nThis AbstractQuasiNewtonDirectionUpdate represents the limited-memory Riemanian BFGS update, where the approximating  oprator is represented by m stored pairs of tangent vectors  widetildes_i widetildey_i_i=k-m^k-1 in the k-th iteration. For the calculation of the search direction η_k, the generalisation of the two-loop recursion is used (see [HuangGallivanAbsil2015]), since it only requires inner products and linear combinations of tangent vectors in T_x_k mathcalM. For that the stored pairs of tangent vectors  widetildes_i widetildey_i_i=k-m^k-1, the gradient f(x_k) of the objective function f in x_k and the positive definite self-adjoint operator\n\nmathcalB^(0)_kcdot = fracg_x_k(s_k-1 y_k-1)g_x_k(y_k-1 y_k-1)  mathrmid_T_x_k mathcalMcdot\n\nare used. The two-loop recursion can be understood as that the InverseBFGS update is executed m times in a row on mathcalB^(0)_kcdot using the tangent vectors  widetildes_i widetildey_i_i=k-m^k-1, and in the same time the resulting operator mathcalB^LRBFGS_k cdot is directly applied on f(x_k). When updating there are two cases: if there is still free memory, i.e. k  m, the previously stored vector pairs  widetildes_i widetildey_i_i=k-m^k-1 have to be transported into the upcoming tangent space T_x_k+1 mathcalM; if there is no free memory, the oldest pair  widetildes_km widetildey_km has to be discarded and then all the remaining vector pairs  widetildes_i widetildey_i_i=k-m+1^k-1 are transported into the tangent space T_x_k+1 mathcalM. After that we calculate and store s_k = widetildes_k = T^S_x_k α_k η_k(α_k η_k) and y_k = widetildey_k. This process ensures that new information about the objective function is always included and the old, probably no longer relevant, information is discarded.\n\nFields\n\nmethod – the maximum number of vector pairs stored.\nmemory_s – the set of the stored (and transported) search directions times step size  widetildes_i_i=k-m^k-1.\nmemory_y – set of the stored gradient differences  widetildey_i_i=k-m^k-1.\nξ – a variable used in the two-loop recursion.\nρ – a variable used in the two-loop recursion.\nscale –\nvector_transport_method – a AbstractVectorTransportMethod\n\nSee also\n\nInverseBFGS QuasiNewtonCautiousDirectionUpdate AbstractQuasiNewtonDirectionUpdate\n\n[HuangGallivanAbsil2015]: Huang, Wen and Gallivan, K. A. and Absil, P.-A., A Broyden Class of Quasi-Newton Methods for Riemannian Optimization, SIAM J. Optim., 25 (2015), pp. 1660-1685. doi: 10.1137/140955483\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.QuasiNewtonCautiousDirectionUpdate","page":"Quasi-Newton","title":"Manopt.QuasiNewtonCautiousDirectionUpdate","text":"QuasiNewtonCautiousDirectionUpdate <: AbstractQuasiNewtonDirectionUpdate\n\nThese AbstractQuasiNewtonDirectionUpdates represent any quasi-Newton update rule, which are based on the idea of a so-called cautious update. The search direction is calculated as given in QuasiNewtonMatrixDirectionUpdate or [LimitedMemoryQuasiNewctionDirectionUpdate]. But the update given in QuasiNewtonMatrixDirectionUpdate or [LimitedMemoryQuasiNewctionDirectionUpdate] is only executed if\n\nfracg_x_k+1(y_ks_k)lVert s_k rVert^2_x_k+1 geq theta(lVert f(x_k) rVert_x_k)\n\nis satisfied, where theta is a monotone increasing function satisfying theta(0) = 0 and theta is strictly increasing at 0. If this is not the case, the corresponding update will be skipped, which means that for QuasiNewtonMatrixDirectionUpdate the matrix H_k or B_k is not updated, but the basis b_i^n_i=1 is nevertheless transported into the upcoming tangent space T_x_k+1 mathcalM, and for [LimitedMemoryQuasiNewctionDirectionUpdate] neither the oldest vector pair  widetildes_km widetildey_km is discarded nor the newest vector pair  widetildes_k widetildey_k is added into storage, but all stored vector pairs  widetildes_i widetildey_i_i=k-m^k-1 are transported into the tangent space T_x_k+1 mathcalM. If InverseBFGS or InverseBFGS is chosen as update, then the resulting method follows the method of [HuangAbsilGallivan2018], taking into account that the corresponding step size is chosen.\n\nFields\n\nupdate – an AbstractQuasiNewtonDirectionUpdate\nθ – a monotone increasing function satisfying θ(0) = 0 and θ is strictly increasing at 0.\n\nSee also\n\nQuasiNewtonMatrixDirectionUpdate QuasiNewtonLimitedMemoryDirectionUpdate\n\n[HuangAbsilGallivan2018]: Huang, Wen and Absil, P.-A and Gallivan, Kyle, A Riemannian BFGS Method Without Differentiated Retraction for Nonconvex Optimization Problems, SIAM J. Optim., 28 (2018), pp. 470-495. doi: 10.1137/17M1127582\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Hessian-Update-Rules-1","page":"Quasi-Newton","title":"Hessian Update Rules","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"Using","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"update_hessian!","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.update_hessian!","page":"Quasi-Newton","title":"Manopt.update_hessian!","text":"update_hessian!(d, p, o, x_old, iter)\n\nupdate the hessian wihtin the QuasiNewtonOptions o given a Problem p as well as the an AbstractQuasiNewtonDirectionUpdate d and the last iterate x_old. Note that the current (iterth) iterate is already stored in o.x.\n\nSee also AbstractQuasiNewtonUpdateRule for the different rules that are available within d.\n\n\n\n\n\n","category":"function"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"the following update formulae for either H_k+1 or B_k+1 are available.","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"AbstractQuasiNewtonUpdateRule\nBFGS\nDFP\nBroyden\nSR1\nInverseBFGS\nInverseDFP\nInverseBroyden\nInverseSR1","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.AbstractQuasiNewtonUpdateRule","page":"Quasi-Newton","title":"Manopt.AbstractQuasiNewtonUpdateRule","text":"AbstractQuasiNewtonUpdateRule\n\nSpecify a type for the different AbstractQuasiNewtonDirectionUpdates.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.BFGS","page":"Quasi-Newton","title":"Manopt.BFGS","text":"BFGS <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the Riemanian BFGS update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeH_k^mathrmBFGS the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nH^mathrmBFGS_k+1 = widetildeH^mathrmBFGS_k  + fracy_k y^mathrmT_k s^mathrmT_k y_k - fracwidetildeH^mathrmBFGS_k s_k s^mathrmT_k widetildeH^mathrmBFGS_k s^mathrmT_k widetildeH^mathrmBFGS_k s_k\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\nf(x_k+1) - T^S_x_k α_k η_k( f(x_k)) in T_x_k+1 mathcalM\n\nrespectively.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.DFP","page":"Quasi-Newton","title":"Manopt.DFP","text":"DFP <: AbstractQuasiNewtonUpdateRule\n\nindicates in an AbstractQuasiNewtonDirectionUpdate that the Riemanian DFP update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeH_k^mathrmDFP the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nH^mathrmDFP_k+1 = Bigl(\n  mathrmid_T_x_k+1 mathcalM - fracy_k s^mathrmT_ks^mathrmT_k y_k\nBigr)\nwidetildeH^mathrmDFP_k\nBigl(\n  mathrmid_T_x_k+1 mathcalM - fracs_k y^mathrmT_ks^mathrmT_k y_k\nBigr) + fracy_k y^mathrmT_ks^mathrmT_k y_k\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\nf(x_k+1) - T^S_x_k α_k η_k( f(x_k)) in T_x_k+1 mathcalM\n\nrespectively.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.Broyden","page":"Quasi-Newton","title":"Manopt.Broyden","text":"Broyden <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the Riemanian Broyden update is used in the Riemannian quasi-Newton method, which is as a convex combination of BFGS and DFP.\n\nWe denote by widetildeH_k^mathrmBr the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nH^mathrmBr_k+1 = widetildeH^mathrmBr_k\n  - fracwidetildeH^mathrmBr_k s_k s^mathrmT_k widetildeH^mathrmBr_ks^mathrmT_k widetildeH^mathrmBr_k s_k + fracy_k y^mathrmT_ks^mathrmT_k y_k\n  + φ_k s^mathrmT_k widetildeH^mathrmBr_k s_k\n  Bigl(\n        fracy_ks^mathrmT_k y_k - fracwidetildeH^mathrmBr_k s_ks^mathrmT_k widetildeH^mathrmBr_k s_k\n  Bigr)\n  Bigl(\n        fracy_ks^mathrmT_k y_k - fracwidetildeH^mathrmBr_k s_ks^mathrmT_k widetildeH^mathrmBr_k s_k\n  Bigr)^mathrmT\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\nf(x_k+1) - T^S_x_k α_k η_k( f(x_k)) in T_x_k+1 mathcalM\n\nrespectively, and φ_k is the Broyden factor which is :constant by default but can also be set to :Davidon.\n\nConstructor\n\nBroyden(φ, update_rule::Symbol = :constant)\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.SR1","page":"Quasi-Newton","title":"Manopt.SR1","text":"SR1 <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the Riemanian SR1 update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeH_k^mathrmSR1 the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nH^mathrmSR1_k+1 = widetildeH^mathrmSR1_k\n+ frac\n  (y_k - widetildeH^mathrmSR1_k s_k) (y_k - widetildeH^mathrmSR1_k s_k)^mathrmT\n\n(y_k - widetildeH^mathrmSR1_k s_k)^mathrmT s_k\n\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\nf(x_k+1) - T^S_x_k α_k η_k( f(x_k)) in T_x_k+1 mathcalM\n\nrespectively.\n\nThis method can be stabilized by only performing the update if denominator is larger than rlVert s_krVert_x_k+1lVert y_k - widetildeH^mathrmSR1_k s_k rVert_x_k+1 for some r0. For more details, see Section 6.2 in [NocedalWright2006]\n\n[NocedalWright2006]: Nocedal, J., Wright, S.: Numerical Optimization, Second Edition, Springer, 2006. doi: 10.1007/978-0-387-40065-5\n\nConstructor\n\nSR1(r::Float64=-1.0)\n\nGenerate the SR1 update, which by default does not include the check (since the default sets t0`)\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.InverseBFGS","page":"Quasi-Newton","title":"Manopt.InverseBFGS","text":"InverseBFGS <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the inverse Riemanian BFGS update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeB_k^mathrmBFGS the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nB^mathrmBFGS_k+1  = Bigl(\n  mathrmid_T_x_k+1 mathcalM - fracs_k y^mathrmT_k s^mathrmT_k y_k\nBigr)\nwidetildeB^mathrmBFGS_k\nBigl(\n  mathrmid_T_x_k+1 mathcalM - fracy_k s^mathrmT_k s^mathrmT_k y_k\nBigr) + fracs_k s^mathrmT_ks^mathrmT_k y_k\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\nf(x_k+1) - T^S_x_k α_k η_k( f(x_k)) in T_x_k+1 mathcalM\n\nrespectively.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.InverseDFP","page":"Quasi-Newton","title":"Manopt.InverseDFP","text":"InverseDFP <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the inverse Riemanian DFP update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeB_k^mathrmDFP the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nB^mathrmDFP_k+1 = widetildeB^mathrmDFP_k\n+ fracs_k s^mathrmT_ks^mathrmT_k y_k\n- fracwidetildeB^mathrmDFP_k y_k y^mathrmT_k widetildeB^mathrmDFP_ky^mathrmT_k widetildeB^mathrmDFP_k y_k\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\nf(x_k+1) - T^S_x_k α_k η_k( f(x_k)) in T_x_k+1 mathcalM\n\nrespectively.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.InverseBroyden","page":"Quasi-Newton","title":"Manopt.InverseBroyden","text":"InverseBroyden <: AbstractQuasiNewtonUpdateRule\n\nIndicates in AbstractQuasiNewtonDirectionUpdate that the Riemanian Broyden update is used in the Riemannian quasi-Newton method, which is as a convex combination of InverseBFGS and InverseDFP.\n\nWe denote by widetildeH_k^mathrmBr the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nB^mathrmBr_k+1 = widetildeB^mathrmBr_k\n - fracwidetildeB^mathrmBr_k y_k y^mathrmT_k widetildeB^mathrmBr_ky^mathrmT_k widetildeB^mathrmBr_k y_k\n   + fracs_k s^mathrmT_ks^mathrmT_k y_k\n + φ_k y^mathrmT_k widetildeB^mathrmBr_k y_k\n Bigl(\n     fracs_ks^mathrmT_k y_k - fracwidetildeB^mathrmBr_k y_ky^mathrmT_k widetildeB^mathrmBr_k y_k\n    Bigr) Bigl(\n        fracs_ks^mathrmT_k y_k - fracwidetildeB^mathrmBr_k y_ky^mathrmT_k widetildeB^mathrmBr_k y_k\n Bigr)^mathrmT\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\nf(x_k+1) - T^S_x_k α_k η_k( f(x_k)) in T_x_k+1 mathcalM\n\nrespectively, and φ_k is the Broyden factor which is :constant by default but can also be set to :Davidon.\n\nConstructor\n\nInverseBroyden(φ, update_rule::Symbol = :constant)\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.InverseSR1","page":"Quasi-Newton","title":"Manopt.InverseSR1","text":"InverseSR1 <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the inverse Riemanian SR1 update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeB_k^mathrmSR1 the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nB^mathrmSR1_k+1 = widetildeB^mathrmSR1_k\n+ frac\n  (s_k - widetildeB^mathrmSR1_k y_k) (s_k - widetildeB^mathrmSR1_k y_k)^mathrmT\n\n  (s_k - widetildeB^mathrmSR1_k y_k)^mathrmT y_k\n\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\nf(x_k+1) - T^S_x_k α_k η_k( f(x_k)) in T_x_k+1 mathcalM\n\nrespectively.\n\nThis method can be stabilized by only performing the update if denominator is larger than rlVert y_krVert_x_k+1lVert s_k - widetildeH^mathrmSR1_k y_k rVert_x_k+1 for some r0. For more details, see Section 6.2 in [NocedalWright2006].\n\nConstructor\n\nInverseSR1(r::Float64=-1.0)\n\nGenerate the InverseSR1 update, which by default does not include the check, since the default sets t0`.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Options-1","page":"Quasi-Newton","title":"Options","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"The quasi Newton algorithm is based on a GradientProblem.","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"QuasiNewtonOptions","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.QuasiNewtonOptions","page":"Quasi-Newton","title":"Manopt.QuasiNewtonOptions","text":"QuasiNewtonOptions <: Options\n\nThese Quasi Newton Options represent any quasi-Newton based method and can be used with any update rule for the direction.\n\nFields\n\nx – the current iterate, a point on a manifold\n∇ – the current gradient\nsk – the current step\nyk the current gradient difference\ndirection_update - an AbstractQuasiNewtonDirectionUpdate rule.\nretraction_method – an AbstractRetractionMethod\nstop – a StoppingCriterion\n\nSee also\n\nGradientProblem\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Literature-1","page":"Quasi-Newton","title":"Literature","text":"","category":"section"},{"location":"list.html#Table-of-Contents,-Types-and-Functions-1","page":"Function Index","title":"Table of Contents, Types and Functions","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"This page lists all pages of this documentations, all available types and functions.","category":"page"},{"location":"list.html#Complete-List-of-Contents-1","page":"Function Index","title":"Complete List of Contents","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Depth = 3","category":"page"},{"location":"list.html#Available-Types-1","page":"Function Index","title":"Available Types","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Modules = [Manopt]\nOrder   = [:type]","category":"page"},{"location":"list.html#Solver-Functions-1","page":"Function Index","title":"Solver Functions","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Modules = [Manopt]\nPages = [\"plans/index.md\", \"solvers/index.md\"]","category":"page"},{"location":"list.html#Functions-1","page":"Function Index","title":"Functions","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Modules = [Manopt]\nPages = [\"functions/adjointDifferentials.md\", \"functions/costs.md\", \"functions/differentials.md\", \"functions/gradients.md\", \"functions/jacobiFields.md\", \"functions/proximalMaps.md\"]","category":"page"},{"location":"solvers/trust_regions.html#trust_regions-1","page":"Trust-Regions Solver","title":"The Riemannian Trust-Regions Solver","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"The aim is to solve an optimization problem on a manifold","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"operatorname*min_x    mathcalM F(x)","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"by using the Riemannian trust-regions solver. It is number one choice for smooth optimization. This trust-region method uses the Steihaug-Toint truncated conjugate-gradient method truncated_conjugate_gradient_descent to solve the inner minimization problem called the trust-regions subproblem. This inner solve can be preconditioned by providing a preconditioner (symmetric and positive deﬁnite, an approximation of the inverse of the Hessian of F). If no Hessian of the cost function F is provided, a standard approximation of the Hessian based on the gradient nabla F with approxHessianFD will be computed.","category":"page"},{"location":"solvers/trust_regions.html#Initialization-1","page":"Trust-Regions Solver","title":"Initialization","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"Initialize x_0 = x with an initial point x on the manifold. It can be given by the caller or set randomly. Set the initial trust-region radius Delta =frac18 barDelta where barDelta is the maximum radius the trust-region can have. Usually one uses the root of the manifold dimension operatornamedim(mathcalM). For accepting the next iterate and evaluating the new trust-region radius one needs an accept/reject threshold rho    0frac14), which is rho = 01 on default. Set k=0.","category":"page"},{"location":"solvers/trust_regions.html#Iteration-1","page":"Trust-Regions Solver","title":"Iteration","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"Repeat until a convergence criterion is reached","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"Set eta as a random tangent vector if using randomized approach. Else  set eta as the zero vector in the tangential space T_x_kmathcalM.\nSet eta^*  as the solution of the trust-region subproblem, computed by  the tcg-method with eta as initial vector.\nIf using randomized approach compare eta^*  with the Cauchy point  eta_c^*  = -tau_c fracDeltaoperatornamenorm(operatornameGradf (x_k)) operatornameGradF (x_k) by the model function m_x_k(cdot). If the  model decrease is larger by using the Cauchy point, set  eta^*  = eta_c^* .\nSet x^*  = operatornameRetr_x_k(eta^* ).\nSet rho = fracF(x_k)-F(x^* )m_x_k(eta)-m_x_k(eta^* ), where  m_x_k(cdot) describes the quadratic model function.\nUpdate the trust-region radius:  Delta = begincases frac14 Delta  rho  frac14   textor  m_x_k(eta)-m_x_k(eta^* ) leq 0  textor    rho = pm   fty   operatornamemin(2 Delta barDelta)   rho  frac34  textand the tcg-method stopped because of negative  curvature or exceeding the trust-region  Delta   textotherwise  endcases\nIf m_x_k(eta)-m_x_k(eta^* ) geq 0 and rho  rho set  x_k = x^* .\nSet k = k+1.","category":"page"},{"location":"solvers/trust_regions.html#Result-1","page":"Trust-Regions Solver","title":"Result","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"The result is given by the last computed x_k.","category":"page"},{"location":"solvers/trust_regions.html#Remarks-1","page":"Trust-Regions Solver","title":"Remarks","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To the Initialization: A random point on the manifold.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 1: Using randomized approach means using a random tangent vector as initial vector for the approximal solve of the trust-regions subproblem. If this is the case, keep in mind that the vector must be in the trust-region radius. This is achieved by multiplying η by sqrt(4,eps(Float64)) as long as its norm is greater than the current trust-region radius Delta. For not using randomized approach, one can get the zero tangent vector.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 2: Obtain eta^*  by (approximately) solving the trust-regions subproblem","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"operatorname*argmin_eta    T_x_kmathcalM m_x_k(eta) = F(x_k) +\nlangle nabla F(x_k) eta rangle_x_k + frac12 langle\noperatornameHessF(eta)_ x_k eta rangle_x_k","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"textst  langle eta eta rangle_x_k leq Delta^2","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"with the Steihaug-Toint truncated conjugate-gradient (tcg) method. The problem as well as the solution method is described in the truncated_conjugate_gradient_descent.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 3: If using a random tangent vector as an initial vector, compare the result of the tcg-method with the Cauchy point. Convergence proofs assume that one achieves at least (a fraction of) the reduction of the Cauchy point. The idea is to go in the direction of the gradient to an optimal point. This can be on the edge, but also before. The parameter tau_c for the optimal length is defined by","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"tau_c = begincases 1  langle operatornameGradF (x_k) \noperatornameHessF (eta_k)_ x_krangle_x_k leq 0  \noperatornamemin(fracoperatornamenorm(operatornameGradF (x_k))^3\nDelta langle operatornameGradF (x_k) \noperatornameHessF (eta_k)_ x_krangle_x_k 1)   textotherwise\nendcases","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To check the model decrease one compares m_x_k(eta_c^* ) = F(x_k) + langle eta_c^*  operatornameGradF (x_k)rangle_x_k + frac12langle eta_c^*  operatornameHessF (eta_c^* )_ x_krangle_x_k with m_x_k(eta^* ) = F(x_k) + langle eta^*  operatornameGradF (x_k)rangle_x_k + frac12langle eta^*  operatornameHessF (eta^* )_ x_krangle_x_k. If m_x_k(eta_c^* )  m_x_k(eta^* ) then is m_x_k(eta_c^* ) the better choice.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 4: operatornameRetr_x_k(cdot) denotes the retraction, a mapping operatornameRetr_x_kT_x_kmathcalM rightarrow mathcalM wich approximates the exponential map. In some cases it is cheaper to use this instead of the exponential.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 6: One knows that the truncated_conjugate_gradient_descent algorithm stopped for these reasons when the stopping criteria StopWhenCurvatureIsNegative, StopWhenTrustRegionIsExceeded are activated.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 7: The last step is to decide if the new point x^*  is accepted.","category":"page"},{"location":"solvers/trust_regions.html#Interface-1","page":"Trust-Regions Solver","title":"Interface","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"trust_regions\ntrust_regions!","category":"page"},{"location":"solvers/trust_regions.html#Manopt.trust_regions","page":"Trust-Regions Solver","title":"Manopt.trust_regions","text":"trust_regions(M, F, ∇F, x, H)\n\nevaluate the Riemannian trust-regions solver for optimization on manifolds. It will attempt to minimize the cost function F on the Manifold M. If no Hessian H is provided, a standard approximation of the Hessian based on the gradient ∇F will be computed. For solving the the inner trust-region subproblem of finding an update-vector, it uses the Steihaug-Toint truncated conjugate-gradient method. For a description of the algorithm and more details see\n\nP.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: 10.1007/s10208-005-0179-9\nA. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: 10.1137/1.9780898719857\n\nInput\n\nM – a manifold mathcal M\nF – a cost function F colon mathcal M to mathbb R to minimize\n∇F- the gradient nabla F colon mathcal M to T mathcal M of F\nx – an initial value x    mathcal M\nH – the hessian H( mathcal M x xi) of F\n\nOptional\n\nretraction – approximation of the exponential map\npreconditioner – a preconditioner (a symmetric, positive definite operator that should approximate the inverse of the Hessian)\nstopping_criterion – (StopWhenAny(StopAfterIteration(1000), StopWhenGradientNormLess(10^(-6))) a functor inheriting from StoppingCriterion indicating when to stop.\nΔ_bar – the maximum trust-region radius\nΔ - the (initial) trust-region radius\nuseRandom – set to true if the trust-region solve is to be initiated with a random tangent vector. If set to true, no preconditioner will be used. This option is set to true in some scenarios to escape saddle points, but is otherwise seldom activated.\nρ_prime – Accept/reject threshold: if ρ (the performance ratio for the iterate) is at least ρ', the outer iteration is accepted. Otherwise, it is rejected. In case it is rejected, the trust-region radius will have been decreased. To ensure this, ρ' >= 0 must be strictly smaller than 1/4. If ρ_prime is negative, the algorithm is not guaranteed to produce monotonically decreasing cost values. It is strongly recommended to set ρ' > 0, to aid convergence.\nρ_regularization – Close to convergence, evaluating the performance ratio ρ is numerically challenging. Meanwhile, close to convergence, the quadratic model should be a good fit and the steps should be accepted. Regularization lets ρ go to 1 as the model decrease and the actual decrease go to zero. Set this option to zero to disable regularization (not recommended). When this is not zero, it may happen that the iterates produced are not monotonically improving the cost when very close to convergence. This is because the corrected cost improvement could change sign if it is negative but very small.\nreturn_options – (false) – if actiavated, the extended result, i.e. the complete Options are returned. This can be used to access recorded values. If set to false (default) just the optimal value x_opt is returned\n\nOutput\n\nx – the last reached point on the manifold\n\nsee also\n\ntruncated_conjugate_gradient_descent\n\n\n\n\n\n","category":"function"},{"location":"solvers/trust_regions.html#Manopt.trust_regions!","page":"Trust-Regions Solver","title":"Manopt.trust_regions!","text":"trust_regions!(M, F, ∇F, x, H; kwargs...)\n\nevaluate the Riemannian trust-regions solver for optimization on manifolds in place of x.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function F colon mathcal M to mathbb R to minimize\n∇F- the gradient nabla F colon mathcal M to T mathcal M of F\nx – an initial value x    mathcal M\nH – the hessian H( mathcal M x xi) of F\n\nfor more details and all options, see trust_regions\n\n\n\n\n\n","category":"function"},{"location":"solvers/trust_regions.html#Options-1","page":"Trust-Regions Solver","title":"Options","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"TrustRegionsOptions","category":"page"},{"location":"solvers/trust_regions.html#Manopt.TrustRegionsOptions","page":"Trust-Regions Solver","title":"Manopt.TrustRegionsOptions","text":"TrustRegionsOptions <: HessianOptions\n\ndescribe the trust-regions solver, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx : a point as starting point\nstop : a function s,r = @(o,iter) returning a stop   indicator and a reason based on an iteration number and the gradient\nΔ : the (initial) trust-region radius\nΔ_bar : the maximum trust-region radius\nuseRand : indicates if the trust-region solve is to be initiated with a       random tangent vector. If set to true, no preconditioner will be       used. This option is set to true in some scenarios to escape saddle       points, but is otherwise seldom activated.\nρ_prime : a lower bound of the performance ratio for the iterate that       decides if the iteration will be accepted or not. If not, the       trust-region radius will have been decreased. To ensure this,       ρ'>= 0 must be strictly smaller than 1/4. If ρ' is negative,       the algorithm is not guaranteed to produce monotonically decreasing       cost values. It is strongly recommended to set ρ' > 0, to aid       convergence.\nρ_regularization : Close to convergence, evaluating the performance ratio ρ       is numerically challenging. Meanwhile, close to convergence, the       quadratic model should be a good fit and the steps should be       accepted. Regularization lets ρ go to 1 as the model decrease and       the actual decrease go to zero. Set this option to zero to disable       regularization (not recommended). When this is not zero, it may happen       that the iterates produced are not monotonically improving the cost       when very close to convergence. This is because the corrected cost       improvement could change sign if it is negative but very small.\n\nConstructor\n\nTrustRegionsOptions(x, stop, delta, delta_bar, uR, rho_prime, rho_reg)\n\nconstruct a trust-regions Option with the fields as above.\n\nSee also\n\ntrust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/trust_regions.html#Approximation-of-the-Hessian-1","page":"Trust-Regions Solver","title":"Approximation of the Hessian","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"approxHessianFD","category":"page"},{"location":"solvers/trust_regions.html#Manopt.approxHessianFD","page":"Trust-Regions Solver","title":"Manopt.approxHessianFD","text":"approxHessianFD(p,x,ξ,[stepsize=2.0^(-14)])\n\nreturn an approximated solution of the Hessian of the cost function applied to a tangent vector ξ by using a generic finite difference approximation based on computations of the gradient.\n\nInput\n\np – a Manopt problem structure (already containing the manifold and enough       information to compute the cost gradient)\nx – a point where the Hessian is ​​to be approximated\nξ – a tangent vector on which the approximated Hessian is ​​to be applied\n\nOptional\n\nstepsize – the length of the step with which the method should work\n\nOutput\n\na tangent vector generated by applying the approximated Hessian to the   tangent vector ξ\n\n\n\n\n\n","category":"function"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/BezierCurves.jl\"","category":"page"},{"location":"tutorials/BezierCurves.html#BezierCurvesTutorial-1","page":"work with Bézier curves","title":"Bezier curves and their acceleration","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"This tutorial illustrates how Bézier curves are generalized to manifolds and how to minimize their acceleration, i.e. how to get a curve that is as straight or as geodesic while fulfilling constraints","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"This example also illustrates how to apply the minimization on the corresponding PowerManifold manifold using a gradient_descent with ArmijoLinesearch.","category":"page"},{"location":"tutorials/BezierCurves.html#Table-of-contents-1","page":"work with Bézier curves","title":"Table of contents","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"Setup\nde Casteljau algorithm on manifolds\nComposite Bézire curves\nMinimizing the acceleration of a Bézier curve\nLiterature","category":"page"},{"location":"tutorials/BezierCurves.html#SetupTB-1","page":"work with Bézier curves","title":"Setup","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"We first initialize the necessary packages","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"using Manopt, Manifolds","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"using Colors\nblack = RGBA{Float64}(colorant\"#000000\")\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\")\nTolVibrantMagenta = RGBA{Float64}(colorant\"#EE3377\")\nTolVibrantCyan = RGBA{Float64}(colorant\"#33BBEE\")\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\")\ngeo_pts = collect(range(0.0, 1.0; length=101)) #hide\nbezier_pts = collect(range(0.0, 3.0; length=201)) #hide\ncamera_position = (-1.0, -0.7, 0.3) #hide\nnothing #hide","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"Then we load our data, see artificial_S2_composite_bezier_curve, a composite Bezier curve consisting of 3 segments on the Sphere","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"B = artificial_S2_composite_bezier_curve();\nnothing #hide","category":"page"},{"location":"tutorials/BezierCurves.html#Casteljau-1","page":"work with Bézier curves","title":"De Casteljau algorithm on manifolds","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"This curve can be evaluated using de Casteljau's algorithm[Casteljau1959][Casteljau1963] named after Paul de Casteljau(*1930). To simplify the idea and understand this algorithm, we will first only look at the points of the first segment","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"M = Sphere(2)\nb = B[2].pts","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"On Euclidean spaces Bézier curves of these n=4 so called control points like this segment yield polynomials of degree 3. The resulting curve gamma 01 to mathbb R^m is called Bezier curve or Bézier spline and is named after Piérre Bezier (1910–1999). They can be evaluated by the de Casteljau algorithm by evaluating line segments between points. While it is not easy to evaluate polynomials on a manifold, evaluating line segments generalizes to the evaluation of shortest_geodesics","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"We will illustrate this using these points b=(b_1b_2b_3b_4) on the Sphere mathbb S^2. Let's evaliuate this at the point t=frac14in 01. We first compute","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"t = 0.66\npts1 = shortest_geodesic.(Ref(M), b[1:3], b[2:4], Ref(t))","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"We obtain 3 points on the geodesics connecting the control points. Repeating this again twice","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"pts2 = shortest_geodesic.(Ref(M), pts1[1:2], pts1[2:3], Ref(t))\np = shortest_geodesic(M, pts2[1], pts2[2], t)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"we obtain the point on the Bézier curve c(t). This procedure is illustrated in the following image:","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Illustration of de Casteljau's algorithm on the Sphere.)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"From the control points (blue) and their geodesics, ont evaluation per geodesic yields three interims points (cyan), their two successive geodeics another two points (teal) and at its geodesic at t=066 we obtain the point on the curve.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"In Manopt.jl, to evaluate a Bézier curve knowing its BezierSegment, use de_casteljau.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"There are a few nice observations to make, that hold also for these Bézier curves on manifolds:","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"The curve starts in the first controlpoint b_0 and ends in the last controlpoint b_3\nThe tangent vector to the curve at the start dot c(0) is equal to log_b_0b_1 = dotgamma_b_0b_0(0), where gamma_ab denotes the shortest geodesic.\nThe tangent vector to the curve at the end dot c(1) is equal to -log_b_3b_2 = -dotgamma_b_3b_2(0) = dotgamma_b_2b_3(1).\nthe curve is differentiable.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"For more details on these properties, see for example [PopielNoakes2007].","category":"page"},{"location":"tutorials/BezierCurves.html#CompositeBezier-1","page":"work with Bézier curves","title":"Composite Bézier curves","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"With the properties of a single Bézier curve, also called Bézier segment, we can “stitch” curves together. Let a_0ldotsa_n and b_0ldotsb_m be two sets of controlpoints for the Bézier segments c(t) and d(t), respectively. We define the composite Bézier curve by B(t) = begincases c(t)  text if  0leq t  1  d(t-1)  text if  1leq t leq 2endcases where tin02. This can of course be generalised straight forward to more than two cases. With the properties from the previous section we can now state that","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"the curve B(t) is continuous if c(1)=d(0) or in other words a_n=b_0\nthe curve B(t) is differentiable if additionally dot c(1)=dot d(0) or in other words -log_a_na_n-1 = log_b_0b_1. This is equivalent to a_n=b_0 = gamma_a_n-1b_1(tfrac12).","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"One nice interpretation of the last characterization is, that the tangents log_a_na_n-1 and log_b_0b_1 point into opposite directions. For a continuous curve, the first point of every segment (except for the first segment) can be ommitted, for a differentiable curve the first two points (except for the first segment) can be ommitted. You can reduce storage by calling get_bezier_points, though for econstruciton with get_bezier_segments you also need get_bezier_degrees. The reduced storage is represented as an array of points, i.e. an element of the corresponding PowerManifold.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"For the three segment example from the beginning this looks as follows[1]","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Illustration of a differentiable composite Bézier curve with 3 segments.)","category":"page"},{"location":"tutorials/BezierCurves.html#MinAccBezier-1","page":"work with Bézier curves","title":"Minimizing the acceleration of a composite Bézier curve","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"The motivation to minimize the acceleration of the composite Bézier curve is, that the curve should get “straighter” or more geodesic like. If we discretize the curve B(t) with its control points denoted by b_ij for the jth note in the ith segment, the discretized model for equispaced t_i, i=0ldotsN in the domain of B reads[BergmannGousenbourger2018]","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"A(b) coloneqqsum_i=1^N-1fracmathrmd^2_2 bigl B(t_i-1) B(t_i) B(t_i+1) bigrDelta_t^3","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"where mathrmd_2 denotes the second order finite difference using the mid point approach, see costTV2[BacakBergmannSteidlWeinmann2016],","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"d_2(xyz) = min_c  mathcal C_xz d_mathcal M(cy)qquad xyzmathcal M","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"Another model is based on logarithmic maps, see [BoumalAbsil2011], but that is not considered here. An advantage of the model considered here is, that it only consist of the evaluation of geodesics. This yields a gradient of A(b) with respect to b adjoint_Jacobi_fields. The following image shows the negative gradient (scaled)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Illustration of the gradient of the acceleration with respect to the control points.)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"In the following we consider two cases: Interpolation, which fixes the junction and end points of B(t) and approximation, where a weight and a dataterm are additionally introduced.","category":"page"},{"location":"tutorials/BezierCurves.html#Interpolation-1","page":"work with Bézier curves","title":"Interpolation","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"For interpolation, the junction points are fixed and their gradient entries are hence set to zero. After transferring to the already mentioned PowerManifold, we can then perform a gradient_descent as follows","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"curve_samples = collect(range(0.0, 3.0; length=151)) #exactness of approximating d^2\npB = get_bezier_points(M, B, :differentiable)\nN = PowerManifold(M, NestedPowerRepresentation(), length(pB))\nF(pB) = cost_acceleration_bezier(M, pB, get_bezier_degrees(M, B), curve_samples)\n∇F(pB) = ∇acceleration_bezier(M, pB, get_bezier_degrees(M, B), curve_samples)\nx0 = pB\npB_opt_ip = gradient_descent(\n    N,\n    F,\n    ∇F,\n    x0;\n    stepsize=ArmijoLinesearch(1.0, ExponentialRetraction(), 0.5, 0.0001),\n    stopping_criterion=StopWhenChangeLess(5 * 10.0^(-7)),\n    debug=[\n        :Iteration,\n        \" | \",\n        :Cost,\n        \" | \",\n        DebugGradientNorm(),\n        \" | \",\n        DebugStepsize(),\n        \" | \",\n        :Change,\n        \"\\n\",\n        :Stop,\n        10,\n    ],\n)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"and the result looks like","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Interpolation Min Acc)","category":"page"},{"location":"tutorials/BezierCurves.html#Approximation-1","page":"work with Bézier curves","title":"Approximation","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"Similarly if we introduce the junction points as data fixed given d_i and set (for simplicity) p_i=b_i0 and p_n+1=b_n4 and set λ=3 in","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"fraclambda2sum_k=0^3 d_mathcal M(d_ip_i)^2 + A(b)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"then λ models how important closeness to the data d_i is.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"λ = 3.0\nd = get_bezier_junctions(M, B)\nF(pB) = cost_L2_acceleration_bezier(M, pB, get_bezier_degrees(M, B), curve_samples, λ, d)\n∇F(pB) = ∇L2_acceleration_bezier(M, pB, get_bezier_degrees(M, B), curve_samples, λ, d)\nx0 = pB\npB_opt_appr = gradient_descent(\n    N,\n    F,\n    ∇F,\n    x0;\n    stepsize=ArmijoLinesearch(1.0, ExponentialRetraction(), 0.5, 0.001),\n    stopping_criterion=StopWhenChangeLess(10.0^(-5)),\n    debug=[\n        :Iteration,\n        \" | \",\n        :Cost,\n        \" | \",\n        DebugGradientNorm(),\n        \" | \",\n        DebugStepsize(),\n        \" | \",\n        :Change,\n        \"\\n\",\n        :Stop,\n        50,\n    ],\n)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"and the result looks like","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Approximation min Acc)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"The role of lambda can be interpreted as follows: for large values of lambda, the minimizer, i.e. the resulting curve, is closer to the original Bézier junction points. For small lambda the resting curve is closer to a geodesic and the control points are closer to the curve. For lambda=0 any (not necessarily shortest) geodesic is a solution and the problem is ill-posed. To illustrate the effect of lambda, the following image contains 1000 runs for lambda=10 in dark currant to lambda=001 in bright yellow.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Approximation min Acc)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"The effect of the data term can also be seen in the following video, which starts a little slow and takes about 40 seconds.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Video of the effect of lambda, the weight of the dataterm)","category":"page"},{"location":"tutorials/BezierCurves.html#LiteratureBT-1","page":"work with Bézier curves","title":"Literature","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[BacakBergmannSteidlWeinmann2016]: Bačák, M., Bergmann, R., Steidl, G. and Weinmann, A.: A second order nonsmooth variational model for restoring manifold-valued images, SIAM Journal on Scientific Computations, Volume 38, Number 1, pp. A567–597, doi: 10.1137/15M101988X, arXiv: 1506.02409","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics, 2018. doi: 10.3389/fams.2018.00059, arXiv: 1807.10090","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[BoumalAbsil2011]: Boumal, N. and Absil, P.-A.: A discrete regression method on manifolds and its application to data on SO(n). In: IFAC Proceedings Volumes (IFAC-PapersOnline). Vol. 18. Milano (2011). p. 2284–89. doi: 10.3182/20110828-6-IT-1002.00542, web: www","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[Casteljau1959]: de Casteljau, P.: Outillage methodes calcul, Enveloppe Soleau 40.040 (1959), Institute National de la Propriété Industrielle, Paris.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[Casteljau1963]: de Casteljau, P.: Courbes et surfaces à pôles, Microfiche P 4147-1, André Citroën Automobile SA, Paris, (1963).","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[PopielNoakes2007]: Popiel, T. and Noakes, L.: Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory (2007), 148(2), pp. 111–127.- doi: 10.1016/j.jat.2007.03.002.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[1]: The images are rendered using asymptote_export_S2_signals. For code examples, see Get started: Optimize!.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/GradientOfSecondOrderDifference.jl\"","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#secondOrderDifferenceGrad-1","page":"see the gradient of d_2","title":"Illustration of the Gradient of a Second Order Difference","text":"","category":"section"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"This example explains how to compute the gradient of the second order difference mid point model using adjoint_Jacobi_fields.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"This example also illustrates the PowerManifold manifold as well as ArmijoLinesearch.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"We first initialize the manifold","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"using Manopt, Manifolds","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"using Colors\nblack = RGBA{Float64}(colorant\"#000000\")\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\") # points\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\") # results\nTolVibrantCyan = RGBA{Float64}(colorant\"#33BBEE\") # vectors\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\") # geo\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"Assume we have two points xy on the equator of the Sphere mathcal M = mathbb S^2 and a point y near the north pole","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"M = Sphere(2)\np = [1.0, 0.0, 0.0]\nq = [0.0, 1.0, 0.0]\nc = mid_point(M, p, q)\nr = shortest_geodesic(M, [0.0, 0.0, 1.0], c, 0.1)\n[c, r]","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"Now the second order absolute difference can be stated as (see [Bačák, Bergmann, Steidl, Weinmann, 2016])","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"d_2(xyz) = min_c  mathcal C_xz d_mathcal M(cy)qquad xyzmathcal M","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"where mathcal C_xz is the set of all mid points g(frac12xz), where g is a (not necessarily minimizing) geodesic connecting x and z.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"For illustration we further define the point opposite of","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"c2 = -c","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"and draw the geodesic connecting y and the nearest mid point c, namely","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"T = [0:0.1:1.0...]\ngeoPts_yc = shortest_geodesic(M, r, c, T)\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"looks as follows using the asymptote_export_S2_signals export","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"asymptote_export_S2_signals(\"secondOrderData.asy\";\n    render = asyResolution,\n    curves = [ geoPts_yc ],\n    points = [ [x,y,z], [c,c2] ],\n    colors=Dict(:curves => [TolVibrantTeal], :points => [black, TolVibrantBlue]),\n    dot_size = 3.5, line_width = 0.75, camera_position = (1.2,1.,.5)\n)\nrender_asymptote(\"SecondOrderData.asy\"; render=2)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Image: Three points $p,r,q$ and the midpoint $c=c(p,q)$ (blue))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"Since we moved r 10% along the geodesic from the north pole to c, the distance to c is frac9pi20approx 14137, and this is also what","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"costTV2(M, (p, r, q))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"returns, see costTV2 for reference. But also its gradient can be easily computed since it is just a distance with respect to y and a concatenation of a geodesic, where the start or end point is the argument, respectively, with a distance. Hence the adjoint differentials adjoint_differential_geodesic_startpoint and adjoint_differential_geodesic_endpoint can be employed, see ∇TV2 for details. we obtain","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Xp, Xr, Xq) = ∇TV2(M, (p, r, q))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"When we aim to minimize this, we look at the negative gradient, i.e. we can draw this as","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"asymptote_export_S2_signals(\"SecondOrderGradient.asy\";\n   points = [ [x,y,z], [c,c2] ],\n   colors=Dict(:tvectors => [TolVibrantCyan], :points => [black, TolVibrantBlue]),\n   dot_size = 3.5, line_width = 0.75, camera_position = (1.2,1.,.5)\n)\nrender_asymptote(\"SecondOrderGradient.asy\"; render=2)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Image: Three points $x,y,z$ and the negative gradient of the second order absolute difference)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"If we now perform a gradient step, we obtain the three points","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"pn, rn, qn = exp.(Ref(M), [p, r, q], [-Xp, -Xr, -Xq])","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"as well we the new mid point","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"cn = mid_point(M, pn, qn)\ngeoPts_yncn = shortest_geodesic(M, rn, cn, T)\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"and obtain the new situation","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"asymptote_export_S2_signals(\"SecondOrderMin1.asy\";\n    points = [ [x,y,z], [c,c2,cn], [xn,yn,zn] ],\n    curves = [ geoPts_yncn ] ,\n    tangent_vectors = [Tuple.([ [p, -Xp], [r, Xr], [q, Xq] ])],\n    colors=Dict(:tvectors => [TolVibrantCyan],\n        :points => [black, TolVibrantBlue, TolVibrantOrange],\n        :curves => [TolVibrantTeal]\n    ),\n    dot_size = 3.5, line_width = 0.75, camera_position = (1.2,1.,.5)\n)\nrender_asymptote(\"SecondOrderMin1.asy\"; render=2)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"#md","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Image: A gradient Step)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"One can see, that this step slightly “overshoots”, i.e. r is now even below c. and the cost function is still at","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"costTV2(M, (pn, rn, qn))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"But we can also search for the best step size using linesearch_backtrack on the PowerManifold manifold mathcal N = mathcal M^3 = (mathbb S^2)^3","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"x = [p, r, q]\nN = PowerManifold(M, NestedPowerRepresentation(), 3)\ns = linesearch_backtrack(\n    M,\n    x -> costTV2(M, Tuple(x)),\n    x,\n    [∇TV2(M, (p, r, q))...],  # transform from tuple to PowTVector\n    1.0, # initial stepsize guess\n    0.999, # decrease\n    0.96,  #contract\n)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"and for the new points","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"pm, rm, qm = exp.(Ref(M), [p, r, q], s * [-Xp, -Xr, -Xq])\ncm = mid_point(M, pm, qm)\ngeoPts_xmzm = shortest_geodesic(M, pm, qm, T)\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"we obtain again with","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"asymptote_export_S2_signals(\"SecondOrderMin2.asy\";\n    points = [ [x,y,z], [c,c2,cm], [xm,ym,zm] ],\n    curves = [ geoPts_xmzm ] ,\n    tangent_vectors = [Tuple.( [-ξx, -ξy, -ξz], [x, y, z] )],\n    colors=Dict(:tvectors => [TolVibrantCyan],\n                :points => [black, TolVibrantBlue, TolVibrantOrange],\n                :curves => [TolVibrantTeal]\n                ),\n    dot_size = 3.5, line_width = 0.75, camera_position = (1.2,1.,.5)\n)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Image: A gradient Step)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"Here, the cost function yields","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"costTV2(M, (pm, rm, qm))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"which is nearly zero, as one can also see, since the new center c and r are quite close.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#Literature-1","page":"see the gradient of d_2","title":"Literature","text":"","category":"section"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"<ul>\n<li id=\"BačákBergmannSteidlWeinmann2016\">[<a>Bačák, Bergmann, Steidl, Weinmann, 2016</a>]\n  Bačák, M; Bergmann, R.; Steidl, G; Weinmann, A.: <emph>A second order nonsmooth\n  variational model for restoring manifold-valued images.</emph>,\n  SIAM Journal on Scientific Computations, Volume 38, Number 1, pp. A567–597,\n  doi: <a href=\"https://doi.org/10.1137/15M101988X\">10.1137/15M101988X</a></li>\n</ul>","category":"page"},{"location":"index.html#Welcome-to-Manopt.jl-1","page":"Home","title":"Welcome to Manopt.jl","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"CurrentModule = Manopt","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"Manopt.Manopt","category":"page"},{"location":"index.html#Manopt.Manopt","page":"Home","title":"Manopt.Manopt","text":"Manopt.jl – Optimization on Manifolds in Julia.\n\n\n\n\n\n","category":"module"},{"location":"index.html#","page":"Home","title":"Home","text":"For a function fcolonmathcal M to mathbb R defined on a Riemannian manifold mathcal M we aim to solve","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"operatorname*argmin_x  mathcal M f(x)","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"or in other words: find the point x on the manifold, where f reaches its minimal function value.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"Manopt.jl provides a framework for optimization on manifolds. Based on Manopt and MVIRT, both implemented in Matlab, this toolbox provide an easy access to optimization methods on manifolds for Julia, including example data and visualization methods.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"If you want to delve right into Manopt.jl check out the Get started: Optimize! tutorial.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"Manopt.jl makes it easy to use an algorithm for your favorite manifold as well as a manifold for your favorite algorithm. It already provides many manifolds and algorithms, which can easily be enhanced, for example to record certain data or display information throughout iterations.","category":"page"},{"location":"index.html#Main-Features-1","page":"Home","title":"Main Features","text":"","category":"section"},{"location":"index.html#Functions-on-Manifolds-1","page":"Home","title":"Functions on Manifolds","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"Several functions are available, implemented on an arbitrary manifold, cost functions, differentials, and gradients as well as proximal maps, but also several jacobi Fields and their adjoints.","category":"page"},{"location":"index.html#Optimization-Algorithms-(Solvers)-1","page":"Home","title":"Optimization Algorithms (Solvers)","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"For every optimization algorithm, a solver is implemented based on a Problem that describes the problem to solve and its Options that set up the solver, store interims values. Together they form a plan.","category":"page"},{"location":"index.html#Visualization-1","page":"Home","title":"Visualization","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"To visualize and interpret results, Manopt.jl aims to provide both easy plot functions as well as exports. Furthermore a system to get debug during the iterations of an algorithms as well as record capabilities, i.e. to record a specified tuple of values per iteration, most prominently RecordCost and RecordIterate. Take a look at the Get started: Optimize! tutorial how to easily activate this.","category":"page"},{"location":"index.html#Manifolds-1","page":"Home","title":"Manifolds","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"This project is build upon ManifoldsBase.jl, a generic interface to implement manifolds. Certain functions are extended for specific manifolds from Manifolds.jl, but all other manifolds from that package can be used here, too.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"The notation in the documentation aims to follow the same notation from these packages.","category":"page"},{"location":"index.html#Literature-1","page":"Home","title":"Literature","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"If you want to get started with manifolds, one book is [do Carmo, 1992], and if you want do directly dive into optimization on manifolds, my favourite reference is [Absil, Mahony, Sepulchre, 2008], which is also available online for free.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"<ul>\n<li id=\"AbsilMahonySepulchre2008\">\n    [<a>Absil, Mahony, Sepulchre, 2008</a>]\n    P.-A. Absil, R. Mahony and R. Sepulchre,\n    <emph>Optimization Algorithms on Matrix Manifolds</emph>,\n    Princeton University Press, 2008,\n    doi: <a href=\"https://doi.org/10.1515/9781400830244\">10.1515/9781400830244</a>,\n    <a href=\"http://press.princeton.edu/chapters/absil/\">open access</a>.\n</li>\n<li id=\"doCarmo1992\">\n    [<a>doCarmo, 1992</a>]\n    M. P. do Carmo,\n    <emph>Riemannian Geometry</emph>,\n    Birkhäuser Boston, 1992,\n    ISBN: 0-8176-3490-8.\n</li>\n</ul>","category":"page"},{"location":"functions/gradients.html#GradientFunctions-1","page":"Gradients","title":"Gradients","text":"","category":"section"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"For a function fcolonmathcal Mtomathbb R the Riemannian gradient nabla f(x) at xmathcal M is given by the unique tangent vector fulfilling","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"langle nabla f(x) xirangle_x = D_xfxiquad\nforall xi  T_xmathcal M","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"where D_xfxi denotes the differential of f at x with respect to the tangent direction (vector) xi or in other words the directional derivative.","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"This page collects the available gradients.","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"Modules = [Manopt]\nPages   = [\"gradients.jl\"]","category":"page"},{"location":"functions/gradients.html#Manopt.forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{ManifoldsBase.PowerManifold{𝔽,TM,TSize,TPR},Any}} where TPR where TSize where TM where 𝔽","page":"Gradients","title":"Manopt.forward_logs","text":"ξ = forward_logs(M,x)\n\ncompute the forward logs F (generalizing forward differences) orrucirng, in the power manifold array, the function\n\nF_i(x) = sum_j  mathcal I_i log_x_i x_jquad i    mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOuput\n\nξ – resulting tangent vector in T_xmathcal M representing the logs, where mathcal N is thw power manifold with the number of dimensions added to size(x).\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.∇L2_acceleration_bezier-Union{Tuple{P}, Tuple{ManifoldsBase.Manifold,AbstractArray{P,1},AbstractArray{#s30,1} where #s30<:Integer,AbstractArray{#s29,1} where #s29<:AbstractFloat,Float64,AbstractArray{P,1}}} where P","page":"Gradients","title":"Manopt.∇L2_acceleration_bezier","text":"∇L2_acceleration_bezier(\n    M::Manifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector{<:AbstractFloat},\n    λ::Float64,\n    d::AbstractVector{P}\n) where {P}\n\ncompute the gradient of the discretized acceleration of a composite Bézier curve on the Manifold M with respect to its control points B together with a data term that relates the junction points p_i to the data d with a weigth lambda comapared to the acceleration. The curve is evaluated at the points given in pts (elementwise in 0N), where N is the number of segments of the Bézier curve. The summands are ∇distance for the data term and ∇acceleration_bezier for the acceleration with interpolation constrains. Here the get_bezier_junctions are included in the optimization, i.e. setting λ=0 yields the unconstrained acceleration minimization. Note that this is ill-posed, since any Bézier curve identical to a geodesic is a minimizer.\n\nNote that the Beziér-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segmenents can internally be reconstructed.\n\nSee also\n\n∇acceleration_bezier, cost_L2_acceleration_bezier, cost_acceleration_bezier.\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.∇TV","page":"Gradients","title":"Manopt.∇TV","text":"ξ = ∇TV(M,λ,x,[p])\n\nCompute the (sub)gradient partial F of all forward differences orrucirng, in the power manifold array, i.e. of the function\n\nF(x) = sum_isum_j  mathcal I_i d^p(x_ix_j)\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOuput\n\nξ – resulting tangent vector in T_xmathcal M.\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.∇TV-Union{Tuple{T}, Tuple{MT}, Tuple{MT,Tuple{T,T}}, Tuple{MT,Tuple{T,T},Any}} where T where MT<:ManifoldsBase.Manifold","page":"Gradients","title":"Manopt.∇TV","text":"∇TV(M,(x,y),[p=1])\n\ncompute the (sub) gradient of frac1pd^p_mathcal M(xy) with respect to both x and y.\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.∇TV2","page":"Gradients","title":"Manopt.∇TV2","text":"∇TV2(M,q [,p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(x_1x_2x_3) with respect to all x_1x_2x_3 occuring along any array dimension in the point x, where M is the corresponding PowerManifold.\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.∇TV2-Union{Tuple{MT}, Tuple{MT,Any}, Tuple{MT,Any,Number}} where MT<:ManifoldsBase.Manifold","page":"Gradients","title":"Manopt.∇TV2","text":"∇TV2(M,(x,y,z),p)\n\ncomputes the (sub) gradient of frac1pd_2^p(xyz) with respect to x, y, and z, where d_2 denotes the second order absolute difference using the mid point model, i.e. let\n\n  mathcal C = bigl c   mathcal M   g(tfrac12x_1x_3) text for some geodesic gbigr\n\ndenote the mid points between x and z on the manifold mathcal M. Then the absolute second order difference is defined as\n\nd_2(xyz) = min_c  mathcal C_xz d(cy)\n\nWhile the (sub)gradient with respect to y is easy, the other two require the evaluation of an adjoint_Jacobi_field. See Illustration of the Gradient of a Second Order Difference for its derivation.\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.∇acceleration_bezier-Union{Tuple{P}, Tuple{ManifoldsBase.Manifold,AbstractArray{P,1},AbstractArray{#s30,1} where #s30<:Integer,AbstractArray{#s29,1} where #s29<:AbstractFloat}} where P","page":"Gradients","title":"Manopt.∇acceleration_bezier","text":"∇acceleration_bezier(\n    M::Manifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer}\n    T::AbstractVector{<:AbstractFloat}\n)\n\ncompute the gradient of the discretized acceleration of a (composite) Bézier curve c_B(t) on the Manifold M with respect to its control points B given as a point on the PowerManifold assuming C1 conditions and known degrees. The curve is evaluated at the points given in T (elementwise in 0N, where N is the number of segments of the Bézier curve). The get_bezier_junctions are fixed for this gradient (interpolation constraint). For the unconstrained gradient, see ∇L2_acceleration_bezier and set λ=0 therein. This gradient is computed using adjoint_Jacobi_fields. For details, see [BergmannGousenbourger2018]. See de_casteljau for more details on the curve.\n\nSee also\n\ncost_acceleration_bezier,  ∇L2_acceleration_bezier, cost_L2_acceleration_bezier.\n\n[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics (2018). doi 10.3389/fams.2018.00059, arXiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.∇distance","page":"Gradients","title":"Manopt.∇distance","text":"∇distance(M,y,x[, p=2])\n\ncompute the (sub)gradient of the distance (squared)\n\nf(x) = frac12 d^p_mathcal M(xy)\n\nto a fixed point y on the manifold M and p is an integer. The gradient reads\n\n  nabla f(x) = -d_mathcal M^p-2(xy)log_xy\n\nfor pneq 1 or xneq  y. Note that for the remaining case p=1, x=y the function is not differentiable. In this case, the function returns the corresponding zero tangent vector, since this is an element of the subdifferential.\n\nOptional\n\np – (2) the exponent of the distance,  i.e. the default is the squared distance\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.∇intrinsic_infimal_convolution_TV12-Union{Tuple{mT}, Tuple{mT,Any,Any,Any,Any,Any}} where mT<:ManifoldsBase.Manifold","page":"Gradients","title":"Manopt.∇intrinsic_infimal_convolution_TV12","text":"∇u,⁠∇v = ∇intrinsic_infimal_convolution_TV12(M,f,u,v,α,β)\n\ncompute (sub)gradient of the intrinsic infimal convolution model using the mid point model of second order differences, see costTV2, i.e. for some f  mathcal M on a PowerManifold manifold mathcal M this function computes the (sub)gradient of\n\nE(uv) =\nfrac12sum_i  mathcal G d_mathcal M(g(frac12v_iw_i)f_i)\n+ alpha\nbigl(\nbetamathrmTV(v) + (1-beta)mathrmTV_2(w)\nbigr)\n\nwhere both total variations refer to the intrinsic ones, ∇TV and ∇TV2, respectively.\n\n\n\n\n\n","category":"method"},{"location":"solvers/NelderMead.html#NelderMeadSolver-1","page":"Nelder–Mead","title":"Nelder Mead Method","text":"","category":"section"},{"location":"solvers/NelderMead.html#","page":"Nelder–Mead","title":"Nelder–Mead","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/NelderMead.html#","page":"Nelder–Mead","title":"Nelder–Mead","text":"    NelderMead\n    NelderMead!","category":"page"},{"location":"solvers/NelderMead.html#Manopt.NelderMead","page":"Nelder–Mead","title":"Manopt.NelderMead","text":"NelderMead(M, F [, p])\n\nperform a nelder mead minimization problem for the cost funciton F on the manifold M. If the initial population p is not given, a random set of points is chosen.\n\nThis algorithm is adapted from the Euclidean Nelder-Mead method, see https://en.wikipedia.org/wiki/Nelder–Mead_method and http://www.optimization-online.org/DB_FILE/2007/08/1742.pdf.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fcolonmathcal Mtomathbb R to minimize\npopulation – (n+1 random_point(M)) an initial population of n+1 points, where n is the dimension of the manifold M.\n\nOptional\n\nstopping_criterion – (StopAfterIteration(2000)) a StoppingCriterion\nα – (1.) reflection parameter (alpha  0)\nγ – (2.) expansion parameter (gamma)\nρ – (1/2) contraction parameter, 0  rho leq frac12,\nσ – (1/2) shrink coefficient, 0  sigma leq 1\nretraction_method – (ExponentialRetraction) the rectraction to use\ninverse_retraction_method - (LogarithmicInverseRetraction) an inverse retraction to use.\n\nand the ones that are passed to decorate_options for decorators.\n\nOutput\n\neither x the last iterate or the complete options depending on the optional keyword return_options, which is false by default (hence then only x is returned).\n\n\n\n\n\n","category":"function"},{"location":"solvers/NelderMead.html#Manopt.NelderMead!","page":"Nelder–Mead","title":"Manopt.NelderMead!","text":"NelderMead(M, F [, p])\n\nperform a nelder mead minimization problem for the cost funciton F on the manifold M. If the initial population p is not given, a random set of points is chosen. If it is given, the computation is done in place of p.\n\nFor more options see NelderMead.\n\n\n\n\n\n","category":"function"},{"location":"solvers/NelderMead.html#Options-1","page":"Nelder–Mead","title":"Options","text":"","category":"section"},{"location":"solvers/NelderMead.html#","page":"Nelder–Mead","title":"Nelder–Mead","text":"    NelderMeadOptions","category":"page"},{"location":"solvers/NelderMead.html#Manopt.NelderMeadOptions","page":"Nelder–Mead","title":"Manopt.NelderMeadOptions","text":"NelderMeadOptions <: Options\n\nDescribes all parameters and the state of a Nealer-Mead heuristic based optimization algorithm.\n\nFields\n\nThe naming of these parameters follows the Wikipedia article of the Euclidean case. The default is given in brackets, the required value range after the description\n\npopulation – an Array{point,1} of n+1 points x_i, i=1ldotsn+1, where n is the dimension of the manifold.\nstopping_criterion – (StopAfterIteration(2000)) a StoppingCriterion\nα – (1.) reflection parameter (alpha  0)\nγ – (2.) expansion parameter (gamma0)\nρ – (1/2) contraction parameter, 0  rho leq frac12,\nσ – (1/2) shrink coefficient, 0  sigma leq 1\nx – (p[1]) - a field to collect the current best value\nretraction_method – ExponentialRetraction() the rectraction to use, defaults to the exponential map\ninverse_retraction_method - LogarithmicInverseRetraction an inverse_retraction(M,x,y) to use.\n\nConstructors\n\nNelderMead(M,stop, retr; α=1. , γ=2., ρ=1/2, σ=1/2)\n\nconstruct a Nelder-Mead Option with a set of dimension(M)+1 random points.\n\nNelderMead(p, stop retr; α=1. , γ=2., ρ=1/2, σ=1/2)\n\nconstruct a Nelder-Mead Option with a set p of points\n\n\n\n\n\n","category":"type"}]
}
