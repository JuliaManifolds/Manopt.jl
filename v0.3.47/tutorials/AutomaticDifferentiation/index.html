<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Use AD in Manopt ¬∑ Manopt.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../Optimize!/">Get Started: Optimize!</a></li><li class="is-active"><a class="tocitem" href>Use AD in Manopt</a><ul class="internal"><li><a class="tocitem" href="#.-(Intrinsic)-Forward-Differences"><span>1. (Intrinsic) Forward Differences</span></a></li><li><a class="tocitem" href="#.-Conversion-of-a-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-a-(not-Necessarily-Isometrically)-Embedded-Manifold"><span>2. Conversion of a Euclidean Gradient in the Embedding to a Riemannian Gradient of a (not Necessarily Isometrically) Embedded Manifold</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../HowToRecord/">Record Values</a></li><li><a class="tocitem" href="../ConstrainedOptimization/">Do constrained Optimization</a></li><li><a class="tocitem" href="../GeodesicRegression/">Do Geodesic Regression</a></li><li><a class="tocitem" href="../Bezier/">Use B√©zier Curves</a></li><li><a class="tocitem" href="../SecondOrderDifference/">Compute a Second Order Difference</a></li><li><a class="tocitem" href="../StochasticGradientDescent/">Do Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../Benchmark/">Speed up! Using <code>gradF!</code></a></li><li><a class="tocitem" href="../JacobiFields/">Illustrate Jacobi Fields</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../../solvers/">Introduction</a></li><li><a class="tocitem" href="../../solvers/alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/augmented_Lagrangian_method/">Augmented Lagrangian Method</a></li><li><a class="tocitem" href="../../solvers/ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../../solvers/conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../../solvers/cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../../solvers/DouglasRachford/">Douglas‚ÄìRachford</a></li><li><a class="tocitem" href="../../solvers/exact_penalty_method/">Exact Penalty Method</a></li><li><a class="tocitem" href="../../solvers/FrankWolfe/">Frank-Wolfe</a></li><li><a class="tocitem" href="../../solvers/gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/NelderMead/">Nelder‚ÄìMead</a></li><li><a class="tocitem" href="../../solvers/particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../../solvers/primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../../solvers/quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../../solvers/stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../../solvers/truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../../solvers/trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/robustPCA/">Robust PCA</a></li><li><a class="tocitem" href="../../examples/smallestEigenvalue/">Rayleigh quotient</a></li><li><a class="tocitem" href="../../examples/FrankWolfeSPDMean/">Frank Wolfe for Riemannian Center of Mass</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/options/">Options</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../../functions/">Introduction</a></li><li><a class="tocitem" href="../../functions/bezier/">B√©zier curves</a></li><li><a class="tocitem" href="../../functions/costs/">Cost functions</a></li><li><a class="tocitem" href="../../functions/differentials/">Differentials</a></li><li><a class="tocitem" href="../../functions/adjointdifferentials/">Adjoint Differentials</a></li><li><a class="tocitem" href="../../functions/gradients/">Gradients</a></li><li><a class="tocitem" href="../../functions/Jacobi_fields/">Jacobi Fields</a></li><li><a class="tocitem" href="../../functions/proximal_maps/">Proximal Maps</a></li><li><a class="tocitem" href="../../functions/manifold/">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/data/">Data</a></li><li><a class="tocitem" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../list/">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">How to...</a></li><li class="is-active"><a href>Use AD in Manopt</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Use AD in Manopt</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/main/nothing" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><style>
    table {
        display: table !important;
        margin: 2rem auto !important;
        border-top: 2pt solid rgba(0,0,0,0.2);
        border-bottom: 2pt solid rgba(0,0,0,0.2);
    }

    pre, div {
        margin-top: 1.4rem !important;
        margin-bottom: 1.4rem !important;
    }

    .code-output {
        padding: 0.7rem 0.5rem !important;
    }

    .admonition-body {
        padding: 0em 1.25em !important;
    }
</style>

<!-- PlutoStaticHTML.Begin -->
<!--
    # This information is used for caching.
    [PlutoStaticHTML.State]
    input_sha = "48a0b212bfae00eab9e96e8d452a3c776a9551f4639e2655516a0c8f96eb84ac"
    julia_version = "1.8.3"
-->

<div class="markdown"><h1>Using (Euclidean) AD in Manopt.jl</h1></div>


<div class="markdown"><p>Since <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/">Manifolds.jl</a> 0.7, the support of automatic differentiation support has been extended.</p><p>This tutorial explains how to use Euclidean tools to derive a gradient for a real-valued function <span class="tex">$f\colon \mathcal M ‚Üí ‚Ñù$</span>. We will consider two methods: an intrinsic variant and a variant employing the embedding. These gradients can then be used within any gradient based optimization algorithm in <a href="https://manoptjl.org">Manopt.jl</a>.</p><p>While by default we use <a href="https://juliadiff.org/FiniteDifferences.jl/latest/">FiniteDifferences.jl</a>, you can also use <a href="https://github.com/JuliaDiff/FiniteDiff.jl">FiniteDiff.jl</a>, <a href="https://juliadiff.org/ForwardDiff.jl/stable/">ForwardDiff.jl</a>, <a href="https://juliadiff.org/ReverseDiff.jl/">ReverseDiff.jl</a>, or  <a href="https://fluxml.ai/Zygote.jl/">Zygote.jl</a>.</p></div>


<div class="markdown"><p>In this Notebook we will take a look at a few possibilities to approximate or derive the gradient of a function <span class="tex">$f:\mathcal M \to ‚Ñù$</span> on a Riemannian manifold, without computing it yourself. There are mainly two different philosophies:</p><ol><li><p>Working <em>instrinsically</em>, i.e. staying on the manifold and in the tangent spaces. Here, we will consider approximating the gradient by forward differences.</p></li><li><p>Working in an embedding ‚Äì¬†there we can use all tools from functions on Euclidean spaces ‚Äì¬†finite differences or automatic differenciation ‚Äì and then compute the corresponding Riemannian gradient from there.</p></li></ol><p>Let's first load all the packages we need.</p></div>

<pre class='language-julia'><code class='language-julia'>using Manifolds, Manopt, Random, LinearAlgebra</code></pre>


<pre class='language-julia'><code class='language-julia'>using FiniteDifferences</code></pre>


<h2 id=".-(Intrinsic)-Forward-Differences"><a class="docs-heading-anchor" href="#.-(Intrinsic)-Forward-Differences">1. (Intrinsic) Forward Differences</a><a id=".-(Intrinsic)-Forward-Differences-1"></a><a class="docs-heading-anchor-permalink" href="#.-(Intrinsic)-Forward-Differences" title="Permalink"></a></h2><div class="markdown">
<p>A first idea is to generalize (multivariate) finite differences to Riemannian manifolds. Let <span class="tex">$X_1,\ldots,X_d ‚àà T_p\mathcal M$</span> denote an orthonormal basis of the tangent space <span class="tex">$T_p\mathcal M$</span> at the point <span class="tex">$p‚àà\mathcal M$</span> on the Riemannian manifold.</p><p>We can generalize the notion of a directional derivative, i.e. for the ‚Äúdirection‚Äù <span class="tex">$Y‚ààT_p\mathcal M$</span>. Let <span class="tex">$c\colon [-Œµ,Œµ]$</span>, <span class="tex">$Œµ&gt;0$</span>, be a curve with <span class="tex">$c(0) = p$</span>, <span class="tex">$\dot c(0) = Y$</span>, e.g. <span class="tex">$c(t)= \exp_p(tY)$</span>. We obtain</p><p class="tex">$$	Df(p)[Y] = \left. \frac{d}{dt} \right|_{t=0} f(c(t)) = \lim_{t \to 0} \frac{1}{t}(f(\exp_p(tY))-f(p))$$</p><p>We can approximate <span class="tex">$Df(p)[X]$</span> by a finite difference scheme for an <span class="tex">$h&gt;0$</span> as</p><p class="tex">$$DF(p)[Y] ‚âà G_h(Y) := \frac{1}{h}(f(\exp_p(hY))-f(p))$$</p><p>Furthermore the gradient <span class="tex">$\operatorname{grad}f$</span> is the Riesz representer of the differential, ie.</p><p class="tex">$$	Df(p)[Y] = g_p(\operatorname{grad}f(p), Y),\qquad \text{ for all } Y ‚àà T_p\mathcal M$$</p><p>and since it is a tangent vector, we can write it in terms of a basis as</p><p class="tex">$$	\operatorname{grad}f(p) = \sum_{i=1}^{d} g_p(\operatorname{grad}f(p),X_i)X_i
	= \sum_{i=1}^{d} Df(p)[X_i]X_i$$</p><p>and perform the approximation from above to obtain</p><p class="tex">$$	\operatorname{grad}f(p) ‚âà \sum_{i=1}^{d} G_h(X_i)X_i$$</p><p>for some suitable step size <span class="tex">$h$</span>. This comes at the cost of <span class="tex">$d+1$</span> function evaluations and <span class="tex">$d$</span> exponential maps.</p></div>


<div class="markdown"><p>This is the first variant we can use. An advantage is that it is <em>intrinsic</em> in the sense that it does not require any embedding of the manifold.</p></div>


<div class="markdown"><h3>An Example: The Rayleigh Quotient</h3><p>The Rayleigh quotient is concerned with finding eigenvalues (and eigenvectors) of a symmetric matrix <span class="tex">$A\in ‚Ñù^{(n+1)√ó(n+1)}$</span>. The optimization problem reads</p><p class="tex">$$F\colon ‚Ñù^{n+1} \to ‚Ñù,\quad F(\mathbf x) = \frac{\mathbf x^\mathrm{T}A\mathbf x}{\mathbf x^\mathrm{T}\mathbf x}$$</p><p>Minimizing this function yields the smallest eigenvalue <span class="tex">$\lambda_1$</span> as a value and the corresponding minimizer <span class="tex">$\mathbf x^*$</span> is a corresponding eigenvector.</p><p>Since the length of an eigenvector is irrelevant, there is an ambiguity in the cost function. It can be better phrased on the sphere <span class="tex">$ùïä^n$</span> of unit vectors in <span class="tex">$\mathbb R^{n+1}$</span>, i.e.</p><p class="tex">$$\operatorname*{arg\,min}_{p \in ùïä^n}\ f(p) = \operatorname*{arg\,min}_{\ p \in ùïä^n} p^\mathrm{T}Ap$$</p><p>We can compute the Riemannian gradient exactly as</p><p class="tex">$$\operatorname{grad} f(p) = 2(Ap - pp^\mathrm{T}Ap)$$</p><p>so we can compare it to the approximation by finite differences.</p></div>

<pre class='language-julia'><code class='language-julia'>begin
    Random.seed!(42)
    n = 200
    A = randn(n + 1, n + 1)
    A = Symmetric(A)
    M = Sphere(n)
    nothing
end</code></pre>


<pre class='language-julia'><code class='language-julia'>f1(p) = p' * A'p</code></pre>
<pre class="code-output documenter-example-output" id="var-f1">f1 (generic function with 1 method)</pre>

<pre class='language-julia'><code class='language-julia'>gradf1(p) = 2 * (A * p - p * p' * A * p)</code></pre>
<pre class="code-output documenter-example-output" id="var-gradf1">gradf1 (generic function with 1 method)</pre>


<div class="markdown"><p>Manifolds provides a finite difference scheme in tangent spaces, that you can introduce to use an existing framework (if the wrapper is implemented) form Euclidean space. Here we use <code>FiniteDiff.jl</code>.</p></div>

<pre class='language-julia'><code class='language-julia'>r_backend = Manifolds.TangentDiffBackend(Manifolds.FiniteDifferencesBackend())</code></pre>
<pre class="code-output documenter-example-output" id="var-r_backend">Manifolds.TangentDiffBackend{Manifolds.FiniteDifferencesBackend{FiniteDifferences.AdaptedFiniteDifferenceMethod{5, 1, FiniteDifferences.UnadaptedFiniteDifferenceMethod{7, 5}}}, ExponentialRetraction, LogarithmicInverseRetraction, DefaultOrthonormalBasis{‚Ñù, ManifoldsBase.TangentSpaceType}, DefaultOrthonormalBasis{‚Ñù, ManifoldsBase.TangentSpaceType}}(Manifolds.FiniteDifferencesBackend{FiniteDifferences.AdaptedFiniteDifferenceMethod{5, 1, FiniteDifferences.UnadaptedFiniteDifferenceMethod{7, 5}}}(FiniteDifferences.AdaptedFiniteDifferenceMethod{5, 1, FiniteDifferences.UnadaptedFiniteDifferenceMethod{7, 5}}([-2, -1, 0, 1, 2], [0.08333333333333333, -0.6666666666666666, 0.0, 0.6666666666666666, -0.08333333333333333], ([-0.08333333333333333, 0.5, -1.5, 0.8333333333333334, 0.25], [0.08333333333333333, -0.6666666666666666, 0.0, 0.6666666666666666, -0.08333333333333333], [-0.25, -0.8333333333333334, 1.5, -0.5, 0.08333333333333333]), 10.0, 1.0, Inf, 0.05555555555555555, 1.4999999999999998, FiniteDifferences.UnadaptedFiniteDifferenceMethod{7, 5}([-3, -2, -1, 0, 1, 2, 3], [-0.5, 2.0, -2.5, 0.0, 2.5, -2.0, 0.5], ([0.5, -4.0, 12.5, -20.0, 17.5, -8.0, 1.5], [-0.5, 2.0, -2.5, 0.0, 2.5, -2.0, 0.5], [-1.5, 8.0, -17.5, 20.0, -12.5, 4.0, -0.5]), 10.0, 1.0, Inf, 0.5365079365079365, 10.0))), ExponentialRetraction(), LogarithmicInverseRetraction(), DefaultOrthonormalBasis(‚Ñù), DefaultOrthonormalBasis(‚Ñù))</pre>

<pre class='language-julia'><code class='language-julia'>gradf1_FD(p) = Manifolds.gradient(M, f1, p, r_backend)</code></pre>
<pre class="code-output documenter-example-output" id="var-gradf1_FD">gradf1_FD (generic function with 1 method)</pre>

<pre class='language-julia'><code class='language-julia'>begin
    p = zeros(n + 1)
    p[1] = 1.0
    X1 = gradf1(p)
    X2 = gradf1_FD(p)
    norm(M, p, X1 - X2)
end</code></pre>
<pre class="code-output documenter-example-output" id="var-p">1.0003414846716736e-12</pre>


<div class="markdown"><p>We obtain quite a good approximation of the gradient.</p></div>

<h2 id=".-Conversion-of-a-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-a-(not-Necessarily-Isometrically)-Embedded-Manifold"><a class="docs-heading-anchor" href="#.-Conversion-of-a-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-a-(not-Necessarily-Isometrically)-Embedded-Manifold">2. Conversion of a Euclidean Gradient in the Embedding to a Riemannian Gradient of a (not Necessarily Isometrically) Embedded Manifold</a><a id=".-Conversion-of-a-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-a-(not-Necessarily-Isometrically)-Embedded-Manifold-1"></a><a class="docs-heading-anchor-permalink" href="#.-Conversion-of-a-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-a-(not-Necessarily-Isometrically)-Embedded-Manifold" title="Permalink"></a></h2><div class="markdown">
<p>Let <span class="tex">$\tilde f\colon\mathbb R^m \to \mathbb R$</span> be a function on the embedding of an <span class="tex">$n$</span>-dimensional manifold <span class="tex">$\mathcal M \subset \mathbb R^m$</span> and let <span class="tex">$f\colon \mathcal M \to \mathbb R$</span> denote the restriction of <span class="tex">$\tilde f$</span> to the manifold <span class="tex">$\mathcal M$</span>.</p><p>Since we can use the pushforward of the embedding to also embed the tangent space <span class="tex">$T_p\mathcal M$</span>, <span class="tex">$p\in \mathcal M$</span>, we can similarly obtain the differential <span class="tex">$Df(p)\colon T_p\mathcal M \to \mathbb R$</span> by restricting the differential <span class="tex">$D\tilde f(p)$</span> to the tangent space.</p><p>If both <span class="tex">$T_p\mathcal M$</span> and <span class="tex">$T_p\mathbb R^m$</span> have the same inner product, or in other words the manifold is isometrically embedded in <span class="tex">$\mathbb R^m$</span> (like for example the sphere <span class="tex">$\mathbb S^n\subset\mathbb R^{m+1}$</span>), then this restriction of the differential directly translates to a projection of the gradient, i.e.</p><p class="tex">$$\operatorname{grad}f(p) = \operatorname{Proj}_{T_p\mathcal M}(\operatorname{grad} \tilde f(p))$$</p><p>More generally we might have to take a change of the metric into account, i.e.</p><p class="tex">$$\langle  \operatorname{Proj}_{T_p\mathcal M}(\operatorname{grad} \tilde f(p)), X \rangle
= Df(p)[X] = g_p(\operatorname{grad}f(p), X)$$</p><p>or in words: we have to change the Riesz representer of the (restricted/projected) differential of <span class="tex">$f$</span> (<span class="tex">$\tilde f$</span>) to the one with respect to the Riemannian metric. This is done using <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/metric.html#Manifolds.change_representer-Tuple{AbstractManifold,%20AbstractMetric,%20Any,%20Any}"><code>change_representer</code></a>.</p></div>


<div class="markdown"><h3>A Continued Example</h3><p>We continue with the Rayleigh Quotient from before, now just starting with the defintion of the Euclidean case in the embedding, the function <span class="tex">$F$</span>.</p></div>

<pre class='language-julia'><code class='language-julia'>F(x) = x' * A * x / (x' * x);</code></pre>



<div class="markdown"><p>The cost function is the same by restriction</p></div>

<pre class='language-julia'><code class='language-julia'>f2(M, p) = F(p);</code></pre>



<div class="markdown"><p>The gradient is now computed combining our gradient scheme with FiniteDifferences.</p></div>

<pre class='language-julia'><code class='language-julia'>function grad_f2_AD(M, p)
    return Manifolds.gradient(
        M, F, p, Manifolds.RiemannianProjectionBackend(Manifolds.FiniteDifferencesBackend())
    )
end</code></pre>
<pre class="code-output documenter-example-output" id="var-grad_f2_AD">grad_f2_AD (generic function with 1 method)</pre>

<pre class='language-julia'><code class='language-julia'>X3 = grad_f2_AD(M, p)</code></pre>
<pre class="code-output documenter-example-output" id="var-X3">201-element Vector{Float64}:
  0.0
  0.14038510230588766
  1.2081562751116681
 -1.8650092022183193
 -1.3296034534632897
  0.8233596435551424
  0.013307949254910802
  ‚ãÆ
 -1.0895103349140873
  2.423453243905215
  2.349106449107357
  0.5799736335284804
 -0.07423232665910076
  2.0859147085739465</pre>

<pre class='language-julia'><code class='language-julia'>norm(M, p, X1 - X3)</code></pre>
<pre class="code-output documenter-example-output" id="var-hash230331">1.69683800899515e-12</pre>


<div class="markdown"><h3>An Example for a Nonisometrically Embedded Manifold</h3><p>on the manifold <span class="tex">$\mathcal P(3)$</span> of symmetric positive definite matrices.</p></div>


<div class="markdown"><p>The following function computes (half) the distance squared (with respect to the linear affine metric) on the manifold <span class="tex">$\mathcal P(3)$</span> to the identity, i.e. <span class="tex">$I_3$</span>. Denoting the unit matrix we consider the function</p><p class="tex">$$	G(q) = \frac{1}{2}d^2_{\mathcal P(3)}(q,I_3) = \lVert \operatorname{Log}(q) \rVert_F^2,$$</p><p>where <span class="tex">$\operatorname{Log}$</span> denotes the matrix logarithm and <span class="tex">$\lVert \cdot \rVert_F$</span> is the Frobenius norm. This can be computed for symmetric positive definite matrices by summing the squares of the logarithms of the eigenvalues of <span class="tex">$q$</span> and dividing by two:</p></div>

<pre class='language-julia'><code class='language-julia'>G(q) = sum(log.(eigvals(Symmetric(q))) .^ 2) / 2</code></pre>
<pre class="code-output documenter-example-output" id="var-G">G (generic function with 1 method)</pre>


<div class="markdown"><p>We can also interpret this as a function on the space of matrices and apply the Euclidean finite differences machinery; in this way we can easily derive the Euclidean gradient. But when computing the Riemannian gradient, we have to change the representer (see again <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/metric.html#Manifolds.change_representer-Tuple{AbstractManifold,%20AbstractMetric,%20Any,%20Any}"><code>change_representer</code></a>) after projecting onto the tangent space <span class="tex">$T_p\mathcal P(n)$</span> at <span class="tex">$p$</span>.</p><p>Let's first define a point and the manifold <span class="tex">$N=\mathcal P(3)$</span>.</p></div>

<pre class='language-julia'><code class='language-julia'>rotM(Œ±) = [1.0 0.0 0.0; 0.0 cos(Œ±) sin(Œ±); 0.0 -sin(Œ±) cos(Œ±)]</code></pre>
<pre class="code-output documenter-example-output" id="var-rotM">rotM (generic function with 1 method)</pre>

<pre class='language-julia'><code class='language-julia'>q = rotM(œÄ / 6) * [1.0 0.0 0.0; 0.0 2.0 0.0; 0.0 0.0 3.0] * transpose(rotM(œÄ / 6))</code></pre>
<pre class="code-output documenter-example-output" id="var-q">3√ó3 Matrix{Float64}:
 1.0  0.0       0.0
 0.0  2.25      0.433013
 0.0  0.433013  2.75</pre>

<pre class='language-julia'><code class='language-julia'>N = SymmetricPositiveDefinite(3)</code></pre>
<pre class="code-output documenter-example-output" id="var-N">SymmetricPositiveDefinite(3)</pre>

<pre class='language-julia'><code class='language-julia'>is_point(N, q)</code></pre>
<pre class="code-output documenter-example-output" id="var-hash141872">true</pre>


<div class="markdown"><p>We could first just compute the gradient using <code>FiniteDifferences.jl</code>, but this yields the Euclidean gradient:</p></div>

<pre class='language-julia'><code class='language-julia'>FiniteDifferences.grad(central_fdm(5, 1), G, q)</code></pre>
<pre class="code-output documenter-example-output" id="var-hash184002">([3.240417492806275e-14 -2.3531899864903462e-14 0.0; 0.0 0.3514812167654708 0.017000516835452926; 0.0 0.0 0.36129646973723023],)</pre>


<div class="markdown"><p>Instead, we use the <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/features/differentiation.html#Manifolds.RiemannianProjectionBackend"><code>RiemannianProjectedBackend</code></a> of <code>Manifolds.jl</code>, which in this case internally uses <code>FiniteDifferences.jl</code> to compute a Euclidean gradient but then uses the conversion explained above to derive the Riemannian gradient.</p><p>We define this here again as a function <code>grad_G_FD</code> that could be used in the <code>Manopt.jl</code> framework within a gradient based optimization.</p></div>

<pre class='language-julia'><code class='language-julia'>function grad_G_FD(N, q)
    return Manifolds.gradient(
        N, G, q, Manifolds.RiemannianProjectionBackend(Manifolds.FiniteDifferencesBackend())
    )
end</code></pre>
<pre class="code-output documenter-example-output" id="var-grad_G_FD">grad_G_FD (generic function with 1 method)</pre>

<pre class='language-julia'><code class='language-julia'>G1 = grad_G_FD(N, q)</code></pre>
<pre class="code-output documenter-example-output" id="var-G1">3√ó3 Matrix{Float64}:
  3.24042e-14  -2.64734e-14  -5.09481e-15
 -2.64734e-14   1.86368       0.826856
 -5.09481e-15   0.826856      2.81845</pre>


<div class="markdown"><p>Now, we can again compare this to the (known) solution of the gradient, namely the gradient of (half of) the distance squared, i.e. <span class="tex">$G(q) = \frac{1}{2}d^2_{\mathcal P(3)}(q,I_3)$</span> is given by <span class="tex">$\operatorname{grad} G(q) = -\operatorname{log}_q I_3$</span>, where <span class="tex">$\operatorname{log}$</span> is the <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/symmetricpositivedefinite.html#Base.log-Tuple{SymmetricPositiveDefinite,%20Vararg{Any,%20N}%20where%20N}">logarithmic map</a> on the manifold.</p></div>

<pre class='language-julia'><code class='language-julia'>G2 = -log(N, q, Matrix{Float64}(I, 3, 3))</code></pre>
<pre class="code-output documenter-example-output" id="var-G2">3√ó3 Matrix{Float64}:
 -0.0  -0.0       -0.0
 -0.0   1.86368    0.826856
 -0.0   0.826856   2.81845</pre>


<div class="markdown"><p>Both terms agree up to <span class="tex">$1.8√ó10^{-12}$</span>:</p></div>

<pre class='language-julia'><code class='language-julia'>norm(G1 - G2)</code></pre>
<pre class="code-output documenter-example-output" id="var-hash135981">1.776388869742036e-12</pre>

<pre class='language-julia'><code class='language-julia'>isapprox(M, q, G1, G2; atol=2 * 1e-12)</code></pre>
<pre class="code-output documenter-example-output" id="var-hash251789">true</pre>

<h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><div class="markdown">
<p>This tutorial illustrates how to use tools from Euclidean spaces, finite differences or automatic differentiation, to compute gradients on Riemannian manifolds. The scheme allows to use <em>any</em> differentiation framework within the embedding to derive a Riemannian gradient.</p></div>

<!-- PlutoStaticHTML.End --></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Optimize!/">¬´ Get Started: Optimize!</a><a class="docs-footer-nextpage" href="../HowToRecord/">Record Values ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Friday 25 November 2022 07:01">Friday 25 November 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
