<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Steihaug-Toint TCG Method · Manopt.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../index.html"><img src="../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">Manopt.jl</span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Home</a></li><li><a class="tocitem" href="../about.html">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../tutorials/MeanAndMedian.html">get Started: Optimize!</a></li><li><a class="tocitem" href="../tutorials/Benchmark.html">speed up! using <code>gradF!</code></a></li><li><a class="tocitem" href="../tutorials/GeodesicRegression.html">Do Geodesic regression</a></li><li><a class="tocitem" href="../tutorials/HowToRecord.html">Record values</a></li><li><a class="tocitem" href="../tutorials/StochasticGradientDescent.html">do stochastic gradient descent</a></li><li><a class="tocitem" href="../tutorials/BezierCurves.html">work with Bézier curves</a></li><li><a class="tocitem" href="../tutorials/GradientOfSecondOrderDifference.html">see the gradient of <span>$d_2$</span></a></li><li><a class="tocitem" href="../tutorials/JacobiFields.html">use Jacobi Fields</a></li><li><a class="tocitem" href="../pluto/AutomaticDifferentiation.html">AD in Manopt</a></li></ul></li><li><a class="tocitem" href="../plans/index.html">Plans</a></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="index.html">Introduction</a></li><li><a class="tocitem" href="alternating_gradient_descent.html">Alternating Gradient Descent</a></li><li><a class="tocitem" href="ChambollePock.html">Chambolle-Pock</a></li><li><a class="tocitem" href="conjugate_gradient_descent.html">Conjugate gradient descent</a></li><li><a class="tocitem" href="cyclic_proximal_point.html">Cyclic Proximal Point</a></li><li><a class="tocitem" href="DouglasRachford.html">Douglas–Rachford</a></li><li><a class="tocitem" href="gradient_descent.html">Gradient Descent</a></li><li><a class="tocitem" href="NelderMead.html">Nelder–Mead</a></li><li><a class="tocitem" href="particle_swarm.html">Particle Swarm Optimization</a></li><li><a class="tocitem" href="quasi_Newton.html">Quasi-Newton</a></li><li><a class="tocitem" href="stochastic_gradient_descent.html">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="subgradient.html">Subgradient method</a></li><li class="is-active"><a class="tocitem" href="truncated_conjugate_gradient_descent.html">Steihaug-Toint TCG Method</a><ul class="internal"><li><a class="tocitem" href="#Initialization-1"><span>Initialization</span></a></li><li><a class="tocitem" href="#Iteration-1"><span>Iteration</span></a></li><li><a class="tocitem" href="#Result-1"><span>Result</span></a></li><li><a class="tocitem" href="#Remarks-1"><span>Remarks</span></a></li><li><a class="tocitem" href="#Interface-1"><span>Interface</span></a></li><li><a class="tocitem" href="#Options-1"><span>Options</span></a></li><li><a class="tocitem" href="#Additional-Stopping-Criteria-1"><span>Additional Stopping Criteria</span></a></li></ul></li><li><a class="tocitem" href="trust_regions.html">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../functions/index.html">Introduction</a></li><li><a class="tocitem" href="../functions/bezier.html">Bézier curves</a></li><li><a class="tocitem" href="../functions/costs.html">Cost functions</a></li><li><a class="tocitem" href="../functions/differentials.html">Differentials</a></li><li><a class="tocitem" href="../functions/adjointdifferentials.html">Adjoint Differentials</a></li><li><a class="tocitem" href="../functions/gradients.html">Gradients</a></li><li><a class="tocitem" href="../functions/Jacobi_fields.html">Jacobi Fields</a></li><li><a class="tocitem" href="../functions/proximal_maps.html">Proximal Maps</a></li><li><a class="tocitem" href="../functions/manifold.html">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../helpers/data.html">Data</a></li><li><a class="tocitem" href="../helpers/errorMeasures.html">Error Measures</a></li><li><a class="tocitem" href="../helpers/exports.html">Exports</a></li></ul></li><li><a class="tocitem" href="../contributing.html">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../notation.html">Notation</a></li><li><a class="tocitem" href="../list.html">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href="truncated_conjugate_gradient_descent.html">Steihaug-Toint TCG Method</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="truncated_conjugate_gradient_descent.html">Steihaug-Toint TCG Method</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/solvers/truncated_conjugate_gradient_descent.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="tCG-1"><a class="docs-heading-anchor" href="#tCG-1">Steihaug-Toint Truncated Conjugate-Gradient Method</a><a class="docs-heading-anchor-permalink" href="#tCG-1" title="Permalink"></a></h1><p>The aim is to solve the trust-region subproblem</p><div>\[\operatorname*{arg\,min}_{η  ∈  T_{x}\mathcal{M}} m_{x}(η) = F(x) +
⟨\operatorname{grad}F(x), η⟩_{x} + \frac{1}{2} ⟨
\operatorname{Hess}F(x)[η], η⟩_{x}\]</div><div>\[\text{s.t.} \; ⟨η, η⟩_{x} \leq {Δ}^2\]</div><p>on a manifold by using the Steihaug-Toint truncated conjugate-gradient method. All terms involving the trust-region radius use an inner product w.r.t. the preconditioner; this is because the iterates grow in length w.r.t. the preconditioner, guaranteeing that we do not re-enter the trust-region.</p><h2 id="Initialization-1"><a class="docs-heading-anchor" href="#Initialization-1">Initialization</a><a class="docs-heading-anchor-permalink" href="#Initialization-1" title="Permalink"></a></h2><p>Initialize <span>$η_0 = η$</span> if using randomized approach and <span>$η$</span> the zero tangent vector otherwise, <span>$r_0 = \operatorname{grad}F(x)$</span>, <span>$z_0 = \operatorname{P}(r_0)$</span>, <span>$δ_0 = z_0$</span> and <span>$k=0$</span></p><h2 id="Iteration-1"><a class="docs-heading-anchor" href="#Iteration-1">Iteration</a><a class="docs-heading-anchor-permalink" href="#Iteration-1" title="Permalink"></a></h2><p>Repeat until a convergence criterion is reached</p><ol><li>Set <span>$κ = ⟨δ_k, \operatorname{Hess}F(x)[δ_k]⟩_x$</span>,  <span>$α =\frac{⟨r_k, z_k⟩_x}{κ}$</span> and  <span>$⟨η_k, η_k⟩_{x}^* = ⟨η_k, \operatorname{P}(η_k)⟩_x +  2α ⟨η_k, \operatorname{P}(δ_k)⟩_{x} +  {α}^2  ⟨δ_k, \operatorname{P}(δ_k)⟩_{x}$</span>.</li><li>If <span>$κ ≤ 0$</span> or <span>$⟨η_k, η_k⟩_x^* ≥ Δ^2$</span>  return <span>$η_{k+1} = η_k + τ δ_k$</span> and stop.</li><li>Set <span>$η_{k}^*= η_k + α δ_k$</span>, if  <span>$⟨η_k, η_k⟩_{x} + \frac{1}{2} ⟨η_k,  \operatorname{Hess}[F] (η_k)_{x}⟩_{x} ≤ ⟨η_k^*,  η_k^*⟩_{x} + \frac{1}{2} ⟨η_k^*,  \operatorname{Hess}[F] (η_k)_ {x}⟩_{x}$</span>  set <span>$η_{k+1} = η_k$</span> else set <span>$η_{k+1} = η_{k}^*$</span>.</li><li>Set <span>$r_{k+1} = r_k + α \operatorname{Hess}[F](δ_k)_x$</span>,   <span>$z_{k+1} = \operatorname{P}(r_{k+1})$</span>,   <span>$β = \frac{⟨r_{k+1},  z_{k+1}⟩_{x}}{⟨r_k, z_k ⟩_{x}}$</span> and <span>$δ_{k+1} = -z_{k+1} + β δ_k$</span>.</li><li>Set <span>$k=k+1$</span>.</li></ol><h2 id="Result-1"><a class="docs-heading-anchor" href="#Result-1">Result</a><a class="docs-heading-anchor-permalink" href="#Result-1" title="Permalink"></a></h2><p>The result is given by the last computed <span>$η_k$</span>.</p><h2 id="Remarks-1"><a class="docs-heading-anchor" href="#Remarks-1">Remarks</a><a class="docs-heading-anchor-permalink" href="#Remarks-1" title="Permalink"></a></h2><p>The <span>$\operatorname{P}(⋅)$</span> denotes the symmetric, positive deﬁnite preconditioner. It is required if a randomized approach is used i.e. using a random tangent vector <span>$η_0$</span> as initial vector. The idea behind it is to avoid saddle points. Preconditioning is simply a rescaling of the variables and thus a redeﬁnition of the shape of the trust region. Ideally <span>$\operatorname{P}(⋅)$</span> is a cheap, positive approximation of the inverse of the Hessian of <span>$F$</span> at <span>$x$</span>. On default, the preconditioner is just the identity.</p><p>To step number 2: Obtain <span>$τ$</span> from the positive root of <span>$\left\lVert η_k + τ δ_k \right\rVert_{\operatorname{P}, x} = Δ$</span> what becomes after the conversion of the equation to</p><div>\[ τ = \frac{-⟨η_k, \operatorname{P}(δ_k)⟩_{x} +
 \sqrt{⟨η_k, \operatorname{P}(δ_k)⟩_{x}^{2} +
 ⟨δ_k, \operatorname{P}(δ_k)⟩_{x} ( Δ^2 -
 ⟨η_k, \operatorname{P}(η_k)⟩_{x})}}
 {⟨δ_k, \operatorname{P}(δ_k)⟩_{x}}.\]</div><p>It can occur that <span>$⟨δ_k, \operatorname{Hess}[F] (δ_k)_{x}⟩_{x} = κ ≤ 0$</span> at iteration <span>$k$</span>. In this case, the model is not strictly convex, and the stepsize <span>$α =\frac{⟨r_k, z_k⟩_{x}} {κ}$</span> computed in step 1. does not give a reduction in the modelfunction <span>$m_x(⋅)$</span>. Indeed, <span>$m_x(⋅)$</span> is unbounded from below along the line <span>$η_k + α δ_k$</span>. If our aim is to minimize the model within the trust-region, it makes far more sense to reduce <span>$m_x(⋅)$</span> along <span>$η_k + α δ_k$</span> as much as we can while staying within the trust-region, and this means moving to the trust-region boundary along this line. Thus when <span>$κ ≤ 0$</span> at iteration k, we replace <span>$α = \frac{⟨r_k, z_k⟩_{x}}{κ}$</span> with <span>$τ$</span> described as above. The other possibility is that <span>$η_{k+1}$</span> would lie outside the trust-region at iteration k (i.e. <span>$⟨η_k, η_k⟩_{x}^{* } ≥ {Δ}^2$</span> what can be identified with the norm of <span>$η_{k+1}$</span>). In particular, when <span>$\operatorname{Hess}[F] (⋅)_{x}$</span> is positive deﬁnite and <span>$η_{k+1}$</span> lies outside the trust region, the solution to the trust-region problem must lie on the trust-region boundary. Thus, there is no reason to continue with the conjugate gradient iteration, as it stands, as subsequent iterates will move further outside the trust-region boundary. A sensible strategy, just as in the case considered above, is to move to the trust-region boundary by ﬁnding <span>$τ$</span>.</p><h2 id="Interface-1"><a class="docs-heading-anchor" href="#Interface-1">Interface</a><a class="docs-heading-anchor-permalink" href="#Interface-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.truncated_conjugate_gradient_descent" href="#Manopt.truncated_conjugate_gradient_descent"><code>Manopt.truncated_conjugate_gradient_descent</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">truncated_conjugate_gradient_descent(M, F, gradF, x, η, HessF, trust_region_radius)</code></pre><p>solve the trust-region subproblem</p><div>\[\operatorname*{arg\,min}_{η ∈ T_xM}
m_x(η) \quad\text{where} 
m_x(η) = F(x) + ⟨\operatorname{grad}F(x),η⟩_x + \frac{1}{2}⟨\operatorname{Hess}F(x)[η],η⟩_x,\]</div><div>\[\text{such that}\quad ⟨η,η⟩_x ≤ Δ^2\]</div><p>with the <a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>. For a description of the algorithm and theorems offering convergence guarantees, see the reference:</p><ul><li>P.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: <a href="https://doi.org/10.1007/s10208-005-0179-9">10.1007/s10208-005-0179-9</a></li><li>A. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: <a href="https://doi.org/10.1137/1.9780898719857">10.1137/1.9780898719857</a></li></ul><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>F</code> – a cost function <span>$F: \mathcal M → ℝ$</span> to minimize</li><li><code>gradF</code> – the gradient <span>$\operatorname{grad}F: \mathcal M → T\mathcal M$</span> of <code>F</code></li><li><code>HessF</code> – the hessian <span>$\operatorname{Hess}F(x): T_x\mathcal M → T_x\mathcal M$</span>, <span>$X ↦ \operatoname{Hess}F(x)[X] = ∇_ξ\operatorname{grad}f(x)$</span></li><li><code>x</code> – a point on the manifold <span>$x ∈ \mathcal M$</span></li><li><code>η</code> – an update tangential vector <span>$η ∈ T_x\mathcal M$</span></li><li><code>trust_region_radius</code> – a trust-region radius</li></ul><p><strong>Optional</strong></p><ul><li><code>evaluation</code> – (<a href="../plans/index.html#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient and hessian work by  allocation (default) or <a href="../plans/index.html#Manopt.MutatingEvaluation"><code>MutatingEvaluation</code></a> in place</li><li><code>preconditioner</code> – a preconditioner for the hessian H</li><li><code>θ</code> – 1+θ is the superlinear convergence target rate. The algorithm will   terminate early if the residual was reduced by a power of 1+theta.</li><li><code>κ</code> – the linear convergence target rate: algorithm will terminate   early if the residual was reduced by a factor of kappa.</li><li><code>randomize</code> – set to true if the trust-region solve is to be initiated with a   random tangent vector. If set to true, no preconditioner will be   used. This option is set to true in some scenarios to escape saddle   points, but is otherwise seldom activated.</li><li><code>project_vector!</code> : (<code>copyto!</code>) specify a projection operation for tangent vectors   for numerical stability. A function <code>(M, Y, p, X) -&gt; ...</code> working in place of <code>Y</code>.   per default, no projection is perfomed, set it to <code>project!</code> to activate projection.</li><li><code>stopping_criterion</code> – (<a href="index.html#Manopt.StopWhenAny"><code>StopWhenAny</code></a>, <a href="index.html#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a>,   <a href="truncated_conjugate_gradient_descent.html#Manopt.StopIfResidualIsReducedByFactor"><code>StopIfResidualIsReducedByFactor</code></a>, <a href="truncated_conjugate_gradient_descent.html#Manopt.StopIfResidualIsReducedByPower"><code>StopIfResidualIsReducedByPower</code></a>,   <a href="truncated_conjugate_gradient_descent.html#Manopt.StopWhenCurvatureIsNegative"><code>StopWhenCurvatureIsNegative</code></a>, <a href="truncated_conjugate_gradient_descent.html#Manopt.StopWhenTrustRegionIsExceeded"><code>StopWhenTrustRegionIsExceeded</code></a> )   a functor inheriting from <a href="index.html#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> indicating when to stop,   where for the default, the maximal number of iterations is set to the dimension of the   manifold, the power factor is <code>θ</code>, the reduction factor is <code>κ</code>.   .</li><li><code>return_options</code> – (<code>false</code>) – if activated, the extended result, i.e. the   complete <a href="../plans/index.html#Manopt.Options"><code>Options</code></a> re returned. This can be used to access recorded values.   If set to false (default) just the optimal value <code>x_opt</code> is returned</li></ul><p>and the ones that are passed to <a href="../plans/index.html#Manopt.decorate_options"><code>decorate_options</code></a> for decorators.</p><p><strong>Output</strong></p><ul><li><code>η</code> – an approximate solution of the trust-region subproblem in <span>$T_{x}\mathcal M$</span>.</li></ul><p>OR</p><ul><li><code>options</code> - the options returned by the solver (see <code>return_options</code>)</li></ul><p><strong>see also</strong></p><p><a href="trust_regions.html#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b0c2c8ea930fe922f5aceda286ef62e6f98d4a36/src/solvers/truncated_conjugate_gradient_descent.jl#L1-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.truncated_conjugate_gradient_descent!" href="#Manopt.truncated_conjugate_gradient_descent!"><code>Manopt.truncated_conjugate_gradient_descent!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">truncated_conjugate_gradient_descent!(M, F, gradF, x, η, HessF, trust_region_radius; kwargs...)</code></pre><p>solve the trust-region subproblem in place of <code>x</code>.</p><p><strong>Input</strong></p><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>F</code> – a cost function <span>$F: \mathcal M → ℝ$</span> to minimize</li><li><code>gradF</code> – the gradient <span>$\operatorname{grad}F: \mathcal M → T\mathcal M$</span> of <code>F</code></li><li><code>HessF</code> – the hessian <span>$\operatorname{Hess}F(x): T_x\mathcal M → T_x\mathcal M$</span>, <span>$X ↦ \operatoname{Hess}F(x)[X] = ∇_ξ\operatorname{grad}f(x)$</span></li><li><code>x</code> – a point on the manifold <span>$x ∈ \mathcal M$</span></li><li><code>η</code> – an update tangential vector <span>$η ∈ T_x\mathcal M$</span></li><li><code>trust_region_radius</code> – a trust-region radius</li></ul><p>For more details and all optional arguments, see <a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b0c2c8ea930fe922f5aceda286ef62e6f98d4a36/src/solvers/truncated_conjugate_gradient_descent.jl#L90-L106">source</a></section></article><h2 id="Options-1"><a class="docs-heading-anchor" href="#Options-1">Options</a><a class="docs-heading-anchor-permalink" href="#Options-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.TruncatedConjugateGradientOptions" href="#Manopt.TruncatedConjugateGradientOptions"><code>Manopt.TruncatedConjugateGradientOptions</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">TruncatedConjugateGradientOptions &lt;: AbstractHessianOptions</code></pre><p>describe the Steihaug-Toint truncated conjugate-gradient method, with</p><p><strong>Fields</strong></p><p>a default value is given in brackets if a parameter can be left out in initialization.</p><ul><li><code>x</code> : a point, where the trust-region subproblem needs   to be solved</li><li><code>stop</code> : a function s,r = @(o,iter,ξ,x,xnew) returning a stop   indicator and a reason based on an iteration number, the gradient and the   last and current iterates</li><li><code>gradient</code> : the gradient at the current iterate</li><li><code>η</code> : a tangent vector (called update vector), which solves the   trust-region subproblem after successful calculation by the algorithm</li><li><code>δ</code> : search direction</li><li><code>trust_region_radius</code> : the trust-region radius</li><li><code>residual</code> : the gradient</li><li><code>randomize</code> : indicates if the trust-region solve and so the algorithm is to be       initiated with a random tangent vector. If set to true, no       preconditioner will be used. This option is set to true in some       scenarios to escape saddle points, but is otherwise seldom activated.</li><li><code>project_vector!</code> : (<code>copyto!</code>) specify a projection operation for tangent vectors   for numerical stability. A function <code>(M, Y, p, X) -&gt; ...</code> working in place of <code>Y</code>.   per default, no projection is perfomed, set it to <code>project!</code> to activate projection.</li></ul><p><strong>Constructor</strong></p><pre><code class="language-none">TruncatedConjugateGradientOptions(x, stop, eta, delta, Delta, res, uR)</code></pre><p>construct a truncated conjugate-gradient Option with the fields as above.</p><p><strong>See also</strong></p><p><a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="trust_regions.html#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b0c2c8ea930fe922f5aceda286ef62e6f98d4a36/src/plans/hessian_plan.jl#L46-L81">source</a></section></article><h2 id="Additional-Stopping-Criteria-1"><a class="docs-heading-anchor" href="#Additional-Stopping-Criteria-1">Additional Stopping Criteria</a><a class="docs-heading-anchor-permalink" href="#Additional-Stopping-Criteria-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopIfResidualIsReducedByPower" href="#Manopt.StopIfResidualIsReducedByPower"><code>Manopt.StopIfResidualIsReducedByPower</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopIfResidualIsReducedByPower &lt;: StoppingCriterion</code></pre><p>A functor for testing if the norm of residual at the current iterate is reduced by a power of 1+θ compared to the norm of the initial residual, i.e. <span>$\Vert r_k \Vert_x \leqq  \Vert r_0 \Vert_{x}^{1+\theta}$</span>. In this case the algorithm reached superlinear convergence.</p><p><strong>Fields</strong></p><ul><li><code>θ</code> – part of the reduction power</li><li><code>initialResidualNorm</code> - stores the norm of the residual at the initial vector   <span>$η$</span> of the Steihaug-Toint tcg mehtod <a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a></li><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="index.html#Manopt.get_reason"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="language-none">StopIfResidualIsReducedByPower(iRN, θ)</code></pre><p>initialize the StopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN to the power of 1+θ.</p><p><strong>See also</strong></p><p><a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="trust_regions.html#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b0c2c8ea930fe922f5aceda286ef62e6f98d4a36/src/plans/hessian_plan.jl#L406-L431">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopIfResidualIsReducedByFactor" href="#Manopt.StopIfResidualIsReducedByFactor"><code>Manopt.StopIfResidualIsReducedByFactor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopIfResidualIsReducedByFactor &lt;: StoppingCriterion</code></pre><p>A functor for testing if the norm of residual at the current iterate is reduced by a factor compared to the norm of the initial residual, i.e. <span>$\Vert r_k \Vert_x \leqq κ \Vert r_0 \Vert_x$</span>. In this case the algorithm reached linear convergence.</p><p><strong>Fields</strong></p><ul><li><code>κ</code> – the reduction factor</li><li><code>initialResidualNorm</code> - stores the norm of the residual at the initial vector   <span>$η$</span> of the Steihaug-Toint tcg mehtod <a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a></li><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be reached, see <a href="index.html#Manopt.get_reason"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="language-none">StopIfResidualIsReducedByFactor(iRN, κ)</code></pre><p>initialize the StopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN times κ.</p><p><strong>See also</strong></p><p><a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="trust_regions.html#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b0c2c8ea930fe922f5aceda286ef62e6f98d4a36/src/plans/hessian_plan.jl#L365-L390">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenTrustRegionIsExceeded" href="#Manopt.StopWhenTrustRegionIsExceeded"><code>Manopt.StopWhenTrustRegionIsExceeded</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopWhenTrustRegionIsExceeded &lt;: StoppingCriterion</code></pre><p>A functor for testing if the norm of the next iterate in the  Steihaug-Toint tcg mehtod is larger than the trust-region radius, i.e. <span>$\Vert η_{k}^{*} \Vert_x ≧ trust_region_radius$</span>. terminate the algorithm when the trust region has been left.</p><p><strong>Fields</strong></p><ul><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="index.html#Manopt.get_reason"><code>get_reason</code></a>.</li><li><code>storage</code> – stores the necessary parameters <code>η, δ, residual</code> to check the   criterion.</li></ul><p><strong>Constructor</strong></p><pre><code class="language-none">StopWhenTrustRegionIsExceeded([a])</code></pre><p>initialize the StopWhenTrustRegionIsExceeded functor to indicate to stop after the norm of the next iterate is greater than the trust-region radius using the <a href="../plans/index.html#Manopt.StoreOptionsAction"><code>StoreOptionsAction</code></a> <code>a</code>, which is initialized to store <code>:η, :δ, :residual</code> by default.</p><p><strong>See also</strong></p><p><a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="trust_regions.html#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b0c2c8ea930fe922f5aceda286ef62e6f98d4a36/src/plans/hessian_plan.jl#L447-L471">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenCurvatureIsNegative" href="#Manopt.StopWhenCurvatureIsNegative"><code>Manopt.StopWhenCurvatureIsNegative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopWhenCurvatureIsNegative &lt;: StoppingCriterion</code></pre><p>A functor for testing if the curvature of the model is negative, i.e. <span>$\langle \delta_k, \operatorname{Hess}[F](\delta_k)\rangle_x \leqq 0$</span>. In this case, the model is not strictly convex, and the stepsize as computed does not give a reduction of the model.</p><p><strong>Fields</strong></p><ul><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="index.html#Manopt.get_reason"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="language-none">StopWhenCurvatureIsNegative()</code></pre><p><strong>See also</strong></p><p><a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="trust_regions.html#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b0c2c8ea930fe922f5aceda286ef62e6f98d4a36/src/plans/hessian_plan.jl#L486-L504">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenModelIncreased" href="#Manopt.StopWhenModelIncreased"><code>Manopt.StopWhenModelIncreased</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">StopWhenModelIncreased &lt;: StoppingCriterion</code></pre><p>A functor for testing if the curvature of the model value increased.</p><p><strong>Fields</strong></p><ul><li><code>reason</code> – stores a reason of stopping if the stopping criterion has one be   reached, see <a href="index.html#Manopt.get_reason"><code>get_reason</code></a>.</li></ul><p><strong>Constructor</strong></p><pre><code class="language-none">StopWhenModelIncreased()</code></pre><p><strong>See also</strong></p><p><a href="truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, <a href="trust_regions.html#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/b0c2c8ea930fe922f5aceda286ef62e6f98d4a36/src/plans/hessian_plan.jl#L519-L534">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="subgradient.html">« Subgradient method</a><a class="docs-footer-nextpage" href="trust_regions.html">Trust-Regions Solver »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 14 March 2022 11:19">Monday 14 March 2022</span>. Using Julia version 1.6.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
