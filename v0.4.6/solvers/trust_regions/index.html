<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Trust-Regions Solver · Manopt.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../../tutorials/Optimize!/">Get Started: Optimize!</a></li><li><a class="tocitem" href="../../tutorials/AutomaticDifferentiation/">Use AD in Manopt</a></li><li><a class="tocitem" href="../../tutorials/HowToRecord/">Record Values</a></li><li><a class="tocitem" href="../../tutorials/ConstrainedOptimization/">Do constrained Optimization</a></li><li><a class="tocitem" href="../../tutorials/GeodesicRegression/">Do Geodesic Regression</a></li><li><a class="tocitem" href="../../tutorials/Bezier/">Use Bézier Curves</a></li><li><a class="tocitem" href="../../tutorials/SecondOrderDifference/">Compute a Second Order Difference</a></li><li><a class="tocitem" href="../../tutorials/StochasticGradientDescent/">Do Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../../tutorials/Benchmark/">Speed up! Using <code>gradF!</code></a></li><li><a class="tocitem" href="../../tutorials/JacobiFields/">Illustrate Jacobi Fields</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../augmented_Lagrangian_method/">Augmented Lagrangian Method</a></li><li><a class="tocitem" href="../ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../DouglasRachford/">Douglas–Rachford</a></li><li><a class="tocitem" href="../exact_penalty_method/">Exact Penalty Method</a></li><li><a class="tocitem" href="../FrankWolfe/">Frank-Wolfe</a></li><li><a class="tocitem" href="../gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../LevenbergMarquardt/">Levenberg–Marquardt</a></li><li><a class="tocitem" href="../NelderMead/">Nelder–Mead</a></li><li><a class="tocitem" href="../particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li class="is-active"><a class="tocitem" href>Trust-Regions Solver</a><ul class="internal"><li><a class="tocitem" href="#Initialization"><span>Initialization</span></a></li><li><a class="tocitem" href="#Iteration"><span>Iteration</span></a></li><li><a class="tocitem" href="#Result"><span>Result</span></a></li><li><a class="tocitem" href="#Remarks"><span>Remarks</span></a></li><li><a class="tocitem" href="#Interface"><span>Interface</span></a></li><li><a class="tocitem" href="#State"><span>State</span></a></li><li><a class="tocitem" href="#Approximation-of-the-Hessian"><span>Approximation of the Hessian</span></a></li></ul></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/objective/">Objective</a></li><li><a class="tocitem" href="../../plans/state/">Solver State</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../../functions/">Introduction</a></li><li><a class="tocitem" href="../../functions/bezier/">Bézier curves</a></li><li><a class="tocitem" href="../../functions/costs/">Cost functions</a></li><li><a class="tocitem" href="../../functions/differentials/">Differentials</a></li><li><a class="tocitem" href="../../functions/adjointdifferentials/">Adjoint Differentials</a></li><li><a class="tocitem" href="../../functions/gradients/">Gradients</a></li><li><a class="tocitem" href="../../functions/proximal_maps/">Proximal Maps</a></li><li><a class="tocitem" href="../../functions/manifold/">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/data/">Data</a></li><li><a class="tocitem" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../list/">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href>Trust-Regions Solver</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Trust-Regions Solver</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/solvers/trust_regions.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="trust_regions"><a class="docs-heading-anchor" href="#trust_regions">The Riemannian Trust-Regions Solver</a><a id="trust_regions-1"></a><a class="docs-heading-anchor-permalink" href="#trust_regions" title="Permalink"></a></h1><p>The aim is to solve an optimization problem on a manifold</p><p class="math-container">\[\operatorname*{min}_{x  ∈  \mathcal{M}} F(x)\]</p><p>by using the Riemannian trust-regions solver. It is number one choice for smooth optimization. This trust-region method uses the Steihaug-Toint truncated conjugate-gradient method <a href="../truncated_conjugate_gradient_descent/#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a> to solve the inner minimization problem called the trust-regions subproblem. This inner solver can be preconditioned by providing a preconditioner (symmetric and positive deﬁnite, an approximation of the inverse of the Hessian of <span>$F$</span>). If no Hessian of the cost function <span>$F$</span> is provided, a standard approximation of the Hessian based on the gradient <span>$\operatorname{grad}F$</span> with <a href="#Manopt.ApproxHessianFiniteDifference"><code>ApproxHessianFiniteDifference</code></a> will be computed.</p><h2 id="Initialization"><a class="docs-heading-anchor" href="#Initialization">Initialization</a><a id="Initialization-1"></a><a class="docs-heading-anchor-permalink" href="#Initialization" title="Permalink"></a></h2><p>Initialize <span>$x_0 = x$</span> with an initial point <span>$x$</span> on the manifold. It can be given by the caller or set randomly. Set the initial trust-region radius <span>$\Delta =\frac{1}{8} \bar{\Delta}$</span> where <span>$\bar{\Delta}$</span> is the maximum radius the trust-region can have. Usually one uses the root of the manifold&#39;s dimension <span>$\operatorname{dim}(\mathcal{M})$</span>. For accepting the next iterate and evaluating the new trust-region radius, one needs an accept/reject threshold <span>$\rho&#39;  ∈  [0,\frac{1}{4})$</span>, which is <span>$\rho&#39; = 0.1$</span> on default. Set <span>$k=0$</span>.</p><h2 id="Iteration"><a class="docs-heading-anchor" href="#Iteration">Iteration</a><a id="Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Iteration" title="Permalink"></a></h2><p>Repeat until a convergence criterion is reached</p><ol><li>Set <span>$η$</span> as a random tangent vector if using randomized approach. Else  set <span>$η$</span> as the zero vector in the tangential space <span>$T_{x_k}\mathcal{M}$</span>.</li><li>Set <span>$η^*$</span> as the solution of the trust-region subproblem, computed by  the tcg-method with <span>$η$</span> as initial vector.</li><li>If using randomized approach, compare <span>$η^*$</span> with the Cauchy point  <span>$η_{c}^* = -\tau_{c} \frac{\Delta}{\lVert \operatorname{Grad}[F] (x_k) \rVert_{x_k}} \operatorname{Grad}[F] (x_k)$</span> by the model function <span>$m_{x_k}(⋅)$</span>. If the  model decrease is larger by using the Cauchy point, set  <span>$η^* = η_{c}^*$</span>.</li><li>Set <span>${x}^* = \operatorname{retr}_{x_k}(η^*)$</span>.</li><li>Set <span>$\rho = \frac{F(x_k)-F({x}^*)}{m_{x_k}(η)-m_{x_k}(η^*)}$</span>, where  <span>$m_{x_k}(⋅)$</span> describes the quadratic model function.</li><li>Update the trust-region radius:<span>$\Delta = \begin{cases}\frac{1}{4} \Delta &amp;\text{ if } \rho &lt; \frac{1}{4} \, \text{or} \, m_{x_k}(η)-m_{x_k}(η^*) \leq 0 \, \text{or}  \, \rho = \pm  ∈ fty , \\\operatorname{min}(2 \Delta, \bar{\Delta}) &amp;\text{ if } \rho &gt; \frac{3}{4} \, \text{and the tcg-method stopped because of negative curvature or exceeding the trust-region},\\\Delta &amp; \, \text{otherwise.}\end{cases}$</span></li><li>If <span>$m_{x_k}(η)-m_{x_k}(η^*) \geq 0$</span> and <span>$\rho &gt; \rho&#39;$</span> set  <span>$x_k = {x}^*$</span>.</li><li>Set <span>$k = k+1$</span>.</li></ol><h2 id="Result"><a class="docs-heading-anchor" href="#Result">Result</a><a id="Result-1"></a><a class="docs-heading-anchor-permalink" href="#Result" title="Permalink"></a></h2><p>The result is given by the last computed <span>$x_k$</span>.</p><h2 id="Remarks"><a class="docs-heading-anchor" href="#Remarks">Remarks</a><a id="Remarks-1"></a><a class="docs-heading-anchor-permalink" href="#Remarks" title="Permalink"></a></h2><p>To the initialization: a random point on the manifold.</p><p>To step number 1: using a randomized approach means using a random tangent vector as initial vector for the approximate solve of the trust-regions subproblem. If this is the case, keep in mind that the vector must be in the trust-region radius. This is achieved by multiplying <code>η</code> by <code>sqrt(4,eps(Float64))</code> as long as its norm is greater than the current trust-region radius <span>$\Delta$</span>. For not using randomized approach, one can get the zero tangent vector.</p><p>To step number 2: obtain <span>$η^*$</span> by (approximately) solving the trust-regions subproblem</p><p class="math-container">\[\operatorname*{arg\,min}_{η  ∈  T_{x_k}\mathcal{M}} m_{x_k}(η) = F(x_k) +
\langle \operatorname{grad}F(x_k), η \rangle_{x_k} + \frac{1}{2} \langle
\operatorname{Hess}[F](η)_ {x_k}, η \rangle_{x_k}\]</p><p class="math-container">\[\text{s.t.} \; \langle η, η \rangle_{x_k} \leq {\Delta}^2\]</p><p>with the Steihaug-Toint truncated conjugate-gradient (tcg) method. The problem as well as the solution method is described in the <a href="../truncated_conjugate_gradient_descent/#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>. In this inner solver, the stopping criterion  <a href="../truncated_conjugate_gradient_descent/#Manopt.StopWhenResidualIsReducedByFactorOrPower"><code>StopWhenResidualIsReducedByFactorOrPower</code></a> so that superlinear or at least linear convergence in the trust-region method can be achieved.</p><p>To step number 3: if using a random tangent vector as an initial vector, compare the result of the tcg-method with the Cauchy point. Convergence proofs assume that one achieves at least (a fraction of) the reduction of the Cauchy point. The idea is to go in the direction of the gradient to an optimal point. This can be on the edge, but also before. The parameter <span>$\tau_{c}$</span> for the optimal length is defined by</p><p class="math-container">\[\tau_{c} = \begin{cases} 1 &amp; \langle \operatorname{Grad}[F] (x_k), \,
\operatorname{Hess}[F] (η_k)_ {x_k}\rangle_{x_k} \leq 0 , \\
\operatorname{min}(\frac{{\operatorname{norm}(\operatorname{Grad}[F] (x_k))}^3}
{\Delta \langle \operatorname{Grad}[F] (x_k), \,
\operatorname{Hess}[F] (η_k)_ {x_k}\rangle_{x_k}}, 1) &amp; \, \text{otherwise.}
\end{cases}\]</p><p>To check the model decrease one compares</p><p class="math-container">\[m_{x_k}(η_{c}^*) = F(x_k) + \langle η_{c}^*,
\operatorname{Grad}[F] (x_k)\rangle_{x_k} + \frac{1}{2}\langle η_{c}^*,
\operatorname{Hess}[F] (η_{c}^*)_ {x_k}\rangle_{x_k}\]</p><p>with</p><p class="math-container">\[m_{x_k}(η^*) = F(x_k) + \langle η^*,
\operatorname{Grad}[F] (x_k)\rangle_{x_k} + \frac{1}{2}\langle η^*,
\operatorname{Hess}[F] (η^*)_ {x_k}\rangle_{x_k}.\]</p><p>If <span>$m_{x_k}(η_{c}^*) &lt; m_{x_k}(η^*)$</span> then <span>$m_{x_k}(η_{c}^*)$</span> is the better choice.</p><p>To step number 4: <span>$\operatorname{retr}_{x_k}(⋅)$</span> denotes the retraction, a mapping <span>$\operatorname{retr}_{x_k}:T_{x_k}\mathcal{M} \rightarrow \mathcal{M}$</span> which approximates the exponential map. In some cases it is cheaper to use this instead of the exponential.</p><p>To step number 6: one knows that the <a href="../truncated_conjugate_gradient_descent/#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a> algorithm stopped for these reasons when the stopping criteria <a href="../truncated_conjugate_gradient_descent/#Manopt.StopWhenCurvatureIsNegative"><code>StopWhenCurvatureIsNegative</code></a>, <a href="../truncated_conjugate_gradient_descent/#Manopt.StopWhenTrustRegionIsExceeded"><code>StopWhenTrustRegionIsExceeded</code></a> are activated.</p><p>To step number 7: the last step is to decide if the new point <span>${x}^*$</span> is accepted.</p><h2 id="Interface"><a class="docs-heading-anchor" href="#Interface">Interface</a><a id="Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Interface" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.trust_regions" href="#Manopt.trust_regions"><code>Manopt.trust_regions</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trust_regions(M, f, grad_f, hess_f, p)
trust_regions(M, f, grad_f, p)</code></pre><p>run the Riemannian trust-regions solver for optimization on manifolds to minmize <code>f</code>.</p><p>For the case that no hessian is provided, the Hessian is computed using finite difference, see <a href="#Manopt.ApproxHessianFiniteDifference"><code>ApproxHessianFiniteDifference</code></a>. For solving the the inner trust-region subproblem of finding an update-vector, see <a href="../truncated_conjugate_gradient_descent/#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>.</p><ul><li>P.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: <a href="https://doi.org/10.1007/s10208-005-0179-9">10.1007/s10208-005-0179-9</a></li><li>A. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: <a href="https://doi.org/10.1137/1.9780898719857">10.1137/1.9780898719857</a></li></ul><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>f</code> – a cost function <span>$F : \mathcal M → ℝ$</span> to minimize</li><li><code>grad_f</code>- the gradient <span>$\operatorname{grad}F : \mathcal M → T \mathcal M$</span> of <span>$F$</span></li><li><code>Hess_f</code> – (optional), the hessian <span>$\operatorname{Hess}F(x): T_x\mathcal M → T_x\mathcal M$</span>, <span>$X ↦ \operatorname{Hess}F(x)[X] = ∇_ξ\operatorname{grad}f(x)$</span></li><li><code>p</code> – an initial value <span>$x  ∈  \mathcal M$</span></li></ul><p><strong>Optional</strong></p><ul><li><code>evaluation</code> – (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient and hessian work by  allocation (default) or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> in place</li><li><code>max_trust_region_radius</code> – the maximum trust-region radius</li><li><code>preconditioner</code> – a preconditioner (a symmetric, positive definite operator that should approximate the inverse of the Hessian)</li><li><code>randomize</code> – set to true if the trust-region solve is to be initiated with a random tangent vector. If set to true, no preconditioner will be used. This option is set to true in some scenarios to escape saddle points, but is otherwise seldom activated.</li><li><code>project!</code> : (<code>copyto!</code>) specify a projection operation for tangent vectors within the TCG   for numerical stability. A function <code>(M, Y, p, X) -&gt; ...</code> working in place of <code>Y</code>.   per default, no projection is perfomed, set it to <code>project!</code> to activate projection.</li><li><code>retraction</code> – (<code>default_retraction_method(M, typeof(p))</code>) approximation of the exponential map</li><li><code>stopping_criterion</code> – (<a href="../../plans/stopping_criteria/#Manopt.StopWhenAny"><code>StopWhenAny</code></a>(<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(1000)</code>, <a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(10^(-6))</code>) a functor inheriting from <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> indicating when to stop.</li><li><code>trust_region_radius</code> - the initial trust-region radius</li><li><code>ρ_prime</code> – Accept/reject threshold: if ρ (the performance ratio for the iterate) is at least ρ&#39;, the outer iteration is accepted. Otherwise, it is rejected. In case it is rejected, the trust-region radius will have been decreased. To ensure this, ρ&#39; &gt;= 0 must be strictly smaller than 1/4. If ρ_prime is negative, the algorithm is not guaranteed to produce monotonically decreasing cost values. It is strongly recommended to set ρ&#39; &gt; 0, to aid convergence.</li><li><code>ρ_regularization</code> – Close to convergence, evaluating the performance ratio ρ is numerically challenging. Meanwhile, close to convergence, the quadratic model should be a good fit and the steps should be accepted. Regularization lets ρ go to 1 as the model decrease and the actual decrease go to zero. Set this option to zero to disable regularization (not recommended). When this is not zero, it may happen that the iterates produced are not monotonically improving the cost when very close to convergence. This is because the corrected cost improvement could change sign if it is negative but very small.</li><li><code>θ</code> – (<code>1.0</code>) 1+θ is the superlinear convergence target rate of the tCG-method   <a href="../truncated_conjugate_gradient_descent/#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, which computes an   approximate solution for the trust-region subproblem. The tCG-method aborts   if the residual is less than or equal to the initial residual to the power of 1+θ.</li><li><code>κ</code> – (<code>0.1</code>) the linear convergence target rate of the tCG-method   <a href="../truncated_conjugate_gradient_descent/#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a>, which computes an   approximate solution for the trust-region subproblem. The method aborts if the   residual is less than or equal to κ times the initial residual.</li><li><code>reduction_threshold</code> – (<code>0.1</code>) Trust-region reduction threshold: if ρ (the performance ratio for   the iterate) is less than this bound, the trust-region radius and thus the trust-regions   decreases.</li><li><code>augmentation_threshold</code> – (<code>0.75</code>) Trust-region augmentation threshold: if ρ (the performance ratio for   the iterate) is greater than this and further conditions apply, the trust-region radius and thus the trust-regions increases.</li></ul><p><strong>Output</strong></p><p>the obtained (approximate) minimizer <span>$p^*$</span>, see <a href="../#Manopt.get_solver_return"><code>get_solver_return</code></a> for details</p><p><strong>see also</strong></p><p><a href="../truncated_conjugate_gradient_descent/#Manopt.truncated_conjugate_gradient_descent"><code>truncated_conjugate_gradient_descent</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/7f17de1497d091260e5d516de9bb8da91df9d2a7/src/solvers/trust_regions.jl#L148-L226">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.trust_regions!" href="#Manopt.trust_regions!"><code>Manopt.trust_regions!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trust_regions!(M, f, grad_f, Hess_f, x; kwargs...)</code></pre><p>evaluate the Riemannian trust-regions solver for optimization on manifolds in place of <code>x</code>.</p><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>f</code> – a cost function <span>$F: \mathcal M → ℝ$</span> to minimize</li><li><code>grad_f</code>- the gradient <span>$\operatorname{grad}F: \mathcal M → T \mathcal M$</span> of <span>$F$</span></li><li><code>Hess_f</code> – the hessian <span>$H( \mathcal M, x, ξ)$</span> of <span>$F$</span></li><li><code>x</code> – an initial value <span>$x  ∈  \mathcal M$</span></li></ul><p>for more details and all options, see <a href="#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/7f17de1497d091260e5d516de9bb8da91df9d2a7/src/solvers/trust_regions.jl#L234-L247">source</a></section></article><h2 id="State"><a class="docs-heading-anchor" href="#State">State</a><a id="State-1"></a><a class="docs-heading-anchor-permalink" href="#State" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.TrustRegionsState" href="#Manopt.TrustRegionsState"><code>Manopt.TrustRegionsState</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TrustRegionsState &lt;: AbstractHessianSolverState</code></pre><p>describe the trust-regions solver, with</p><p><strong>Fields</strong></p><p>where all but <code>x</code> are keyword arguments in the constructor</p><ul><li><code>p</code> : the current iterate</li><li><code>stop</code> : (`StopAfterIteration(1000) | StopWhenGradientNormLess(1e-6))</li><li><code>max_trust_region_radius</code> : (<code>sqrt(manifold_dimension(M))</code>) the maximum trust-region radius</li><li><code>project!</code> : (<code>copyto!</code>) specify a projection operation for tangent vectors   for numerical stability. A function <code>(M, Y, p, X) -&gt; ...</code> working in place of <code>Y</code>.   per default, no projection is perfomed, set it to <code>project!</code> to activate projection.</li><li><code>randomize</code> : (<code>false</code>) indicates if the trust-region solve is to be initiated with a       random tangent vector. If set to true, no preconditioner will be       used. This option is set to true in some scenarios to escape saddle       points, but is otherwise seldom activated.</li><li><code>ρ_prime</code> : (<code>0.1</code>) a lower bound of the performance ratio for the iterate that       decides if the iteration will be accepted or not. If not, the       trust-region radius will have been decreased. To ensure this,       ρ&#39;&gt;= 0 must be strictly smaller than 1/4. If ρ&#39; is negative,       the algorithm is not guaranteed to produce monotonically decreasing       cost values. It is strongly recommended to set ρ&#39; &gt; 0, to aid       convergence.</li><li><code>ρ_regularization</code> : (<code>10000.0</code>) Close to convergence, evaluating the performance ratio ρ       is numerically challenging. Meanwhile, close to convergence, the       quadratic model should be a good fit and the steps should be       accepted. Regularization lets ρ go to 1 as the model decrease and       the actual decrease go to zero. Set this option to zero to disable       regularization (not recommended). When this is not zero, it may happen       that the iterates produced are not monotonically improving the cost       when very close to convergence. This is because the corrected cost       improvement could change sign if it is negative but very small.</li><li><code>trust_region_radius</code> : the (initial) trust-region radius</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">TrustRegionsState(M, p)</code></pre><p>construct a trust-regions Option with all other fields from above being keyword arguments</p><p><strong>See also</strong></p><p><a href="#Manopt.trust_regions"><code>trust_regions</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/7f17de1497d091260e5d516de9bb8da91df9d2a7/src/solvers/trust_regions.jl#L2-L49">source</a></section></article><h2 id="Approximation-of-the-Hessian"><a class="docs-heading-anchor" href="#Approximation-of-the-Hessian">Approximation of the Hessian</a><a id="Approximation-of-the-Hessian-1"></a><a class="docs-heading-anchor-permalink" href="#Approximation-of-the-Hessian" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.ApproxHessianFiniteDifference" href="#Manopt.ApproxHessianFiniteDifference"><code>Manopt.ApproxHessianFiniteDifference</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ApproxHessianFiniteDifference{E, P, T, G, RTR,, VTR, R &lt;: Real}</code></pre><p>A functor to approximate the Hessian by a finite difference of gradient evaluation.</p><p>Given a point <code>p</code> and a direction <code>X</code> and the gradient <span>$\operatorname{grad}F: \mathcal M \to T\mathcal M$</span> of a function <span>$F$</span> the Hessian is approximated as follows: Let <span>$c$</span> be a stepsize, <span>$X∈ T_p\mathcal M$</span> a tangent vector and <span>$q = \operatorname{retr}_p(\frac{c}{\lVert X \rVert_p}X)$</span> be a step in direction <span>$X$</span> of length <span>$c$</span> following a retraction Then we approximate the Hessian by the finite difference of the gradients, where <span>$\mathcal T_{\cdot\gets\cdot}$</span> is a vector transport.</p><p class="math-container">\[\operatorname{Hess}F(p)[X]
 ≈
\frac{\lVert X \rVert_p}{c}\Bigl( \mathcal T_{p\gets q}\bigr(\operatorname{grad}F(q)\bigl) - \operatorname{grad}F(p)\Bigl)\]</p><p><strong>Fields</strong></p><ul><li><code>gradient!!</code> the gradient function (either allocating or mutating, see <code>evaluation</code> parameter)</li><li><code>step_length</code> a step length for the finite difference</li><li><code>retraction_method</code> - a retraction to use</li><li><code>vector_transport_method</code> a vector transport to use</li></ul><p><strong>Internal temporary fields</strong></p><ul><li><code>grad_tmp</code> a temporary storage for the gradient at the current <code>p</code></li><li><code>grad_dir_tmp</code> a temporary storage for the gradient at the current <code>p_dir</code></li><li><code>p_dir::P</code> a temporary storage to the forward direction (i.e. <span>$q$</span> above)</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">ApproximateFiniteDifference(M, p, grad_f; kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>evaluation</code> (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) whether the gradient is given as an allocation function or an in-place (<a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a>).</li><li><code>steplength</code> (<span>$2^{-14}$</span>) step length <span>$c$</span> to approximate the gradient evaluations</li><li><code>retraction_method</code> – (<code>default_retraction_method(M, typeof(p))</code>) a <code>retraction(M, p, X)</code> to use in the approximation.</li><li><code>vector_transport_method</code> - (<code>default_vector_transport_method(M, typeof(p))</code>) a vector transport to use</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/7f17de1497d091260e5d516de9bb8da91df9d2a7/src/plans/hessian_plan.jl#L153-L193">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.ApproxHessianSymmetricRankOne" href="#Manopt.ApproxHessianSymmetricRankOne"><code>Manopt.ApproxHessianSymmetricRankOne</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ApproxHessianSymmetricRankOne{E, P, G, T, B&lt;:AbstractBasis{ℝ}, VTR, R&lt;:Real}</code></pre><p>A functor to approximate the Hessian by the symmetric rank one update.</p><p><strong>Fields</strong></p><ul><li><code>gradient!!</code> the gradient function (either allocating or mutating, see <code>evaluation</code> parameter).</li><li><code>ν</code> a small real number to ensure that the denominator in the update does not become too small and thus the method does not break down.</li><li><code>vector_transport_method</code> a vector transport to use.</li></ul><p><strong>Internal temporary fields</strong></p><ul><li><code>p_tmp</code> a temporary storage the current point <code>p</code>.</li><li><code>grad_tmp</code> a temporary storage for the gradient at the current <code>p</code>.</li><li><code>matrix</code> a temporary storage for the matrix representation of the approximating operator.</li><li><code>basis</code> a temporary storage for an orthonormal basis at the current <code>p</code>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">ApproxHessianSymmetricRankOne(M, p, gradF; kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>initial_operator</code> (<code>Matrix{Float64}(I, manifold_dimension(M), manifold_dimension(M))</code>) the matrix representation of the initial approximating operator.</li><li><code>basis</code> (<code>DefaultOrthonormalBasis()</code>) an orthonormal basis in the tangent space of the initial iterate p.</li><li><code>nu</code> (<code>-1</code>)</li><li><code>evaluation</code> (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) whether the gradient is given as an allocation function or an in-place (<a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a>).</li><li><code>vector_transport_method</code> (<code>ParallelTransport()</code>) vector transport <span>$\mathcal T_{\cdot\gets\cdot}$</span> to use.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/7f17de1497d091260e5d516de9bb8da91df9d2a7/src/plans/hessian_plan.jl#L253-L273">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.ApproxHessianBFGS" href="#Manopt.ApproxHessianBFGS"><code>Manopt.ApproxHessianBFGS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ApproxHessianBFGS{E, P, G, T, B&lt;:AbstractBasis{ℝ}, VTR, R&lt;:Real}</code></pre><p>A functor to approximate the Hessian by the BFGS update.</p><p><strong>Fields</strong></p><ul><li><code>gradient!!</code> the gradient function (either allocating or mutating, see <code>evaluation</code> parameter).</li><li><code>scale</code></li><li><code>vector_transport_method</code> a vector transport to use.</li></ul><p><strong>Internal temporary fields</strong></p><ul><li><code>p_tmp</code> a temporary storage the current point <code>p</code>.</li><li><code>grad_tmp</code> a temporary storage for the gradient at the current <code>p</code>.</li><li><code>matrix</code> a temporary storage for the matrix representation of the approximating operator.</li><li><code>basis</code> a temporary storage for an orthonormal basis at the current <code>p</code>.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">ApproxHessianBFGS(M, p, gradF; kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>initial_operator</code> (<code>Matrix{Float64}(I, manifold_dimension(M), manifold_dimension(M))</code>) the matrix representation of the initial approximating operator.</li><li><code>basis</code> (<code>DefaultOrthonormalBasis()</code>) an orthonormal basis in the tangent space of the initial iterate p.</li><li><code>nu</code> (<code>-1</code>)</li><li><code>evaluation</code> (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) whether the gradient is given as an allocation function or an in-place (<a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a>).</li><li><code>vector_transport_method</code> (<code>ParallelTransport()</code>) vector transport <span>$\mathcal T_{\cdot\gets\cdot}$</span> to use.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/7f17de1497d091260e5d516de9bb8da91df9d2a7/src/plans/hessian_plan.jl#L387-L407">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../truncated_conjugate_gradient_descent/">« Steihaug-Toint TCG Method</a><a class="docs-footer-nextpage" href="../../plans/">Specify a Solver »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Friday 3 February 2023 20:47">Friday 3 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
