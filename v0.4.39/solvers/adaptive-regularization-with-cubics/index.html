<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Adaptive Regularization with Cubics · Manopt.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../../tutorials/Optimize!/">Get started: Optimize!</a></li><li><a class="tocitem" href="../../tutorials/InplaceGradient/">Speedup using Inplace computations</a></li><li><a class="tocitem" href="../../tutorials/AutomaticDifferentiation/">Use Automatic Differentiation</a></li><li><a class="tocitem" href="../../tutorials/EmbeddingObjectives/">Define Objectives in the Embedding</a></li><li><a class="tocitem" href="../../tutorials/CountAndCache/">Count and use a Cache</a></li><li><a class="tocitem" href="../../tutorials/HowToDebug/">Print Debug Output</a></li><li><a class="tocitem" href="../../tutorials/HowToRecord/">Record values</a></li><li><a class="tocitem" href="../../tutorials/ImplementASolver/">Implement a Solver</a></li><li><a class="tocitem" href="../../tutorials/ConstrainedOptimization/">Do Constrained Optimization</a></li><li><a class="tocitem" href="../../tutorials/GeodesicRegression/">Do Geodesic Regression</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../">Introduction</a></li><li class="is-active"><a class="tocitem" href>Adaptive Regularization with Cubics</a><ul class="internal"><li><a class="tocitem" href="#State"><span>State</span></a></li><li><a class="tocitem" href="#Sub-solvers"><span>Sub solvers</span></a></li><li><a class="tocitem" href="#Lanczos-Iteration"><span>Lanczos Iteration</span></a></li><li><a class="tocitem" href="#(Conjugate)-Gradient-Descent"><span>(Conjugate) Gradient Descent</span></a></li><li><a class="tocitem" href="#Additional-Stopping-Criteria"><span>Additional Stopping Criteria</span></a></li><li><a class="tocitem" href="#Literature"><span>Literature</span></a></li></ul></li><li><a class="tocitem" href="../alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../augmented_Lagrangian_method/">Augmented Lagrangian Method</a></li><li><a class="tocitem" href="../ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../difference_of_convex/">Difference of Convex</a></li><li><a class="tocitem" href="../DouglasRachford/">Douglas–Rachford</a></li><li><a class="tocitem" href="../exact_penalty_method/">Exact Penalty Method</a></li><li><a class="tocitem" href="../FrankWolfe/">Frank-Wolfe</a></li><li><a class="tocitem" href="../gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../LevenbergMarquardt/">Levenberg–Marquardt</a></li><li><a class="tocitem" href="../NelderMead/">Nelder–Mead</a></li><li><a class="tocitem" href="../particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/objective/">Objective</a></li><li><a class="tocitem" href="../../plans/state/">Solver State</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../../functions/">Introduction</a></li><li><a class="tocitem" href="../../functions/bezier/">Bézier curves</a></li><li><a class="tocitem" href="../../functions/costs/">Cost functions</a></li><li><a class="tocitem" href="../../functions/differentials/">Differentials</a></li><li><a class="tocitem" href="../../functions/adjoint_differentials/">Adjoint Differentials</a></li><li><a class="tocitem" href="../../functions/gradients/">Gradients</a></li><li><a class="tocitem" href="../../functions/proximal_maps/">Proximal Maps</a></li><li><a class="tocitem" href="../../functions/manifold/">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/data/">Data</a></li><li><a class="tocitem" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../extensions/">Extensions</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href>Adaptive Regularization with Cubics</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Adaptive Regularization with Cubics</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/solvers/adaptive-regularization-with-cubics.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ARSSection"><a class="docs-heading-anchor" href="#ARSSection">Adaptive regularization with Cubics</a><a id="ARSSection-1"></a><a class="docs-heading-anchor-permalink" href="#ARSSection" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="Manopt.adaptive_regularization_with_cubics" href="#Manopt.adaptive_regularization_with_cubics"><code>Manopt.adaptive_regularization_with_cubics</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">adaptive_regularization_with_cubics(M, f, grad_f, Hess_f, p=rand(M); kwargs...)
adaptive_regularization_with_cubics(M, f, grad_f, p=rand(M); kwargs...)
adaptive_regularization_with_cubics(M, mho, p=rand(M); kwargs...)</code></pre><p>Solve an optimization problem on the manifold <code>M</code> by iteratively minimizing</p><p class="math-container">\[m_k(X) = f(p_k) + ⟨X, \operatorname{grad} f(p_k)⟩ + \frac{1}{2}⟨X, \operatorname{Hess} f(p_k)[X]⟩ + \frac{σ_k}{3}\lVert X \rVert^3\]</p><p>on the tangent space at the current iterate <span>$p_k$</span>, i.e. <span>$X ∈ T_{p_k}\mathcal M$</span> and where <span>$σ_k &gt; 0$</span> is a regularization parameter.</p><p>Let <span>$X_k$</span> denote the minimizer of the model <span>$m_k$</span>, then we use the model improvement</p><p class="math-container">\[ρ_k = \frac{f(p_k) - f(\operatorname{retr}_{p_k}(X_k))}{m_k(0) - m_k(s) + \frac{σ_k}{3}\lVert X_k\rVert^3}.\]</p><p>We use two thresholds <span>$η_2 ≥ η_1 &gt; 0$</span> and set <span>$p_{k+1} = \operatorname{retr}_{p_k}(X_k)$</span> if <span>$ρ ≥ η_1$</span> and reject the candidate otherwise, i.e. set <span>$p_{k+1} = p_k$</span>.</p><p>We further update the regularization parameter using factors <span>$0 &lt; γ_1 &lt; 1 &lt; γ_2$</span></p><p class="math-container">\[σ_{k+1} =
\begin{cases}
    \max\{σ_{\min}, γ_1σ_k\} &amp; \text{ if } ρ \geq η_2 &amp;\text{   (the model was very successful)},\\
    σ_k &amp; \text{ if } ρ \in [η_1, η_2)&amp;\text{   (the model was successful)},\\
    γ_2σ_k &amp; \text{ if } ρ &lt; η_1&amp;\text{   (the model was unsuccessful)}.
\end{cases}\]</p><p>For more details see <a href="../../references/#AgarwalBoumalBullinsCartis:2020">Agarwal, Boumal, Bullins, Cartis, Math. Prog., 2020</a>.</p><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>f</code> – a cost function <span>$F: \mathcal M → ℝ$</span> to minimize</li><li><code>grad_f</code>- the gradient <span>$\operatorname{grad}F: \mathcal M → T \mathcal M$</span> of <span>$F$</span></li><li><code>Hess_f</code> – (optional) the hessian <span>$H( \mathcal M, x, ξ)$</span> of <span>$F$</span></li><li><code>p</code> – an initial value <span>$p  ∈  \mathcal M$</span></li></ul><p>For the case that no hessian is provided, the Hessian is computed using finite difference, see <a href="../trust_regions/#Manopt.ApproxHessianFiniteDifference"><code>ApproxHessianFiniteDifference</code></a>.</p><p>the cost <code>f</code> and its gradient and hessian might also be provided as a <a href="../../plans/objective/#Manopt.ManifoldHessianObjective"><code>ManifoldHessianObjective</code></a></p><p><strong>Keyword arguments</strong></p><p>the default values are given in brackets</p><ul><li><code>σ</code>                      - (<code>100.0 / sqrt(manifold_dimension(M)</code>) initial regularization parameter</li><li><code>σmin</code>                   - (<code>1e-10</code>) minimal regularization value <span>$σ_{\min}$</span></li><li><code>η1</code>                     - (<code>0.1</code>) lower model success threshold</li><li><code>η2</code>                     - (<code>0.9</code>) upper model success threshold</li><li><code>γ1</code>                     - (<code>0.1</code>) regularization reduction factor (for the success case)</li><li><code>γ2</code>                     - (<code>2.0</code>) regularization increment factor (for the non-success case)</li><li><code>evaluation</code>             – (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient works by allocation (default) form <code>grad_f(M, p)</code>                            or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> in place, i.e. is of the form <code>grad_f!(M, X, p)</code> and analogously for the hessian.</li><li><code>retraction_method</code>      – (<code>default_retraction_method(M, typeof(p))</code>) a retraction to use</li><li><code>initial_tangent_vector</code> - (<code>zero_vector(M, p)</code>) initialize any tangent vector data,</li><li><code>maxIterLanczos</code>         - (<code>200</code>) a shortcut to set the stopping criterion in the sub_solver,</li><li><code>ρ_regularization</code>       - (<code>1e3</code>) a regularization to avoid dividing by zero for small values of cost and model</li><li><code>stopping_criterion</code>     - (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(40) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(1e-9) |</code><a href="#Manopt.StopWhenAllLanczosVectorsUsed"><code>StopWhenAllLanczosVectorsUsed</code></a><code>(maxIterLanczos)</code>)</li><li><code>sub_state</code>              - <a href="#Manopt.LanczosState"><code>LanczosState</code></a><code>(M, copy(M, p); maxIterLanczos=maxIterLanczos, σ=σ)                            a state for the subproblem or an [</code>AbstractEvaluationType`](@ref) if the problem is a function.</li><li><code>sub_objective</code>               - a shortcut to modify the objective of the subproblem used within in the</li><li><code>sub_problem</code>            - <a href="../../plans/problem/#Manopt.DefaultManoptProblem"><code>DefaultManoptProblem</code></a><code>(M, sub_objective)</code> the problem (or a function) for the sub problem</li></ul><p>All other keyword arguments are passed to <a href="../../plans/state/#Manopt.decorate_state!"><code>decorate_state!</code></a> for state decorators or <a href="../../plans/objective/#Manopt.decorate_objective!"><code>decorate_objective!</code></a> for objective, respectively. If you provide the <a href="../../plans/objective/#Manopt.ManifoldGradientObjective"><code>ManifoldGradientObjective</code></a> directly, these decorations can still be specified</p><p>By default the <code>debug=</code> keyword is set to <a href="../../plans/debug/#Manopt.DebugIfEntry"><code>DebugIfEntry</code></a><code>(:ρ_denonimator, &gt;(0); message=&quot;Denominator nonpositive&quot;, type=:error)</code><code>to avoid that by rounding errors the denominator in the computation of</code>ρ` gets nonpositive.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/c4bc49dc6c028c8c6bf994d3e053c9c8c1098a11/src/solvers/adaptive_regularization_with_cubics.jl#L158-L234">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.adaptive_regularization_with_cubics!" href="#Manopt.adaptive_regularization_with_cubics!"><code>Manopt.adaptive_regularization_with_cubics!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">adaptive_regularization_with_cubics!(M, f, grad_f, Hess_f, p; kwargs...)
adaptive_regularization_with_cubics!(M, f, grad_f, p; kwargs...)
adaptive_regularization_with_cubics!(M, mho, p; kwargs...)</code></pre><p>evaluate the Riemannian adaptive regularization with cubics solver in place of <code>p</code>.</p><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>f</code> – a cost function <span>$F: \mathcal M → ℝ$</span> to minimize</li><li><code>grad_f</code>- the gradient <span>$\operatorname{grad}F: \mathcal M → T \mathcal M$</span> of <span>$F$</span></li><li><code>Hess_f</code> – (optional) the hessian <span>$H( \mathcal M, x, ξ)$</span> of <span>$F$</span></li><li><code>p</code> – an initial value <span>$p  ∈  \mathcal M$</span></li></ul><p>For the case that no hessian is provided, the Hessian is computed using finite difference, see <a href="../trust_regions/#Manopt.ApproxHessianFiniteDifference"><code>ApproxHessianFiniteDifference</code></a>.</p><p>the cost <code>f</code> and its gradient and hessian might also be provided as a <a href="../../plans/objective/#Manopt.ManifoldHessianObjective"><code>ManifoldHessianObjective</code></a></p><p>for more details and all options, see <a href="#Manopt.adaptive_regularization_with_cubics"><code>adaptive_regularization_with_cubics</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/c4bc49dc6c028c8c6bf994d3e053c9c8c1098a11/src/solvers/adaptive_regularization_with_cubics.jl#L313-L333">source</a></section></article><h2 id="State"><a class="docs-heading-anchor" href="#State">State</a><a id="State-1"></a><a class="docs-heading-anchor-permalink" href="#State" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.AdaptiveRegularizationState" href="#Manopt.AdaptiveRegularizationState"><code>Manopt.AdaptiveRegularizationState</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaptiveRegularizationState{P,T} &lt;: AbstractHessianSolverState</code></pre><p>A state for the <a href="#Manopt.adaptive_regularization_with_cubics"><code>adaptive_regularization_with_cubics</code></a> solver.</p><p><strong>Fields</strong></p><p>a default value is given in brackets if a parameter can be left out in initialization.</p><ul><li><code>η1</code>, <code>η2</code>           – (<code>0.1</code>, <code>0.9</code>) bounds for evaluating the regularization parameter</li><li><code>γ1</code>, <code>γ2</code>           – (<code>0.1</code>, <code>2.0</code>) shrinking and expansion factors for regularization parameter <code>σ</code></li><li><code>p</code>                  – (<code>rand(M)</code> the current iterate</li><li><code>X</code>                  – (<code>zero_vector(M,p)</code>) the current gradient <span>$\operatorname{grad}f(p)$</span></li><li><code>s</code>                  - (<code>zero_vector(M,p)</code>) the tangent vector step resulting from minimizing the model problem in the tangent space <span>$\mathcal T_{p} \mathcal M$</span></li><li><code>σ</code>                 – the current cubic regularization parameter</li><li><code>σmin</code>               – (<code>1e-7</code>) lower bound for the cubic regularization parameter</li><li><code>ρ_regularization</code>   – (1e3) regularization parameter for computing ρ. As we approach convergence the ρ may be difficult to compute with numerator and denominator approaching zero. Regularizing the the ratio lets ρ go to 1 near convergence.</li><li><code>evaluation</code>         - (<code>AllocatingEvaluation()</code>) if you provide a</li><li><code>retraction_method</code>  – (<code>default_retraction_method(M)</code>) the retraction to use</li><li><code>stopping_criterion</code> – (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(100)</code>) a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a></li><li><code>sub_problem</code>        - sub problem solved in each iteration</li><li><code>sub_state</code>          - sub state for solving the sub problem – either a solver state if                        the problem is an <a href="../../plans/problem/#Manopt.AbstractManoptProblem"><code>AbstractManoptProblem</code></a> or an <a href="../../plans/objective/#Manopt.AbstractEvaluationType"><code>AbstractEvaluationType</code></a> if it is a function,                        where it defaults to <a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>.</li></ul><p>Furthermore the following integral fields are defined</p><ul><li><code>q</code>                  - (<code>copy(M,p)</code>) a point for the candidates to evaluate model and ρ</li><li><code>H</code>                  – (<code>copy(M, p, X)</code>) the current hessian, <span>$\operatorname{Hess}F(p)[⋅]$</span></li><li><code>S</code>                  – (<code>copy(M, p, X)</code>) the current solution from the subsolver</li><li><code>ρ</code>                  – the current regularized ratio of actual improvement and model improvement.</li><li><code>ρ_denominator</code>      – (<code>one(ρ)</code>) a value to store the denominator from the computation of ρ                        to allow for a warning or error when this value is non-positive.</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">AdaptiveRegularizationState(M, p=rand(M); X=zero_vector(M, p); kwargs...)</code></pre><p>Construct the solver state with all fields stated above as keyword arguments.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/c4bc49dc6c028c8c6bf994d3e053c9c8c1098a11/src/solvers/adaptive_regularization_with_cubics.jl#L1-L40">source</a></section></article><h2 id="Sub-solvers"><a class="docs-heading-anchor" href="#Sub-solvers">Sub solvers</a><a id="Sub-solvers-1"></a><a class="docs-heading-anchor-permalink" href="#Sub-solvers" title="Permalink"></a></h2><p>There are several ways to approach the subsolver. The default is the first one.</p><h2 id="Lanczos-Iteration"><a class="docs-heading-anchor" href="#Lanczos-Iteration">Lanczos Iteration</a><a id="Lanczos-Iteration-1"></a><a class="docs-heading-anchor-permalink" href="#Lanczos-Iteration" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.LanczosState" href="#Manopt.LanczosState"><code>Manopt.LanczosState</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LanczosState{P,T,SC,B,I,R,TM,V,Y} &lt;: AbstractManoptSolverState</code></pre><p>Solve the adaptive regularized subproblem with a Lanczos iteration</p><p><strong>Fields</strong></p><ul><li><code>p</code> the current iterate</li><li><code>stop</code> – the stopping criterion</li><li><code>σ</code> – the current regularization parameter</li><li><code>X</code> the current gradient</li><li><code>Lanczos_vectors</code> – the obtained Lanczos vectors</li><li><code>tridig_matrix</code> the tridiagonal coefficient matrix T</li><li><code>coefficients</code> the coefficients <code>y_1,...y_k</code>` that determine the solution</li><li><code>Hp</code> – a temporary vector containing the evaluation of the Hessian</li><li><code>Hp_residual</code> – a temporary vector containing the residual to the Hessian</li><li><code>S</code> – the current obtained / approximated solution</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/c4bc49dc6c028c8c6bf994d3e053c9c8c1098a11/src/solvers/adaptive_regularization_with_cubics.jl#L508-L525">source</a></section></article><h2 id="(Conjugate)-Gradient-Descent"><a class="docs-heading-anchor" href="#(Conjugate)-Gradient-Descent">(Conjugate) Gradient Descent</a><a id="(Conjugate)-Gradient-Descent-1"></a><a class="docs-heading-anchor-permalink" href="#(Conjugate)-Gradient-Descent" title="Permalink"></a></h2><p>There are two generic functors, that implement the sub problem</p><article class="docstring"><header><a class="docstring-binding" id="Manopt.AdaptiveRegularizationCubicCost" href="#Manopt.AdaptiveRegularizationCubicCost"><code>Manopt.AdaptiveRegularizationCubicCost</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaptiveRegularizationCubicCost</code></pre><p>We define the model <span>$m(X)$</span> in the tangent space of the current iterate <span>$p=p_k$</span> as</p><p class="math-container">\[    m(X) = f(p) + &lt;X, \operatorname{grad}f(p)&gt;
      + \frac{1}{2} &lt;X, \operatorname{Hess} f(p)[X]&gt; +  \frac{σ}{3} \lVert X \rVert^3\]</p><p><strong>Fields</strong></p><ul><li><code>mho</code> – an <a href="../../plans/objective/#Manopt.AbstractManifoldObjective"><code>AbstractManifoldObjective</code></a> that should provide at least <a href="../../plans/objective/#Manopt.get_cost"><code>get_cost</code></a>, <a href="../../plans/objective/#Manopt.get_gradient"><code>get_gradient</code></a> and <a href="../../plans/objective/#Manopt.get_hessian"><code>get_hessian</code></a>.</li><li><code>σ</code> – the current regularization parameter</li><li><code>X</code> – a storage for the gradient at <code>p</code> of the original cost</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">AdaptiveRegularizationCubicCost(mho, σ, X)
AdaptiveRegularizationCubicCost(M, mho, σ; p=rand(M), X=get_gradient(M, mho, p))</code></pre><p>Initialize the cubic cost to the objective <code>mho</code>, regularization parameter <code>σ</code>, and (temporary) gradient <code>X</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>For this gradient function to work, we require the <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/vector_bundle.html#Manifolds.TangentSpaceAtPoint"><code>TangentSpaceAtPoint</code></a> from <code>Manifolds.jl</code></p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/c4bc49dc6c028c8c6bf994d3e053c9c8c1098a11/src/plans/adabtive_regularization_with_cubics_plan.jl#L1-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.AdaptiveRegularizationCubicGrad" href="#Manopt.AdaptiveRegularizationCubicGrad"><code>Manopt.AdaptiveRegularizationCubicGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaptiveRegularizationCubicGrad</code></pre><p>We define the model <span>$m(X)$</span> in the tangent space of the current iterate <span>$p=p_k$</span> as</p><p class="math-container">\[    m(X) = f(p) + &lt;X, \operatorname{grad}f(p)&gt;
      + \frac{1}{2} &lt;X, \operatorname{Hess} f(p)[X]&gt; +  \frac{σ}{3} \lVert X \rVert^3\]</p><p>This struct represents its gradient, given by</p><p class="math-container">\[    \operatorname{grad} m(X) = \operatorname{grad}f(p) + \operatorname{Hess} f(p)[X] + σ \lVert X \rVert X\]</p><p><strong>Fields</strong></p><ul><li><code>mho</code> – an <a href="../../plans/objective/#Manopt.AbstractManifoldObjective"><code>AbstractManifoldObjective</code></a> that should provide at least <a href="../../plans/objective/#Manopt.get_cost"><code>get_cost</code></a>, <a href="../../plans/objective/#Manopt.get_gradient"><code>get_gradient</code></a> and <a href="../../plans/objective/#Manopt.get_hessian"><code>get_hessian</code></a>.</li><li><code>σ</code> – the current regularization parameter</li><li><code>X</code> – a storage for the gradient at <code>p</code> of the original cost</li></ul><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">AdaptiveRegularizationCubicGrad(mho, σ, X)
AdaptiveRegularizationCubicGrad(M, mho, σ; p=rand(M), X=get_gradient(M, mho, p))</code></pre><p>Initialize the cubic cost to the original objective <code>mho</code>, regularization parameter <code>σ</code>, and (temporary) gradient <code>X</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><ul><li>For this gradient function to work, we require the <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/vector_bundle.html#Manifolds.TangentSpaceAtPoint"><code>TangentSpaceAtPoint</code></a></li></ul><p>from <code>Manifolds.jl</code></p><ul><li>The gradient functor provides both an allocating as well as an in-place variant.</li></ul></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/c4bc49dc6c028c8c6bf994d3e053c9c8c1098a11/src/plans/adabtive_regularization_with_cubics_plan.jl#L47-L81">source</a></section></article><p>Since the sub problem is given on the tangent space, you have to provide</p><pre><code class="nohighlight hljs">g = AdaptiveRegularizationCubicCost(M, mho, σ)
grad_g = AdaptiveRegularizationCubicGrad(M, mho, σ)
sub_problem = DefaultProblem(TangentSpaceAt(M,p), ManifoldGradienObjective(g, grad_g))</code></pre><p>where <code>mho</code> is the hessian objective of <code>f</code> to solve. Then use this for the <code>sub_problem</code> keyword and use your favourite gradient based solver for the <code>sub_state</code> keyword, for example a <a href="../conjugate_gradient_descent/#Manopt.ConjugateGradientDescentState"><code>ConjugateGradientDescentState</code></a></p><h2 id="Additional-Stopping-Criteria"><a class="docs-heading-anchor" href="#Additional-Stopping-Criteria">Additional Stopping Criteria</a><a id="Additional-Stopping-Criteria-1"></a><a class="docs-heading-anchor-permalink" href="#Additional-Stopping-Criteria" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenAllLanczosVectorsUsed" href="#Manopt.StopWhenAllLanczosVectorsUsed"><code>Manopt.StopWhenAllLanczosVectorsUsed</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopWhenAllLanczosVectorsUsed &lt;: StoppingCriterion</code></pre><p>When an inner iteration has used up all Lanczos vectors, then this stopping criterion is a fallback / security stopping criterion in order to not access a non-existing field in the array allocated for vectors.</p><p>Note that this stopping criterion (for now) is only implemented for the case that an <a href="#Manopt.AdaptiveRegularizationState"><code>AdaptiveRegularizationState</code></a> when using a <a href="#Manopt.LanczosState"><code>LanczosState</code></a> subsolver</p><p><strong>Fields</strong></p><ul><li><code>maxLanczosVectors</code> – maximal number of Lanczos vectors</li><li><code>reason</code> – a String indicating the reason if the criterion indicated to stop</li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">StopWhenAllLanczosVectorsUsed(maxLancosVectors::Int)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/c4bc49dc6c028c8c6bf994d3e053c9c8c1098a11/src/solvers/adaptive_regularization_with_cubics.jl#L793-L812">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Manopt.StopWhenFirstOrderProgress" href="#Manopt.StopWhenFirstOrderProgress"><code>Manopt.StopWhenFirstOrderProgress</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StopWhenFirstOrderProgress &lt;: StoppingCriterion</code></pre><p>A stopping criterion related to the Riemannian adaptive regularization with cubics (ARC) solver indicating that the model function at the current (outer) iterate, i.e.</p><p class="math-container">\[    m(X) = f(p) + &lt;X, \operatorname{grad}f(p)&gt;
      + \frac{1}{2} &lt;X, \operatorname{Hess} f(p)[X]&gt; +  \frac{σ}{3} \lVert X \rVert^3,\]</p><p>defined on the tangent space <span>$T_{p}\mathcal M$</span> fulfills at the current iterate <span>$X_k$</span> that</p><p class="math-container">\[m(X_k) \leq m(0)
\quad\text{ and }\quad
\lVert \operatorname{grad} m(X_k) \rVert ≤ θ \lVert X_k \rVert^2\]</p><p><strong>Fields</strong></p><ul><li><code>θ</code> – the factor <span>$θ$</span> in the second condition above</li><li><code>reason</code> – a String indicating the reason if the criterion indicated to stop</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/c4bc49dc6c028c8c6bf994d3e053c9c8c1098a11/src/solvers/adaptive_regularization_with_cubics.jl#L729-L753">source</a></section></article><h2 id="Literature"><a class="docs-heading-anchor" href="#Literature">Literature</a><a id="Literature-1"></a><a class="docs-heading-anchor-permalink" href="#Literature" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[ABBC20]</dt>
<dd>
<div id="AgarwalBoumalBullinsCartis:2020">N. Agarwal, N. Boumal, B. Bullins and C. Cartis. <i>Adaptive regularization with cubics on manifolds</i>. <a href='https://doi.org/10.1007/s10107-020-01505-1'>Mathematical Programming (2020)</a>.</div>
</dd>
</dl></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Introduction</a><a class="docs-footer-nextpage" href="../alternating_gradient_descent/">Alternating Gradient Descent »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Monday 9 October 2023 21:14">Monday 9 October 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
