var documenterSearchIndex = {"docs":
[{"location":"solvers/NelderMead.html#NelderMeadSolver-1","page":"Nelder–Mead","title":"Nelder Mead Method","text":"","category":"section"},{"location":"solvers/NelderMead.html#","page":"Nelder–Mead","title":"Nelder–Mead","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/NelderMead.html#","page":"Nelder–Mead","title":"Nelder–Mead","text":"    NelderMead\n    NelderMead!","category":"page"},{"location":"solvers/NelderMead.html#Manopt.NelderMead","page":"Nelder–Mead","title":"Manopt.NelderMead","text":"NelderMead(M, F [, p])\n\nperform a nelder mead minimization problem for the cost function F on the manifold M. If the initial population p is not given, a random set of points is chosen.\n\nThis algorithm is adapted from the Euclidean Nelder-Mead method, see https://en.wikipedia.org/wiki/Nelder–Mead_method and http://www.optimization-online.org/DB_FILE/2007/08/1742.pdf.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fmathcal Mℝ to minimize\npopulation – (n+1 random_point(M)) an initial population of n+1 points, where n is the dimension of the manifold M.\n\nOptional\n\nstopping_criterion – (StopAfterIteration(2000)) a StoppingCriterion\nα – (1.) reflection parameter (α  0)\nγ – (2.) expansion parameter (γ)\nρ – (1/2) contraction parameter, 0  ρ  frac12,\nσ – (1/2) shrink coefficient, 0  σ  1\nretraction_method – (default_retraction_method(M)) the rectraction to use\ninverse_retraction_method - (default_inverse_retraction_method(M)) an inverse retraction to use.\n\nand the ones that are passed to decorate_options for decorators.\n\nnote: Note\nThe manifold M used here has to either provide a mean(M, pts) or you have to load Manifolds.jl to use its statistics part.\n\nOutput\n\neither x the last iterate or the complete options depending on the optional keyword return_options, which is false by default (hence then only x is returned).\n\n\n\n\n\n","category":"function"},{"location":"solvers/NelderMead.html#Manopt.NelderMead!","page":"Nelder–Mead","title":"Manopt.NelderMead!","text":"NelderMead(M, F [, p])\n\nperform a Nelder Mead minimization problem for the cost function F on the manifold M. If the initial population p is not given, a random set of points is chosen. If it is given, the computation is done in place of p.\n\nFor more options see NelderMead.\n\n\n\n\n\n","category":"function"},{"location":"solvers/NelderMead.html#Options-1","page":"Nelder–Mead","title":"Options","text":"","category":"section"},{"location":"solvers/NelderMead.html#","page":"Nelder–Mead","title":"Nelder–Mead","text":"    NelderMeadOptions","category":"page"},{"location":"solvers/NelderMead.html#Manopt.NelderMeadOptions","page":"Nelder–Mead","title":"Manopt.NelderMeadOptions","text":"NelderMeadOptions <: Options\n\nDescribes all parameters and the state of a Nealer-Mead heuristic based optimization algorithm.\n\nFields\n\nThe naming of these parameters follows the Wikipedia article of the Euclidean case. The default is given in brackets, the required value range after the description\n\npopulation – an Array{point,1} of n+1 points x_i, i=1n+1, where n is the dimension of the manifold.\nstopping_criterion – (StopAfterIteration(2000)) a StoppingCriterion\nα – (1.) reflection parameter (α  0)\nγ – (2.) expansion parameter (γ  0)\nρ – (1/2) contraction parameter, 0  ρ  frac12,\nσ – (1/2) shrink coefficient, 0  σ  1\nx – (p[1]) - a field to collect the current best value\nretraction_method – ExponentialRetraction() the rectraction to use, defaults to the exponential map\ninverse_retraction_method - LogarithmicInverseRetraction an inverse_retraction(M,x,y) to use.\n\nConstructors\n\nNelderMead(M,stop, retr; α=1. , γ=2., ρ=1/2, σ=1/2)\n\nconstruct a Nelder-Mead Option with a set of dimension(M)+1 random points.\n\nNelderMead(p, stop retr; α=1. , γ=2., ρ=1/2, σ=1/2)\n\nconstruct a Nelder-Mead Option with a set p of points\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#CPPSolver-1","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"The Cyclic Proximal Point (CPP) algorithm is a Proximal Problem.","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"It aims to minimize","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"F(x) = sum_i=1^c f_i(x)","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"assuming that the proximal maps operatornameprox_λ f_i(x) are given in closed form or can be computed efficiently (at least approximately).","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"The algorithm then cycles through these proximal maps, where the type of cycle might differ and the proximal parameter λ_k changes after each cycle k.","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"For a convergence result on Hadamard manifolds see [Bačák, 2014].","category":"page"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"cyclic_proximal_point\ncyclic_proximal_point!","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.cyclic_proximal_point","page":"Cyclic Proximal Point","title":"Manopt.cyclic_proximal_point","text":"cyclic_proximal_point(M, F, proxes, x)\n\nperform a cyclic proximal point algorithm.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fmathcal Mℝ to minimize\nproxes – an Array of proximal maps (Functions) (λ,x) -> y for the summands of F\nx – an initial value x  mathcal M\n\nOptional\n\nthe default values are given in brackets\n\nevaluation – (AllocatingEvaluation) specify whether the proximal maps work by allocation (default) form prox(M, λ, x) or MutatingEvaluation in place, i.e. is of the form prox!(M, y, λ, x).\nevaluation_order – (:Linear) – whether to use a randomly permuted sequence (:FixedRandom), a per cycle permuted sequence (Random) or the default linear one.\nλ – ( iter -> 1/iter ) a function returning the (square summable but not summable) sequence of λi\nstopping_criterion – (StopWhenAny(StopAfterIteration(5000),StopWhenChangeLess(10.0^-8))) a StoppingCriterion.\nreturn_options – (false) – if activated, the extended result, i.e. the complete Options are returned. This can be used to access recorded values. If set to false (default) just the optimal value x_opt if returned\n\nand the ones that are passed to decorate_options for decorators.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/cyclic_proximal_point.html#Manopt.cyclic_proximal_point!","page":"Cyclic Proximal Point","title":"Manopt.cyclic_proximal_point!","text":"cyclic_proximal_point!(M, F, proxes, x)\n\nperform a cyclic proximal point algorithm in place of x.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fmathcal Mℝ to minimize\nproxes – an Array of proximal maps (Functions) (λ,x) -> y for the summands of F\nx – an initial value x  mathcal M\n\nfor all options, see cyclic_proximal_point.\n\n\n\n\n\n","category":"function"},{"location":"solvers/cyclic_proximal_point.html#Options-1","page":"Cyclic Proximal Point","title":"Options","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"CyclicProximalPointOptions","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.CyclicProximalPointOptions","page":"Cyclic Proximal Point","title":"Manopt.CyclicProximalPointOptions","text":"CyclicProximalPointOptions <: Options\n\nstores options for the cyclic_proximal_point algorithm. These are the\n\nFields\n\nx – the current iterate\nstopping_criterion – a StoppingCriterion\nλ – (@(iter) -> 1/iter) a function for the values of λ_k per iteration/cycle\nevaluation_order – (:LinearOrder) – whether to use a randomly permuted sequence (:FixedRandomOrder), a per cycle permuted sequence (RandomOrder) or the default linear one.\n\nSee also\n\ncyclic_proximal_point\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#Debug-Functions-1","page":"Cyclic Proximal Point","title":"Debug Functions","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"DebugProximalParameter","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.DebugProximalParameter","page":"Cyclic Proximal Point","title":"Manopt.DebugProximalParameter","text":"DebugProximalParameter <: DebugAction\n\nprint the current iterates proximal point algorithm parameter given by Optionss o.λ.\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#Record-Functions-1","page":"Cyclic Proximal Point","title":"Record Functions","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"RecordProximalParameter","category":"page"},{"location":"solvers/cyclic_proximal_point.html#Manopt.RecordProximalParameter","page":"Cyclic Proximal Point","title":"Manopt.RecordProximalParameter","text":"RecordProximalParameter <: RecordAction\n\nrecoed the current iterates proximal point algorithm parameter given by in Optionss o.λ.\n\n\n\n\n\n","category":"type"},{"location":"solvers/cyclic_proximal_point.html#Literature-1","page":"Cyclic Proximal Point","title":"Literature","text":"","category":"section"},{"location":"solvers/cyclic_proximal_point.html#","page":"Cyclic Proximal Point","title":"Cyclic Proximal Point","text":"<ul>\n<li id=\"Bačák2014\">[<a>Bačák, 2014</a>]\n  Bačák, M: <emph>Computing Medians and Means in Hadamard Spaces.</emph>,\n  SIAM Journal on Optimization, Volume 24, Number 3, pp. 1542–1566,\n  doi: <a href=\"https://doi.org/10.1137/140953393\">10.1137/140953393</a>,\n  arxiv: <a href=\"https://arxiv.org/abs/1210.2145\">1210.2145</a>.\n  </li>\n</ul>","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/StochasticGradientDescent.jl\"","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#SGDTutorial-1","page":"do stochastic gradient descent","title":"Stochastic Gradient Descent","text":"","category":"section"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"This tutorial illustrates how to use the stochastic_gradient_descent solver and different DirectionUpdateRules in order to introduce the average or momentum variant, see Stochastic Gradient Descent.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Computationally we look at a very simple but large scale problem, the Riemannian Center of Mass or Fréchet mean: For given points p_i mathcal M, i=1N this optimization problem reads","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"operatorname*argmin_xmathcal M frac12sum_i=1^N\n  operatornamed^2_mathcal M(xp_i)","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"which of course can be (and is) solved by a gradient descent, see the introductionary tutorial. If N is very large it might be quite expensive to evaluate the complete gradient. A remedy is, to evaluate only one of the terms at a time and choose a random order for these.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"We first initialize the manifold (see [])","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"using Manopt, Manifolds, Random, Colors","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"black = RGBA{Float64}(colorant\"#000000\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\") # Start\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\") # a path\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\") # points\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"And optain a large data set","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"n = 5000\nσ = π / 12\nM = Sphere(2)\nx = 1 / sqrt(2) * [1.0, 0.0, 1.0]\nRandom.seed!(42)\ndata = [exp(M, x, random_tangent(M, x, Val(:Gaussian), σ)) for i in 1:n]\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"which looks like","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"asymptote_export_S2_signals(\"centerAndLargeData.asy\";\n    points = [ [x], data],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal]),\n    dot_sizes = [2.5, 1.0], camera_position = (1.,.5,.5)\n)\nrender_asymptote(\"centerAndLargeData.asy\"; render = 2)","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"(Image: The data of noisy versions of $x$)","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Note that due to the construction of the points as zero mean tangent vectors, the mean should be very close to our initial point x.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"In order to use the stochastic gradient, we now need a function that returns the vector of gradients. There are two ways to define it in Manopt.jl: as one function, that returns a vector or a vector of funtions.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"The first variant is of course easier to define, but the second is more efficient when only evaluating one of the gradients. For the mean we have as a gradient","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":" gradF(x) = sum_i=1^N operatornamegradf_i(x) quad textwhere operatornamegradf_i(x) = -log_x p_i","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Which we define as","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"F(M, x) = 1 / (2 * n) * sum(map(p -> distance(M, x, p)^2, data))\ngradF(M, x) = [grad_distance(M, p, x) for p in data]\ngradf = [(M, x) -> grad_distance(M, p, x) for p in data];\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"The calls are only slightly different, but notice that accessing the 2nd gradient element requires evaluating all logs in the first function. So while you can use both gradF and gradf in the following call, the second one is faster:","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt1 = stochastic_gradient_descent(M, gradF, x);\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"versus","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt2 = stochastic_gradient_descent(M, gradf, x);\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"This result is reasonably close. But we can improve it by using a DirectionUpdateRule, namely: On the one hand MomentumGradient, which requires both the manifold and the initial value,    in order to keep track of the iterate and parallel transport the last direction to the current iterate.    you can also set a vector_transport_method, if ParallelTransport() is not    available on your manifold. Here we simply do","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt3 = stochastic_gradient_descent(\n    M, gradf, x; direction=MomentumGradient(M, x, StochasticGradient(zero_vector(M, x)))\n);\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"And on the other hand the AverageGradient computes an average of the last n gradients, i.e.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt4 = stochastic_gradient_descent(\n    M, gradf, x; direction=AverageGradient(M, x, 10, StochasticGradient(zero_vector(M, x)))\n);\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"note that the default StoppingCriterion is a fixed number of iterations.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Note that since you can apply both also in case of gradient_descent, i.e. to use IdentityUpdateRule and evaluate the classical gradient, both constructors have to know that internally the default evaluation of the Stochastic gradient (choosing one gradient operatornamegradf_k at random) has to be specified.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"For this small example you can of course also use a gradient descent with ArmijoLinesearch, but it will be a little slower usually","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"@time x_opt5 = gradient_descent(\n    M, F, (M, x) -> sum(gradF(M, x)), x; stepsize=ArmijoLinesearch()\n);\nnothing #hide","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"but it is for sure faster than the variant above that evaluates the full gradient on every iteration, since stochastic gradient descent takes more iterations.","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"Note that all 5 of couse yield the same result","category":"page"},{"location":"tutorials/StochasticGradientDescent.html#","page":"do stochastic gradient descent","title":"do stochastic gradient descent","text":"[distance(M, x, y) for y in [x_opt1, x_opt2, x_opt3, x_opt4, x_opt5]]","category":"page"},{"location":"functions/costs.html#CostFunctions-1","page":"Cost functions","title":"Cost Functions","text":"","category":"section"},{"location":"functions/costs.html#","page":"Cost functions","title":"Cost functions","text":"The following cost functions are available","category":"page"},{"location":"functions/costs.html#","page":"Cost functions","title":"Cost functions","text":"Modules = [Manopt]\nPages   = [\"costs.jl\"]","category":"page"},{"location":"functions/costs.html#Manopt.costIntrICTV12-Tuple{AbstractManifold, Any, Any, Any, Any, Any}","page":"Cost functions","title":"Manopt.costIntrICTV12","text":"costIntrICTV12(M, f, u, v, α, β)\n\nCompute the intrinsic infimal convolution model, where the addition is replaced by a mid point approach and the two functions involved are costTV2 and costTV. The model reads\n\nE(uv) =\n  frac12sum_i  mathcal G\n    d_mathcal Mbigl(g(frac12v_iw_i)f_ibigr)\n  +alphabigl( βmathrmTV(v) + (1-β)mathrmTV_2(w) bigr)\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costL2TV-NTuple{4, Any}","page":"Cost functions","title":"Manopt.costL2TV","text":"costL2TV(M, f, α, x)\n\ncompute the ℓ^2-TV functional on the PowerManifold manifoldMfor given (fixed) dataf(onM), a nonnegative weightα, and evaluated atx(onM`), i.e.\n\nE(x) = d_mathcal M^2(fx) + alpha operatornameTV(x)\n\nSee also\n\ncostTV\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costL2TV2-Tuple{PowerManifold, Any, Any, Any}","page":"Cost functions","title":"Manopt.costL2TV2","text":"costL2TV2(M, f, β, x)\n\ncompute the ℓ^2-TV2 functional on the PowerManifold manifold M for given data f, nonnegative parameter β, and evaluated at x, i.e.\n\nE(x) = d_mathcal M^2(fx) + βoperatornameTV_2(x)\n\nSee also\n\ncostTV2\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costL2TVTV2-Tuple{PowerManifold, Any, Any, Any, Any}","page":"Cost functions","title":"Manopt.costL2TVTV2","text":"costL2TVTV2(M, f, α, β, x)\n\ncompute the ℓ^2-TV-TV2 functional on the PowerManifold manifold M for given (fixed) data f (on M), nonnegative weight α, β, and evaluated at x (on M), i.e.\n\nE(x) = d_mathcal M^2(fx) + alphaoperatornameTV(x)\n  + βoperatornameTV_2(x)\n\nSee also\n\ncostTV, costTV2\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costTV","page":"Cost functions","title":"Manopt.costTV","text":"costTV(M,x [,p=2,q=1])\n\nCompute the operatornameTV^p functional for data xon the PowerManifold manifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i denote the forward neighbors, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I_i = i+e_j j=1kcap mathcal G. The formula reads\n\nE^q(x) = sum_i  mathcal G\n  bigl( sum_j   mathcal I_i d^p_mathcal M(x_ix_j) bigr)^qp\n\nSee also\n\ngrad_TV, prox_TV\n\n\n\n\n\n","category":"function"},{"location":"functions/costs.html#Manopt.costTV-Union{Tuple{T}, Tuple{AbstractManifold, Tuple{T, T}}, Tuple{AbstractManifold, Tuple{T, T}, Int64}} where T","page":"Cost functions","title":"Manopt.costTV","text":"costTV(M, x, p)\n\nCompute the operatornameTV^p functional for a tuple pT of pointss on a Manifold M, i.e.\n\nE(x_1x_2) = d_mathcal M^p(x_1x_2) quad x_1x_2  mathcal M\n\nSee also\n\ngrad_TV, prox_TV\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.costTV2","page":"Cost functions","title":"Manopt.costTV2","text":"costTV2(M,x [,p=1])\n\ncompute the operatornameTV_2^p functional for data x on the PowerManifold manifoldmanifold M, i.e. mathcal M = mathcal N^n, where n  mathbb N^k denotes the dimensions of the data x. Let mathcal I_i^pm denote the forward and backward neighbors, respectively, i.e. with mathcal G as all indices from mathbf1  mathbb N^k to n we have mathcal I^pm_i = ipm e_j j=1kcap mathcal I. The formula then reads\n\nE(x) = sum_i  mathcal I j_1   mathcal I^+_i j_2   mathcal I^-_i\nd^p_mathcal M(c_i(x_j_1x_j_2) x_i)\n\nwhere c_i() denotes the mid point between its two arguments that is nearest to x_i.\n\nSee also\n\ngrad_TV2, prox_TV2\n\n\n\n\n\n","category":"function"},{"location":"functions/costs.html#Manopt.costTV2-Union{Tuple{T}, Tuple{MT}, Tuple{MT, Tuple{T, T, T}}, Tuple{MT, Tuple{T, T, T}, Any}} where {MT<:AbstractManifold, T}","page":"Cost functions","title":"Manopt.costTV2","text":"costTV2(M,(x1,x2,x3) [,p=1])\n\nCompute the operatornameTV_2^p functional for the 3-tuple of points (x1,x2,x3)on the Manifold M. Denote by\n\n  mathcal C = bigl c   mathcal M   g(tfrac12x_1x_3) text for some geodesic gbigr\n\nthe set of mid points between x_1 and x_3. Then the function reads\n\nd_2^p(x_1x_2x_3) = min_c  mathcal C d_mathcal M(cx_2)\n\nSee also\n\ngrad_TV2, prox_TV2\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.cost_L2_acceleration_bezier-Union{Tuple{P}, Tuple{AbstractManifold, AbstractVector{P}, AbstractVector{var\"#s53\"} where var\"#s53\"<:Integer, AbstractVector{var\"#s52\"} where var\"#s52\"<:AbstractFloat, AbstractFloat, AbstractVector{P}}} where P","page":"Cost functions","title":"Manopt.cost_L2_acceleration_bezier","text":"cost_L2_acceleration_bezier(M,B,pts,λ,d)\n\ncompute the value of the discrete Acceleration of the composite Bezier curve together with a data term, i.e.\n\nfracλ2sum_i=0^N d_mathcal M(d_i c_B(i))^2+\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i and d_2 refers to the second order absolute difference costTV2 (squared), the junction points are denoted by p_i, and to each p_i corresponds one data item in the manifold points given in d. For details on the acceleration approximation, see cost_acceleration_bezier. Note that the Beziér-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nSee also\n\ngrad_L2_acceleration_bezier, cost_acceleration_bezier, grad_acceleration_bezier\n\n\n\n\n\n","category":"method"},{"location":"functions/costs.html#Manopt.cost_acceleration_bezier-Union{Tuple{P}, Tuple{AbstractManifold, AbstractVector{P}, AbstractVector{var\"#s53\"} where var\"#s53\"<:Integer, AbstractVector{var\"#s52\"} where var\"#s52\"<:AbstractFloat}} where P","page":"Cost functions","title":"Manopt.cost_acceleration_bezier","text":"cost_acceleration_bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector{<:AbstractFloat},\n) where {P}\n\ncompute the value of the discrete Acceleration of the composite Bezier curve\n\nsum_i=1^N-1fracd^2_2  B(t_i-1) B(t_i) B(t_i+1)Delta_t^3\n\nwhere for this formula the pts along the curve are equispaced and denoted by t_i, i=1N, and d_2 refers to the second order absolute difference costTV2 (squared). Note that the Beziér-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nThis acceleration discretization was introduced in[BergmannGousenbourger2018].\n\nSee also\n\ngrad_acceleration_bezier, cost_L2_acceleration_bezier, grad_L2_acceleration_bezier\n\n[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics (2018). doi 10.3389/fams.2018.00059, arXiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/CONTRIBUTING.md\"","category":"page"},{"location":"contributing.html#Contributing-to-Manopt.jl-1","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"","category":"section"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"First, thanks for taking the time to contribute. Any contribution is appreciated and welcome.","category":"page"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"The following is a set of guidelines to Manopt.jl.","category":"page"},{"location":"contributing.html#Table-of-Contents-1","page":"Contributing to Manopt.jl","title":"Table of Contents","text":"","category":"section"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"Contributing to Manopt.jl     - Table of Contents\nI just have a question\nHow can I file an issue?\nHow can I contribute?\nAdd a missing method\nProvide a new algorithm\nProvide a new example\nCode style","category":"page"},{"location":"contributing.html#I-just-have-a-question-1","page":"Contributing to Manopt.jl","title":"I just have a question","text":"","category":"section"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"The developer can most easily be reached in the Julia Slack channel #manifolds. You can apply for the Julia Slack workspace here if you haven't joined yet. You can also ask your question on discourse.julialang.org.","category":"page"},{"location":"contributing.html#How-can-I-file-an-issue?-1","page":"Contributing to Manopt.jl","title":"How can I file an issue?","text":"","category":"section"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"If you found a bug or want to propose a feature, we track our issues within the GitHub repository.","category":"page"},{"location":"contributing.html#How-can-I-contribute?-1","page":"Contributing to Manopt.jl","title":"How can I contribute?","text":"","category":"section"},{"location":"contributing.html#Add-a-missing-method-1","page":"Contributing to Manopt.jl","title":"Add a missing method","text":"","category":"section"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"There is still a lot of methods for within the optimisation framework of  Manopt.jl, may it be functions, gradients, differentials, proximal maps, step size rules or stopping criteria. If you notice a method missing and can contribute an implementation, please do so! Even providing a single new method is a good contribution.","category":"page"},{"location":"contributing.html#Provide-a-new-algorithm-1","page":"Contributing to Manopt.jl","title":"Provide a new algorithm","text":"","category":"section"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"A main contribution you can provide is another algorithm that is not yet included in the package. An alorithm is always based on a is a concrete type of a Problem storing the main information of the task and a concrete type of an Option storing all information that needs to be known to the solver in general. The actual algorithm is split into an initialization phase, see initialize_solver!, and the implementation of the ith step of the sovler itself, see  before the iterative procedure, see step_solver!. For these two functions it would be great if a new algorithm uses functions from the ManifoldsBase.jl interface as generic as possible. For example, if possible use retract!(M,q,p,X) in favour of exp!(M,q,p,X) to perform a step starting in p in direction X (in place of q), since the exponential map might be too expensive to evaluate or might not be available on a certain manifold. See Retractions and inverse retractions for more details. Further, if possible, prefer retract!(M,q,p,X) in favour of retract(M,p,X), since a computation in place of a suitable variable q reduces memory allocations.","category":"page"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"Usually, the methods implemented in Manopt.jl also have a high-level interface, that is easier to call, creates the necessary problem and options structure and calls the solver.","category":"page"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"The two technical functions initialize_solver! and step_solver! should be documented with technical details, while the high level interface should usually provide a general description and some literature references to the algorithm at hand.","category":"page"},{"location":"contributing.html#Provide-a-new-example-1","page":"Contributing to Manopt.jl","title":"Provide a new example","text":"","category":"section"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"The examples/ folder features several examples covering all solvers. Still, if you have a new example that you implemented yourself for fun or for a paper, feel free to add it to the repository as well. Also if you have a Pluto notebook of your example, feel free to contribute that.","category":"page"},{"location":"contributing.html#Code-style-1","page":"Contributing to Manopt.jl","title":"Code style","text":"","category":"section"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"We try to follow the documentation guidelines from the Julia documentation as well as Blue Style. We run JuliaFormatter.jl on the repo in the way set in the .JuliaFormatter.toml file, which enforces a number of conventions consistent with the Blue Style.","category":"page"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"We also follow a few internal conventions:","category":"page"},{"location":"contributing.html#","page":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","text":"It is preferred that the Problem's struct contains information about the general structure of the problem\nAny implemented function should be accompanied by its mathematical formulae if a closed form exists.\nProblem and option structures are stored within the plan/ folder and sorted by properties of the problem and/or solver at hand\nWithin the source code of one algorithm, the high level interface should be first, then the initialisation, then the step.\nOtherwise an alphabetical order is preferrable.\nThe above implies that the mutating variant of a function follows the non-mutating variant.\nThere should be no dangling = signs.\nAlways add a newline between things of different types (struct/method/const).\nAlways add a newline between methods for different functions (including mutating/nonmutating variants).\nPrefer to have no newline between methods for the same function; when reasonable, merge the docstrings.\nAll import/using/include should be in the main module file.","category":"page"},{"location":"solvers/DouglasRachford.html#DRSolver-1","page":"Douglas–Rachford","title":"Douglas–Rachford Algorithm","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"The (Parallel) Douglas–Rachford ((P)DR) Algorithm was generalized to Hadamard manifolds in [Bergmann, Persch, Steidl, 2016].","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"The aim is to minimize the sum","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"F(x) = f(x) + g(x)","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"on a manifold, where the two summands have proximal maps operatornameprox_λ f operatornameprox_λ g that are easy to evaluate (maybe in closed form or not too costly to approximate). Further define the Reflection operator at the proximal map as","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"operatornamerefl_λ f(x) = exp_operatornameprox_λ f(x) bigl( -log_operatornameprox_λ f(x) x bigr)","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":".","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Let alpha_k   01 with sum_k  mathbb N alpha_k(1-alpha_k) =   fty and λ  0 which might depend on iteration k as well) be given.","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Then the (P)DRA algorithm for initial data x_0  mathcal H as","category":"page"},{"location":"solvers/DouglasRachford.html#Initialization-1","page":"Douglas–Rachford","title":"Initialization","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Initialize t_0 = x_0 and k=0","category":"page"},{"location":"solvers/DouglasRachford.html#Iteration-1","page":"Douglas–Rachford","title":"Iteration","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Repeat  until a convergence criterion is reached","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"Compute s_k = operatornamerefl_λ foperatornamerefl_λ g(t_k)\nwithin that operation store x_k+1 = operatornameprox_λ g(t_k) which is the prox the inner reflection reflects at.\nCompute t_k+1 = g(alpha_k t_k s_k)\nSet k = k+1","category":"page"},{"location":"solvers/DouglasRachford.html#Result-1","page":"Douglas–Rachford","title":"Result","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"The result is given by the last computed x_K.","category":"page"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"For the parallel version, the first proximal map is a vectorial version, where in each component one prox is applied to the corresponding copy of t_k and the second proximal map corresponds to the indicator function of the set, where all copies are equal (in mathcal H^n, where n is the number of copies), leading to the second prox being the Riemannian mean.","category":"page"},{"location":"solvers/DouglasRachford.html#Interface-1","page":"Douglas–Rachford","title":"Interface","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"  DouglasRachford\n  DouglasRachford!","category":"page"},{"location":"solvers/DouglasRachford.html#Manopt.DouglasRachford","page":"Douglas–Rachford","title":"Manopt.DouglasRachford","text":" DouglasRachford(M, F, proxMaps, x)\n\nComputes the Douglas-Rachford algorithm on the manifold mathcal M, initial data x_0 and the (two) proximal maps proxMaps.\n\nFor k2 proximal maps the problem is reformulated using the parallelDouglasRachford: a vectorial proximal map on the power manifold mathcal M^k and the proximal map of the set that identifies all entries again, i.e. the Karcher mean. This henve also boild down to two proximal maps, though each evauates proximal maps in parallel, i.e. component wise in a vector.\n\nInput\n\nM – a Riemannian Manifold mathcal M\nF – a cost function consisting of a sum of cost functions\nproxes – functions of the form (λ,x)->... performing a proximal map, where ⁠λ denotes the proximal parameter, for each of the summands of F.\nx0 – initial data x_0  mathcal M\n\nOptional values\n\nthe default parameter is given in brackets\n\nevaluation – (AllocatingEvaluation) specify whether the proximal maps work by allocation (default) form prox(M, λ, x) or MutatingEvaluation in place, i.e. is of the form prox!(M, y, λ, x).\nλ – ((iter) -> 1.0) function to provide the value for the proximal parameter during the calls\nα – ((iter) -> 0.9) relaxation of the step from old to new iterate, i.e. t_k+1 = g(α_k t_k s_k), where s_k is the result of the double reflection involved in the DR algorithm\nR – (reflect) method employed in the iteration to perform the reflection of x at the prox p.\nstopping_criterion – (StopWhenAny(StopAfterIteration(200),StopWhenChangeLess(10.0^-5))) a StoppingCriterion.\nparallel – (false) clarify that we are doing a parallel DR, i.e. on a PowerManifold manifold with two proxes. This can be used to trigger parallel Douglas–Rachford if you enter with two proxes. Keep in mind, that a parallel Douglas–Rachford implicitly works on a PowerManifold manifold and its first argument is the result then (assuming all are equal after the second prox.\nreturn_options – (false) – if activated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/DouglasRachford.html#Manopt.DouglasRachford!","page":"Douglas–Rachford","title":"Manopt.DouglasRachford!","text":" DouglasRachford(M, F, proxMaps, x)\n\nComputes the Douglas-Rachford algorithm on the manifold mathcal M, initial data x_0 and the (two) proximal maps proxMaps in place of x.\n\nFor k2 proximal maps the problem is reformulated using the parallelDouglasRachford: a vectorial proximal map on the power manifold mathcal M^k and the proximal map of the set that identifies all entries again, i.e. the Karcher mean. This hence also boils down to two proximal maps, though each evaluates proximal maps in parallel, i.e. component wise in a vector.\n\nInput\n\nM – a Riemannian Manifold mathcal M\nF – a cost function consisting of a sum of cost functions\nproxes – functions of the form (λ,x)->... performing a proximal map, where ⁠λ denotes the proximal parameter, for each of the summands of F.\nx0 – initial data x_0  mathcal M\n\nFor more options, see DouglasRachford.\n\n\n\n\n\n","category":"function"},{"location":"solvers/DouglasRachford.html#Options-1","page":"Douglas–Rachford","title":"Options","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"DouglasRachfordOptions","category":"page"},{"location":"solvers/DouglasRachford.html#Manopt.DouglasRachfordOptions","page":"Douglas–Rachford","title":"Manopt.DouglasRachfordOptions","text":"DouglasRachfordOptions <: Options\n\nStore all options required for the DouglasRachford algorithm,\n\nFields\n\nx - the current iterate (result) For the parallel Douglas-Rachford, this is not a value from the PowerManifold manifold but the mean.\ns – the last result of the double reflection at the proxes relaxed by α.\nλ – ((iter)->1.0) function to provide the value for the proximal parameter during the calls\nα – ((iter)->0.9) relaxation of the step from old to new iterate, i.e. x^(k+1) = g(α(k) x^(k) t^(k)), where t^(k) is the result of the double reflection involved in the DR algorithm\nR – (reflect) method employed in the iteration to perform the reflection of x at the prox p.\nstop – (StopAfterIteration(300)) a StoppingCriterion\nparallel – (false) indicate whether we are running a parallel Douglas-Rachford or not.\n\n\n\n\n\n","category":"type"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"For specific DebugActions and RecordActions see also Cyclic Proximal Point.","category":"page"},{"location":"solvers/DouglasRachford.html#Literature-1","page":"Douglas–Rachford","title":"Literature","text":"","category":"section"},{"location":"solvers/DouglasRachford.html#","page":"Douglas–Rachford","title":"Douglas–Rachford","text":"<ul>\n<li id=\"BergmannPerschSteidl2016\">[<a>Bergmann, Persch, Steidl, 2016</a>]\n  Bergmann, R; Persch, J.; Steidl, G.: <emph>A Parallel Douglas–Rachford\n  Algorithm for Minimizing ROF-like Functionals on Images with Values in\n  Symmetric Hadamard Manifolds.</emph>\n  SIAM Journal on Imaging Sciences, Volume 9, Number 3, pp. 901–937, 2016.\n  doi: <a href=\"https://doi.org/10.1137/15M1052858\">10.1137/15M1052858</a>,\n  arXiv: <a href=\"https://arxiv.org/abs/1512.02814\">1512.02814</a>.\n</li>\n</ul>","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/HowToRecord.jl\"","category":"page"},{"location":"tutorials/HowToRecord.html#RecordingTutorial-1","page":"Record values","title":"Advanced Recording Example","text":"","category":"section"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"The recording and debug possiblities make it possible to record nearly any data during the iterations. This tutorial illustrates how to","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"record one value during the iterations\nrecord multiple values during the iterations and access them afterwards\ndefine an own RecordAction to perform individual recordings.","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"Several predefined recordings exist, for example RecordCost or RecordGradient, depending on the solver used. For fields of the Options this can be directly done using the RecordEntry. For others, an own RecordAction can be defined.","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"We illustrate these using the gradient descent used in the introductionary Get Started: Optimize! tutorial example of computing the Riemannian Center of mass and refer to that tutorial for the mathematical details.","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"using Manopt, Manifolds, Random\nRandom.seed!(42)\nm = 30\nM = Sphere(m)\nn = 800\nσ = π / 8\nx = zeros(Float64, m + 1)\nx[2] = 1.0\ndata = [exp(M, x, random_tangent(M, x, Val(:Gaussian), σ)) for i in 1:n];\nF(M, y) = sum(1 / (2 * n) * distance.(Ref(M), Ref(y), data) .^ 2)\ngradF(M, y) = sum(1 / n * grad_distance.(Ref(M), data, Ref(y)))\nnothing #hide","category":"page"},{"location":"tutorials/HowToRecord.html#Plain-examples-1","page":"Record values","title":"Plain examples","text":"","category":"section"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"For the high level interfaces of the solvers, like gradient_descent we have to set return_options to true to obtain the whole options structure and not only the resulting resulting minimizer.","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"R = gradient_descent(M, F, gradF, data[1]; record=:Cost, return_options=true)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"You can attach different recorders to some operations (:Start. :Stop, :Iteration at time of writing), where :Iteration is the default, so the following is the same as get_record(R, :Iteration). We get","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"get_record(R)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"To record more than one value, you can pass a array of a mix of symbols and RecordAction which gets mapped to a RecordGroup","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"R = gradient_descent(M, F, gradF, data[1]; record=[:Iteration, :Cost], return_options=true)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"Here, the Symbol :Cost is mapped to using the RecordCost action. The same holds for :Iteration and :Iterate and any member field of the current Options. To access these you can first extract the group of records (of the :Iteration action) and then access the :Cost","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"ra = get_record_action(R)[:Cost]","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"Or similarly","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"get_record(R, :Iteration, :Cost)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"Note that the first symbol again refers to the point where we record (not to the thing we record). We can also pass a Tuple as second argument to have our own order (not that now the second :Iteration refers to the recorded iteratons)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"get_record(R, :Iteration, (:Cost, :Iteration))","category":"page"},{"location":"tutorials/HowToRecord.html#A-more-complex-example-1","page":"Record values","title":"A more complex example","text":"","category":"section"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"To illustrate a complicated example let's record","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"the iteration number, cost and gradient field, but only every sixth iteration\nthe iteration at which we stop","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"We first generate the problem and the options","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"p = GradientProblem(M, F, gradF)\no = GradientDescentOptions(\n    M,\n    copy(data[1]);\n    stopping_criterion=StopAfterIteration(200) | StopWhenGradientNormLess(10.0^-9),\n)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"and now decorate these with RecordOptions","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"rI = RecordEvery(\n    RecordGroup([\n        :Iteration => RecordIteration(),\n        :Cost => RecordCost(),\n        :Gradient => RecordEntry(similar(data[1]), :gradient),\n    ]),\n    6,\n)\nsI = RecordIteration()\nr = RecordOptions(o, Dict(:Iteration => rI, :Stop => sI))\nr2 = solve(p, r)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"and we see","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"get_record(r2, :Stop)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"as well as","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"get_record(r2, :Iteration, (:Iteration, :Cost))","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"Here it is interesting to see, that a meta-record like RecordEvery just passes the tuple further on, so we can again also do","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"get_record_action(r2, :Iteration)[:Gradient]","category":"page"},{"location":"tutorials/HowToRecord.html#Writing-an-own-[RecordAction](@ref)s-1","page":"Record values","title":"Writing an own RecordActions","text":"","category":"section"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"Let's investigate where we want to count the number of function evaluations, again just to illustrate, since for the gradient this is just one evaluation per iteration. We first define a cost, that counts it's own calls.","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"mutable struct MyCost{T}\n    data::T\n    count::Int\nend\nMyCost(data::T) where {T} = MyCost{T}(data, 0)\nfunction (c::MyCost)(M, x)\n    c.count += 1\n    return sum(1 / (2 * length(c.data)) * distance.(Ref(M), Ref(x), c.data) .^ 2)\nend\nnothing #hide","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"and we define the following RecordAction,","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"mutable struct RecordCount <: RecordAction\n    recorded_values::Vector{Int}\n    RecordCount() = new(Vector{Int}())\nend\nfunction (r::RecordCount)(p::Problem, ::Options, i)\n    if i > 0\n        push!(r.recorded_values, p.cost.count)\n    elseif i < 0 # reset if negative\n        r.recorded_values = Vector{Int}()\n    end\nend\nnothing #hide","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"And now we can initialize the new cost and call the gradient descent. Note that this illustrates also the last use case – you can pass symbol-Action pairs into the record=array.","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"F2 = MyCost(data)\nR = gradient_descent(\n    M,\n    F2,\n    gradF,\n    data[1];\n    record=[:Iteration, :Count => RecordCount(), :Cost],\n    return_options=true,\n)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"We can again access the whole sets of records","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"get_record(R)","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"this is equivalent to calling R[:Iteration. Note that since we introduced :Count we can also access a single recorded value using","category":"page"},{"location":"tutorials/HowToRecord.html#","page":"Record values","title":"Record values","text":"R[:Iteration, :Count]","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#CGSolver-1","page":"Conjugate gradient descent","title":"Conjugate Gradient Descent","text":"","category":"section"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"conjugate_gradient_descent\nconjugate_gradient_descent!","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.conjugate_gradient_descent","page":"Conjugate gradient descent","title":"Manopt.conjugate_gradient_descent","text":"conjugate_gradient_descent(M, F, gradF, x)\n\nperform a conjugate gradient based descent\n\nx_k+1 = operatornameretr_x_k bigl( s_kδ_k bigr)\n\nwhere operatornameretr denotes a retraction on the Manifold M and one can employ different rules to update the descent direction δ_k based on the last direction δ_k-1 and both gradients operatornamegradf(x_k),operatornamegradf(x_k-1). The Stepsize s_k may be determined by a Linesearch.\n\nAvailable update rules are SteepestDirectionUpdateRule, which yields a gradient_descent, ConjugateDescentCoefficient (the default), DaiYuanCoefficient, FletcherReevesCoefficient, HagerZhangCoefficient, HeestenesStiefelCoefficient, LiuStoreyCoefficient, and PolakRibiereCoefficient.\n\nThey all compute β_k such that this algorithm updates the search direction as\n\ndelta_k=operatornamegradf(x_k) + β_k delta_k-1\n\nInput\n\nM : a manifold mathcal M\nF : a cost function Fmathcal Mℝ to minimize\ngradF: the gradient operatornamegradFmathcal M  Tmathcal M of F\nx : an initial value xmathcal M\n\nOptional\n\ncoefficient : (ConjugateDescentCoefficient <: DirectionUpdateRule) rule to compute the descent direction update coefficient β_k, as a functor, i.e. the resulting function maps (p,o,i) -> β, where p is the current GradientProblem, o are the ConjugateGradientDescentOptions o and i is the current iterate.\nevaluation – (AllocatingEvaluation) specify whether the gradient works by allocation (default) form gradF(M, x) or MutatingEvaluation in place, i.e. is of the form gradF!(M, X, x).\nretraction_method - (default_retraction_method(M) a retraction method to use.\nreturn_options – (false) – if actiavated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt if returned\nstepsize - (Constant(1.)) A Stepsize function applied to the search direction. The default is a constant step size 1.\nstopping_criterion : (stopWhenAny( stopAtIteration(200), stopGradientNormLess(10.0^-8))) a function indicating when to stop.\nvector_transport_method – (default_vector_transport_method(M)) vector transport method to transport the old descent direction when computing the new descent direction.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.conjugate_gradient_descent!","page":"Conjugate gradient descent","title":"Manopt.conjugate_gradient_descent!","text":"conjugate_gradient_descent!(M, F, gradF, x)\n\nperform a conjugate gradient based descent in place of x, i.e.\n\nx_k+1 = operatornameretr_x_k bigl( s_kdelta_k bigr)\n\nwhere operatornameretr denotes a retraction on the Manifold M\n\nInput\n\nM : a manifold mathcal M\nF : a cost function Fmathcal Mℝ to minimize\ngradF: the gradient operatornamegradFmathcal M Tmathcal M of F\nx : an initial value xmathcal M\n\nfor more details and options, especially the DirectionUpdateRules,  see conjugate_gradient_descent.\n\n\n\n\n\n","category":"function"},{"location":"solvers/conjugate_gradient_descent.html#Options-1","page":"Conjugate gradient descent","title":"Options","text":"","category":"section"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"ConjugateGradientDescentOptions","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.ConjugateGradientDescentOptions","page":"Conjugate gradient descent","title":"Manopt.ConjugateGradientDescentOptions","text":"ConjugateGradientOptions <: AbstractGradientOptions\n\nspecify options for a conjugate gradient descent algorithm, that solves a [GradientProblem].\n\nFields\n\nx – the current iterate, a point on a manifold\ngradient – the current gradient\nδ – the current descent direction, i.e. also tangent vector\nβ – the current update coefficient rule, see .\ncoefficient – a DirectionUpdateRule function to determine the new β\nstepsize – a Stepsize function\nstop – a StoppingCriterion\nretraction_method – (ExponentialRetraction()) a type of retraction\n\nSee also\n\nconjugate_gradient_descent, GradientProblem, ArmijoLinesearch\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Available-Coefficients-1","page":"Conjugate gradient descent","title":"Available Coefficients","text":"","category":"section"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"The update rules act as DirectionUpdateRule, which internally always first evaluate the gradient itself.","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#","page":"Conjugate gradient descent","title":"Conjugate gradient descent","text":"ConjugateDescentCoefficient\nDaiYuanCoefficient\nFletcherReevesCoefficient\nHagerZhangCoefficient\nHeestenesStiefelCoefficient\nLiuStoreyCoefficient\nPolakRibiereCoefficient\nSteepestDirectionUpdateRule","category":"page"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.ConjugateDescentCoefficient","page":"Conjugate gradient descent","title":"Manopt.ConjugateDescentCoefficient","text":"ConjugateDescentCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kξ_k, the current iterates x_k+1ξ_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [Flethcer1987] adapted to manifolds:\n\nβ_k =\nfrac lVert ξ_k+1 rVert_x_k+1^2 \nlangle -delta_kξ_k rangle_x_k\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nConjugateDescentCoefficient(a::StoreOptionsAction=())\n\nConstruct the conjugate descent coefficient update rule, a new storage is created by default.\n\n[Flethcer1987]: R. Fletcher, Practical Methods of Optimization vol. 1: Unconstrained Optimization John Wiley & Sons, New York, 1987. doi 10.1137/1024028\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.DaiYuanCoefficient","page":"Conjugate gradient descent","title":"Manopt.DaiYuanCoefficient","text":"DaiYuanCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kξ_k, the current iterates x_k+1ξ_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [DaiYuan1999]\n\nadapted to manifolds: let nu_k = ξ_k+1 - P_x_k+1gets x_kξ_k, where P_agets b() denotes a vector transport from the tangent space at a to b.\n\nThen the coefficient reads\n\nβ_k =\nfrac lVert ξ_k+1 rVert_x_k+1^2 \nlangle P_x_k+1gets x_kdelta_k nu_k rangle_x_k+1\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nDaiYuanCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=(),\n)\n\nConstruct the Dai Yuan coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\n[DaiYuan1999]: [Y. H. Dai and Y. Yuan, A nonlinear conjugate gradient method with a strong global convergence property, SIAM J. Optim., 10 (1999), pp. 177–182. doi: 10.1137/S1052623497318992\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.FletcherReevesCoefficient","page":"Conjugate gradient descent","title":"Manopt.FletcherReevesCoefficient","text":"FletcherReevesCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kξ_k, the current iterates x_k+1ξ_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [FletcherReeves1964] adapted to manifolds:\n\nβ_k =\nfraclVert ξ_k+1rVert_x_k+1^2lVert ξ_krVert_x_k^2\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nFletcherReevesCoefficient(a::StoreOptionsAction=())\n\nConstruct the Fletcher Reeves coefficient update rule, a new storage is created by default.\n\n[FletcherReeves1964]: R. Fletcher and C. Reeves, Function minimization by conjugate gradients, Comput. J., 7 (1964), pp. 149–154. doi: 10.1093/comjnl/7.2.149\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.HagerZhangCoefficient","page":"Conjugate gradient descent","title":"Manopt.HagerZhangCoefficient","text":"HagerZhangCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kξ_k, the current iterates x_k+1ξ_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [HagerZhang2005] adapted to manifolds: let nu_k = ξ_k+1 - P_x_k+1gets x_kξ_k, where P_agets b() denotes a vector transport from the tangent space at a to b.\n\nβ_k = Bigllanglenu_k -\nfrac 2lVert nu_krVert_x_k+1^2  langle P_x_k+1gets x_kdelta_k nu_k rangle_x_k+1 \nP_x_k+1gets x_kdelta_k\nfracξ_k+1 langle P_x_k+1gets x_kdelta_k nu_k rangle_x_k+1 \nBigrrangle_x_k+1\n\nThis method includes a numerical stability proposed by those authors.\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nHagerZhangCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=(),\n)\n\nConstruct the Hager Zhang coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\n[HagerZhang2005]: [W. W. Hager and H. Zhang, A new conjugate gradient method with guaranteed descent and an efficient line search, SIAM J. Optim, (16), pp. 170-192, 2005. doi: 10.1137/030601880\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.HeestenesStiefelCoefficient","page":"Conjugate gradient descent","title":"Manopt.HeestenesStiefelCoefficient","text":"HeestenesStiefelCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kξ_k, the current iterates x_k+1ξ_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [HeestensStiefel1952]\n\nadapted to manifolds as follows: let nu_k = ξ_k+1 - P_x_k+1gets x_kξ_k. Then the update reads\n\nβ_k = fraclangle ξ_k+1 nu_k rangle_x_k+1 \n     langle P_x_k+1gets x_k delta_k nu_krangle_x_k+1 \n\nwhere P_agets b() denotes a vector transport from the tangent space at a to b.\n\nConstructor\n\nHeestenesStiefelCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=()\n)\n\nConstruct the Heestens Stiefel coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\nSee also conjugate_gradient_descent\n\n[HeestensStiefel1952]: M.R. Hestenes, E.L. Stiefel, Methods of conjugate gradients for solving linear systems, J. Research Nat. Bur. Standards, 49 (1952), pp. 409–436. doi: 10.6028/jres.049.044\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.LiuStoreyCoefficient","page":"Conjugate gradient descent","title":"Manopt.LiuStoreyCoefficient","text":"LiuStoreyCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kξ_k, the current iterates x_k+1ξ_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [LuiStorey1991] adapted to manifolds: let nu_k = ξ_k+1 - P_x_k+1gets x_kξ_k, where P_agets b() denotes a vector transport from the tangent space at a to b.\n\nThen the coefficient reads\n\nβ_k = -\nfrac langle ξ_k+1nu_k rangle_x_k+1 \nlangle delta_kξ_k rangle_x_k\n\nSee also conjugate_gradient_descent\n\nConstructor\n\nLiuStoreyCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=()\n)\n\nConstruct the Lui Storey coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\n[LuiStorey1991]: [Y. Liu and C. Storey, Efficient generalized conjugate gradient algorithms, Part 1: Theory J. Optim. Theory Appl., 69 (1991), pp. 129–137. doi: 10.1007/BF00940464\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.PolakRibiereCoefficient","page":"Conjugate gradient descent","title":"Manopt.PolakRibiereCoefficient","text":"PolakRibiereCoefficient <: DirectionUpdateRule\n\nComputes an update coefficient for the conjugate gradient method, where the ConjugateGradientDescentOptionso include the last iterates x_kξ_k, the current iterates x_k+1ξ_k+1 and the last update direction delta=delta_k, where the last three ones are stored in the variables with prequel Old based on [PolakRibiere1969][Polyak1969]\n\nadapted to manifolds: let nu_k = ξ_k+1 - P_x_k+1gets x_kξ_k, where P_agets b() denotes a vector transport from the tangent space at a to b.\n\nThen the update reads\n\nβ_k =\nfrac langle ξ_k+1 nu_k rangle_x_k+1 \nlVert ξ_k rVert_x_k^2 \n\nConstructor\n\nPolakRibiereCoefficient(\n    t::AbstractVectorTransportMethod=ParallelTransport(),\n    a::StoreOptionsAction=()\n)\n\nConstruct the PolakRibiere coefficient update rule, where the parallel transport is the default vector transport and a new storage is created by default.\n\nSee also conjugate_gradient_descent\n\n[PolakRibiere1969]: E. Polak, G. Ribiere, Note sur la convergence de méthodes de directions conjuguées ESAIM: Mathematical Modelling and Numerical Analysis - Modélisation Mathématique et Analyse Numérique, Tome 3 (1969) no. R1, p. 35-43, url: http://www.numdam.org/item/?id=M2AN1969__31350\n\n[Polyak1969]: B. T. Polyak, The conjugate gradient method in extreme problems, USSR Comp. Math. Math. Phys., 9 (1969), pp. 94–112. doi: 10.1016/0041-5553(69)90035-4\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Manopt.SteepestDirectionUpdateRule","page":"Conjugate gradient descent","title":"Manopt.SteepestDirectionUpdateRule","text":"SteepestDirectionUpdateRule <: DirectionUpdateRule\n\nThe simplest rule to update is to have no influence of the last direction and hence return an update β = 0 for all ConjugateGradientDescentOptionso\n\nSee also conjugate_gradient_descent\n\n\n\n\n\n","category":"type"},{"location":"solvers/conjugate_gradient_descent.html#Literature-1","page":"Conjugate gradient descent","title":"Literature","text":"","category":"section"},{"location":"about.html#About-1","page":"About","title":"About","text":"","category":"section"},{"location":"about.html#","page":"About","title":"About","text":"Manopt.jl inherited its name from Manopt, a Matlab toolbox. It is currently Maintained by Ronny Bergmann (manopt@ronnybergmann.net) with contributions from Tom Christian Riemer, who implemented the trust regions solver.","category":"page"},{"location":"about.html#","page":"About","title":"About","text":"If you want to contribute a manifold or algorithm or have any questions, visit the GitHub repository to clone/fork the repository or open an issue.","category":"page"},{"location":"functions/bezier.html#BezierCurves-1","page":"Bézier curves","title":"Bézier curves","text":"","category":"section"},{"location":"functions/bezier.html#","page":"Bézier curves","title":"Bézier curves","text":"Modules = [Manopt]\nPages   = [\"bezier_curves.jl\"]","category":"page"},{"location":"functions/bezier.html#Manopt.BezierSegment","page":"Bézier curves","title":"Manopt.BezierSegment","text":"BezierSegment\n\nA type to capture a Bezier segment. With n points, a Beziér segment of degree n-1 is stored. On the Euclidean manifold, this yields a polynomial of degree n-1.\n\nThis type is mainly used to encapsulate the points within a composite Bezier curve, which consist of an AbstractVector of BezierSegments where each of the points might be a nested array on a PowerManifold already.\n\nNot that this can also be used to represent tangent vectors on the control points of a segment.\n\nSee also: de_casteljau.\n\nConstructor\n\nBezierSegment(pts::AbstractVector)\n\nGiven an abstract vector of pts generate the corresponding Bézier segment.\n\n\n\n","category":"type"},{"location":"functions/bezier.html#Manopt.de_casteljau-Tuple{AbstractManifold, Vararg{Any, N} where N}","page":"Bézier curves","title":"Manopt.de_casteljau","text":"de_casteljau(M::AbstractManifold, b::BezierSegment NTuple{N,P}) -> Function\n\nreturn the Bézier curve β(b_0b_n) 01  mathcal M defined by the control points b_0b_nmathcal M, nmathbb N, as a BezierSegment. This function implements de Casteljau's algorithm[Casteljau1959][Casteljau1963] gneralized to manifolds[PopielNoakes2007]: Let γ_ab(t) denote the shortest geodesic connecting abmathcal M. Then the curve is defined by the recursion\n\nbeginaligned\n    β(tb_0b_1) = gamma_b_0b_1(t)\n    β(tb_0b_n) = gamma_β(tb_0b_n-1) β(tb_1b_n)(t)\nendaligned\n\nand P is the type of a point on the Manifold M.\n\nde_casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}) -> Function\n\nGiven a vector of Bézier segments, i.e. a vector of control points B=bigl( (b_00b_n_00)(b_0m b_n_mm) bigr), where the different segments might be of different degree(s) n_0n_m. The resulting composite Bézier curve c_B0m  mathcal M consists of m segments which are Bézier curves.\n\nc_B(t) =\n    begincases\n        β(t b_00b_n_00)  text if  t 01\n        β(t-i b_0ib_n_ii)  text if \n            t(ii+1 quad i1m-1\n    endcases\n\nde_casteljau(M::AbstractManifold, b::BezierSegment, t::Real)\nde_casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}, t::Real)\nde_casteljau(M::AbstractManifold, b::BezierSegment, T::AbstractVector) -> AbstractVector\nde_casteljau(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n) -> AbstractVector\n\nEvaluate the Bézier curve at time t or at times t in T.\n\n[Casteljau1959]: de Casteljau, P.: Outillage methodes calcul, Enveloppe Soleau 40.040 (1959), Institute National de la Propriété Industrielle, Paris.\n\n[Casteljau1963]: de Casteljau, P.: Courbes et surfaces à pôles, Microfiche P 4147-1, André Citroën Automobile SA, Paris, (1963).\n\n[PopielNoakes2007]: Popiel, T. and Noakes, L.: Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory (2007), 148(2), pp. 111–127.- doi: 10.1016/j.jat.2007.03.002.\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_degree-Tuple{AbstractManifold, BezierSegment}","page":"Bézier curves","title":"Manopt.get_bezier_degree","text":"get_bezier_degree(M::AbstractManifold, b::BezierSegment)\n\nreturn the degree of the Bézier curve represented by the tuple b of control points on the manifold M, i.e. the number of points minus 1.\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_degrees-Tuple{AbstractManifold, AbstractVector{var\"#s54\"} where var\"#s54\"<:BezierSegment}","page":"Bézier curves","title":"Manopt.get_bezier_degrees","text":"get_bezier_degrees(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\n\nreturn the degrees of the components of a composite Bézier curve represented by tuples in B containing points on the manifold M.\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_inner_points-Tuple{AbstractManifold, AbstractVector{var\"#s54\"} where var\"#s54\"<:BezierSegment}","page":"Bézier curves","title":"Manopt.get_bezier_inner_points","text":"get_bezier_inner_points(M::AbstractManifold, B::AbstractVector{<:BezierSegment} )\nget_bezier_inner_points(M::AbstractManifold, b::BezierSegment)\n\nreturns the inner (i.e. despite start and end) points of the segments of the composite Bézier curve specified by the control points B. For a single segment b, its inner points are returned\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_junction_tangent_vectors-Union{Tuple{P}, Tuple{AbstractManifold, AbstractVector{var\"#s54\"} where var\"#s54\"<:BezierSegment}} where P","page":"Bézier curves","title":"Manopt.get_bezier_junction_tangent_vectors","text":"get_bezier_junction_tangent_vectors(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_bezier_junction_tangent_vectors(M::AbstractManifold, b::BezierSegment)\n\nreturns the tangent vectors at start and end points of the composite Bézier curve pointing from a junction point to the first and last inner control points for each segment of the composite Bezier curve specified by the control points B, either a vector of segments of controlpoints.\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Manopt.get_bezier_junctions","page":"Bézier curves","title":"Manopt.get_bezier_junctions","text":"get_bezier_junctions(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_bezier_junctions(M::AbstractManifold, b::BezierSegment)\n\nreturns the start and end point(s) of the segments of the composite Bézier curve specified by the control points B. For just one segment b, its start and end points are returned.\n\n\n\n\n\n","category":"function"},{"location":"functions/bezier.html#Manopt.get_bezier_points","page":"Bézier curves","title":"Manopt.get_bezier_points","text":"get_bezier_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    reduce::Symbol=:default\n)\nget_bezier_points(M::AbstractManifold, b::BezierSegment, reduce::Symbol=:default)\n\nreturns the control points of the segments of the composite Bézier curve specified by the control points B, either a vector of segments of controlpoints or a.\n\nThis method reduces the points depending on the optional reduce symbol\n\n:default – no reduction is performed\n:continuous – for a continuous function, the junction points are doubled at b_0i=b_n_i-1i-1, so only b_0i is in the vector.\n:differentiable – for a differentiable function additionally log_b_0ib_1i = -log_b_n_i-1i-1b_n_i-1-1i-1 holds. hence b_n_i-1-1i-1 is ommited.\n\nIf only one segment is given, all points of b – i.e. b.pts is returned.\n\n\n\n\n\n","category":"function"},{"location":"functions/bezier.html#Manopt.get_bezier_segments-Union{Tuple{P}, Tuple{AbstractManifold, Vector{P}, Any}, Tuple{AbstractManifold, Vector{P}, Any, Symbol}} where P","page":"Bézier curves","title":"Manopt.get_bezier_segments","text":"get_bezier_segments(M::AbstractManifold, c::AbstractArray{P}, d[, s::Symbol=:default])\n\nreturns the array of BezierSegments B of a composite Bézier curve reconstructed from an array c of points on the manifold M and an array of degrees d.\n\nThere are a few (reduced) representations that can get extended; see also get_bezier_points. For ease of the following, let c=(c_1c_k) and d=(d_1d_m), where m denotes the number of components the composite Bézier curve consists of. Then\n\n:default – k = m + sum_i=1^m d_i since each component requires one point more than its degree. The points are then ordered in tuples, i.e.\nB = bigl c_1c_d_1+1 (c_d_1+2c_d_1+d_2+2 c_k-m+1+d_mc_k bigr\n:continuous – k = 1+ sum_i=1m d_i, since for a continuous curve start and end point of successive components are the same, so the very first start point and the end points are stored.\nB = bigl c_1c_d_1+1 c_d_1+1c_d_1+d_2+1 c_k-1+d_mb_k) bigr\n:differentiable – for a differentiable function additionally to the last explanation, also the second point of any segment was not stored except for the first segment. Hence k = 2 - m + sum_i=1m d_i and at a junction point b_n with its given prior point c_n-1, i.e. this is the last inner point of a segment, the first inner point in the next segment the junction is computed as b = exp_c_n(-log_c_n c_n-1) such that the assumed differentiability holds\n\n\n\n\n\n","category":"method"},{"location":"functions/bezier.html#Literature-1","page":"Bézier curves","title":"Literature","text":"","category":"section"},{"location":"solvers/ChambollePock.html#ChambollePockSolver-1","page":"Chambolle-Pock","title":"The Riemannian Chambolle-Pock Algorithm","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"The Riemannian Chambolle–Pock is a generalization of the Chambolle–Pock algorithm[ChambollePock2011]. It is also known as primal dual hybrig gradient (PDHG) or primal dual proximal splitting (PDPS) algorithm.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"In order to minimize over p∈\\mathcal M§ the cost function consisting of","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"F(p) + G(Λ(p))","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"where Fmathcal M  overlineℝ, Gmathcal N  overlineℝ, and Λmathcal M mathcal N. If the manifolds mathcal M or mathcal N are not Hadamard, it has to be considered locally, i.e. on geodesically convex sets mathcal C subset mathcal M and mathcal D subsetmathcal N such that Λ(mathcal C) subset mathcal D.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"The algorithm is available in four variants: exact versus linearized (see variant) as well as with primal versus dual relaxation (see relax). For more details, see [BergmannHerzogSilvaLouzeiroTenbrinckVidalNunez2020]. In the following we note the case of the exact, primal relaxed Riemannian Chambolle–Pock algorithm.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"Given base points mmathcal C, n=Λ(m)mathcal D, initial primal and dual values p^(0) mathcal C, ξ_n^(0) T_n^*mathcal N, and primal and dual step sizes sigma_0, tau_0, relaxation theta_0, as well as acceleration gamma.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"As an initialization, perform bar p^(0) gets p^(0).","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"The algorithms performs the steps k=1 (until a StoppingCriterion is fulfilled with)","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"ξ^(k+1)_n = operatornameprox_tau_k G_n^*Bigl(ξ_n^(k) + tau_k bigl(log_n Λ (bar p^(k))bigr)^flatBigr)\np^(k+1) = operatornameprox_sigma_k Fbiggl(exp_p^(k)Bigl( operatornamePT_p^(k)gets mbigl(-sigma_k DΛ(m)^*ξ_n^(k+1)bigr)^sharpBigr)biggr)\nUpdate\ntheta_k = (1+2gammasigma_k)^-frac12\nsigma_k+1 = sigma_ktheta_k\ntau_k+1 =  fractau_ktheta_k\nbar p^(k+1)  = exp_p^(k+1)bigl(-theta_k log_p^(k+1) p^(k)bigr)","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"Furthermore you can exchange the exponential map, the logarithmic map, and the parallel transport by a retraction, an in verse retraction and a vector transport.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"Finally you can also update the base points m and n during the iterations. This introduces a few additional vector transports. The same holds for the case that Λ(m^(k))neq n^(k) at some point. All these cases are covered in the algorithm.","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"ChambollePock\nChambollePock!","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.ChambollePock","page":"Chambolle-Pock","title":"Manopt.ChambollePock","text":"ChambollePock(\n    M, N, cost, x0, ξ0, m, n, prox_F, prox_G_dual, adjoint_linear_operator;\n    forward_operator=missing,\n    linearized_forward_operator=missing,\n    evaluation=AllocatingEvaluation()\n)\n\nPerform the Riemannian Chambolle–Pock algorithm.\n\nGiven a cost function mathcal Emathcal M  ℝ of the form\n\nmathcal E(x) = F(x) + G( Λ(x) )\n\nwhere Fmathcal M  ℝ, Gmathcal N  ℝ, and Λmathcal M  mathcal N. The remaining input parameters are\n\nx,ξ primal and dual start points xmathcal M and ξT_nmathcal N\nm,n base points on mathcal M and mathcal N, respectively.\nadjoint_linearized_operator the adjoint DΛ^* of the linearized operator DΛ(m) T_mmathcal M  T_Λ(m)mathcal N\nprox_F, prox_G_Dual the proximal maps of F and G^ast_n\n\nnote that depending on the AbstractEvaluationType evaluation the last three parameters as well as the forwardoperator Λ and the `linearizedforward_operatorcan be given as allocating functions(Manifolds, parameters) -> resultor as mutating functions(Manifold, result, parameters)-> result to spare allocations.\n\nBy default, this performs the exact Riemannian Chambolle Pock algorithm, see the optional parameter DΛ for their linearized variant.\n\nFor more details on the algorithm, see[BergmannHerzogSilvaLouzeiroTenbrinckVidalNunez2020].\n\nOptional Parameters\n\nacceleration – (0.05)\ndual_stepsize – (1/sqrt(8)) proximal parameter of the primal prox\nevaluation (AllocatingEvaluation()) specify whether the proximal maps and operators are allocating functions(Manifolds, parameters) -> resultor given as mutating functions(Manifold, result, parameters)-> result to spare allocations.\nΛ (missing) the (forward) operator Λ() (required for the :exact variant)\nlinearized_forward_operator (missing) its linearization DΛ() (required for the :linearized variant)\nprimal_stepsize – (1/sqrt(8)) proximal parameter of the dual prox\nrelaxation – (1.)\nrelax – (:primal) whether to relax the primal or dual\nvariant - (:exact if Λ is missing, otherwise :linearized) variant to use. Note that this changes the arguments the forward_operator will be called.\nstopping_criterion – (stopAtIteration(100)) a StoppingCriterion\nupdate_primal_base – (missing) function to update m (identity by default/missing)\nupdate_dual_base – (missing) function to update n (identity by default/missing)\nretraction_method – (default_retraction_method(M)) the rectraction to use\ninverse_retraction_method - (default_inverse_retraction_method(M)) an inverse retraction to use.\nvector_transport_method - (default_vector_transport_method(M)) a vector transport to use\n\n[BergmannHerzogSilvaLouzeiroTenbrinckVidalNunez2020]: R. Bergmann, R. Herzog, M. Silva Louzeiro, D. Tenbrinck, J. Vidal-Núñez: Fenchel Duality Theory and a Primal-Dual Algorithm on Riemannian Manifolds, Foundations of Computational Mathematics, 2021. doi: 10.1007/s10208-020-09486-5 arXiv: 1908.02022\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.ChambollePock!","page":"Chambolle-Pock","title":"Manopt.ChambollePock!","text":"ChambollePock(M, N, cost, x0, ξ0, m, n, prox_F, prox_G_dual, adjoint_linear_operator)\n\nPerform the Riemannian Chambolle–Pock algorithm in place of x, ξ, and potentially m, n if they are not fixed. See ChambollePock for details and optional parameters.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Problem-and-Options-1","page":"Chambolle-Pock","title":"Problem & Options","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"PrimalDualOptions\nChambollePockOptions","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.PrimalDualOptions","page":"Chambolle-Pock","title":"Manopt.PrimalDualOptions","text":"PrimalDualOptions\n\nA general type for all primal dual based options to be used within primal dual based algorithms\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Manopt.ChambollePockOptions","page":"Chambolle-Pock","title":"Manopt.ChambollePockOptions","text":"ChambollePockOptions <: PrimalDualOptions\n\nstores all options and variables within a linearized or exact Chambolle Pock. The following list provides the order for the constructor, where the previous iterates are initialized automatically and values with a default may be left out.\n\nm - base point on mathcal M\nn - base point on mathcal N\nx - an initial point on x^(0) mathcal M (and its previous iterate)\nξ - an initial tangent vector ξ^(0)T^*mathcal N (and its previous iterate)\nxbar - the relaxed iterate used in the next dual update step (when using :primal relaxation)\nξbar - the relaxed iterate used in the next primal update step (when using :dual relaxation)\nΘ – factor to damp the helping tilde x\nprimal_stepsize – (1/sqrt(8)) proximal parameter of the primal prox\ndual_stepsize – (1/sqrt(8)) proximal parameter of the dual prox\nacceleration – (0.) acceleration factor due to Chambolle & Pock\nrelaxation – (1.) relaxation in the primal relaxation step (to compute xbar)\nrelax – (_primal) which variable to relax (:primal or :dual)\nstop - a StoppingCriterion\ntype – (exact) whether to perform an :exact or :linearized Chambolle-Pock\nupdate_primal_base ((p,o,i) -> o.m) function to update the primal base\nupdate_dual_base ((p,o,i) -> o.n) function to update the dual base\nretraction_method – (ExponentialRetraction()) the retraction to use\ninverse_retraction_method - (LogarithmicInverseRetraction()) an inverse retraction to use.\nvector_transport_method - (ParallelTransport()) a vector transport to use\n\nwhere for the last two the functions a Problemp, Optionso and the current iterate i are the arguments. If you activate these to be different from the default identity, you have to provide p.Λ for the algorithm to work (which might be missing in the linearized case).\n\nConstructor\n\nChambollePockOptions(m::P, n::Q, x::P, ξ::T, primal_stepsize::Float64, dual_stepsize::Float64;\n    acceleration::Float64 = 0.0,\n    relaxation::Float64 = 1.0,\n    relax::Symbol = :primal,\n    stopping_criterion::StoppingCriterion = StopAfterIteration(300),\n    variant::Symbol = :exact,\n    update_primal_base::Union{Function,Missing} = missing,\n    update_dual_base::Union{Function,Missing} = missing,\n    retraction_method = ExponentialRetraction(),\n    inverse_retraction_method = LogarithmicInverseRetraction(),\n    vector_transport_method = ParallelTransport(),\n)\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Useful-Terms-1","page":"Chambolle-Pock","title":"Useful Terms","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"primal_residual\ndual_residual","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.primal_residual","page":"Chambolle-Pock","title":"Manopt.primal_residual","text":"primal_residual(p, o, x_old, ξ_old, n_old)\n\nCompute the primal residual at current iterate k given the necessary values x_k-1 ξ_k-1, and n_k-1 from the previous iterate.\n\nBigllVert\nfrac1σoperatornameretr^-1_x_kx_k-1 -\nV_x_kgets m_kbigl(DΛ^*(m_k)biglV_n_kgets n_k-1ξ_k-1 - ξ_k bigr\nBigrrVert\n\nwhere V_gets is the vector transport used in the ChambollePockOptions\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.dual_residual","page":"Chambolle-Pock","title":"Manopt.dual_residual","text":"dual_residual(p, o, x_old, ξ_old, n_old)\n\nCompute the dual residual at current iterate k given the necessary values x_k-1 ξ_k-1, and n_k-1 from the previous iterate. The formula is slightly different depending on the o.variant used:\n\nFor the :lineaized it reads\n\nBigllVert\nfrac1τbigl(\nV_n_kgets n_k-1(ξ_k-1)\n- ξ_k\nbigr)\n-\nDΛ(m_k)bigl\nV_m_kgets x_koperatornameretr^-1_x_kx_k-1\nbigr\nBigrrVert\n\nand for the :exact variant\n\nBigllVert\nfrac1τ V_n_kgets n_k-1(ξ_k-1)\n-\noperatornameretr^-1_n_kbigl(\nΛ(operatornameretr_m_k(V_m_kgets x_koperatornameretr^-1_x_kx_k-1))\nbigr)\nBigrrVert\n\nwhere in both cases V_gets is the vector transport used in the ChambollePockOptions.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Debug-1","page":"Chambolle-Pock","title":"Debug","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"DebugDualBaseIterate\nDebugDualBaseChange\nDebugPrimalBaseIterate\nDebugPrimalBaseChange\nDebugDualChange\nDebugDualIterate\nDebugDualResidual\nDebugPrimalChange\nDebugPrimalIterate\nDebugPrimalResidual\nDebugPrimalDualResidual","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualBaseIterate","page":"Chambolle-Pock","title":"Manopt.DebugDualBaseIterate","text":"DebugDualBaseIterate(io::IO=stdout)\n\nPrint the dual base variable by using DebugEntry, see their constructors for detail. This method is further set display o.n.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualBaseChange","page":"Chambolle-Pock","title":"Manopt.DebugDualBaseChange","text":"DebugDualChange(; storage=StoreOptionsAction((:ξ)), io::IO=stdout)\n\nPrint the change of the dual base variable by using DebugEntryChange, see their constructors for detail, on o.n.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalBaseIterate","page":"Chambolle-Pock","title":"Manopt.DebugPrimalBaseIterate","text":"DebugPrimalBaseIterate()\n\nPrint the primal base variable by using DebugEntry, see their constructors for detail. This method is further set display o.m.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalBaseChange","page":"Chambolle-Pock","title":"Manopt.DebugPrimalBaseChange","text":"DebugPrimalBaseChange(a::StoreOptionsAction=StoreOptionsAction((:m)),io::IO=stdout)\n\nPrint the change of the primal base variable by using DebugEntryChange, see their constructors for detail, on o.n.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualChange","page":"Chambolle-Pock","title":"Manopt.DebugDualChange","text":"DebugDualChange(opts...)\n\nPrint the change of the dual variable, similar to DebugChange, see their constructors for detail, but with a different calculation of the change, since the dual variable lives in (possibly different) tangent spaces.\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualIterate","page":"Chambolle-Pock","title":"Manopt.DebugDualIterate","text":"DebugDualIterate(e)\n\nPrint the dual variable by using DebugEntry, see their constructors for detail. This method is further set display o.ξ.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugDualResidual","page":"Chambolle-Pock","title":"Manopt.DebugDualResidual","text":"DebugDualResidual <: DebugAction\n\nA Debug action to print the dual residual. The constructor accepts a printing function and some (shared) storage, which should at least record :x, :ξ and :n.\n\nConstructor\n\nDebugDualResidual()\n\nwith the keywords\n\nio (stdout) - stream to perform the debug to\nformat (\"$prefix%s\") format to print the dual residual, using the\nprefix (\"Dual Residual: \") short form to just set the prefix\nstorage (a new StoreOptionsAction) to store values for the debug.\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalChange","page":"Chambolle-Pock","title":"Manopt.DebugPrimalChange","text":"DebugPrimalChange(opts...)\n\nPrint the change of the primal variable by using DebugChange, see their constructors for detail.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalIterate","page":"Chambolle-Pock","title":"Manopt.DebugPrimalIterate","text":"DebugPrimalIterate(opts...;kwargs...)\n\nPrint the change of the primal variable by using DebugIterate, see their constructors for detail.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalResidual","page":"Chambolle-Pock","title":"Manopt.DebugPrimalResidual","text":"DebugPrimalResidual <: DebugAction\n\nA Debug action to print the primal residual. The constructor accepts a printing function and some (shared) storage, which should at least record :x, :ξ and :n.\n\nConstructor\n\nDebugPrimalResidual()\n\nwith the keywords\n\nio (stdout) - stream to perform the debug to\nformat (\"$prefix%s\") format to print the dual residual, using the\nprefix (\"Primal Residual: \") short form to just set the prefix\nstorage (a new StoreOptionsAction) to store values for the debug.\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Manopt.DebugPrimalDualResidual","page":"Chambolle-Pock","title":"Manopt.DebugPrimalDualResidual","text":"DebugPrimalDualResidual <: DebugAction\n\nA Debug action to print the primaldual residual. The constructor accepts a printing function and some (shared) storage, which should at least record :x, :ξ and :n.\n\nConstructor\n\nDebugPrimalDualResidual()\n\nwith the keywords\n\nio (stdout) - stream to perform the debug to\nformat (\"$prefix%s\") format to print the dual residual, using the\nprefix (\"Primal Residual: \") short form to just set the prefix\nstorage (a new StoreOptionsAction) to store values for the debug.\n\n\n\n\n\n","category":"type"},{"location":"solvers/ChambollePock.html#Record-1","page":"Chambolle-Pock","title":"Record","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"RecordDualBaseIterate\nRecordDualBaseChange\nRecordDualChange\nRecordDualIterate\nRecordPrimalBaseIterate\nRecordPrimalBaseChange\nRecordPrimalChange\nRecordPrimalIterate","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.RecordDualBaseIterate","page":"Chambolle-Pock","title":"Manopt.RecordDualBaseIterate","text":"RecordDualBaseIterate(n)\n\nCreate an RecordAction that records the dual base point, i.e. RecordEntry of o.n.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordDualBaseChange","page":"Chambolle-Pock","title":"Manopt.RecordDualBaseChange","text":"RecordDualBaseChange(e)\n\nCreate an RecordAction that records the dual base point change, i.e. RecordEntryChange of o.n with distance to the last value to store a value.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordDualChange","page":"Chambolle-Pock","title":"Manopt.RecordDualChange","text":"RecordDualChange()\n\nCreate the action either with a given (shared) Storage, which can be set to the values Tuple, if that is provided).\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordDualIterate","page":"Chambolle-Pock","title":"Manopt.RecordDualIterate","text":"RecordDualIterate(ξ)\n\nCreate an RecordAction that records the dual base point, i.e. RecordEntry of o.ξ, so .\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordPrimalBaseIterate","page":"Chambolle-Pock","title":"Manopt.RecordPrimalBaseIterate","text":"RecordPrimalBaseIterate(x)\n\nCreate an RecordAction that records the primal base point, i.e. RecordEntry of o.m.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordPrimalBaseChange","page":"Chambolle-Pock","title":"Manopt.RecordPrimalBaseChange","text":"RecordPrimalBaseChange()\n\nCreate an RecordAction that records the primal base point change, i.e. RecordEntryChange of o.m with distance to the last value to store a value.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordPrimalChange","page":"Chambolle-Pock","title":"Manopt.RecordPrimalChange","text":"RecordPrimalChange(a)\n\nCreate an RecordAction that records the primal value change, i.e. RecordChange, since we just record the change of o.x.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Manopt.RecordPrimalIterate","page":"Chambolle-Pock","title":"Manopt.RecordPrimalIterate","text":"RecordDualBaseIterate(x)\n\nCreate an RecordAction that records the dual base point, i.e. RecordIterate, i.e. o.x.\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#Internals-1","page":"Chambolle-Pock","title":"Internals","text":"","category":"section"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"Manopt.update_prox_parameters!","category":"page"},{"location":"solvers/ChambollePock.html#Manopt.update_prox_parameters!","page":"Chambolle-Pock","title":"Manopt.update_prox_parameters!","text":"update_prox_parameters!(o)\n\nupdate the prox parameters as described in Algorithm 2 of Chambolle, Pock, 2010, i.e.\n\nθ_n = frac1sqrt1+2γτ_n\nτ_n+1 = θ_nτ_n\nσ_n+1 = fracσ_nθ_n\n\n\n\n\n\n","category":"function"},{"location":"solvers/ChambollePock.html#","page":"Chambolle-Pock","title":"Chambolle-Pock","text":"[ChambollePock2011]: A. Chambolle, T. Pock: A first-order primal-dual algorithm for convex problems with applications to imaging, Journal of Mathematical Imaging and Vision 40(1), 120–145, 2011. doi: 10.1007/s10851-010-0251-1","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/GeodesicRegression.jl\"","category":"page"},{"location":"tutorials/GeodesicRegression.html#Geodesic-Regression-1","page":"Do Geodesic regression","title":"Geodesic Regression","text":"","category":"section"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"Geodesic regression generalizes linear regression to Riemannian manifolds. Let's first phrase it informally as follows:","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"For given data points d_1ldotsd_n on a Riemannian manifold mathcal M find the geodesic that “best explains” the data.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"The meaning of “best explain” has still to be clarified. We distinguish two cases: time labelled data and unlabelled data","category":"page"},{"location":"tutorials/GeodesicRegression.html#regression-setup-1","page":"Do Geodesic regression","title":"Setup","text":"","category":"section"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"using Manopt, Manifolds, Colors, Random\nusing LinearAlgebra: svd\n\nblack = RGBA{Float64}(colorant\"#000000\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\")\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\")\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\")\nTolVibrantMagenta = RGBA{Float64}(colorant\"#EE3377\")\nTolVibrantCyan = RGBA{Float64}(colorant\"#33BBEE\")\nRandom.seed!(42)\n\nn = 7\nhighlighted = 4\n(highlighted > n - 1) && error(\n    \"Please choose a highlighted point from {1,...,$(n-1)} – you set it to $highlighted.\",\n)\nσ = π / 8\nS = Sphere(2)\nbase = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndir = [-0.75, 0.5, 0.75]\ndata = [exp(S, base, dir, t) for t in range(-0.5, 0.5; length=n)]\ndata = map(x -> exp(S, x, random_tangent(S, x, :Gaussian, σ)), data)\nnothing #hide","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"which looks as follows (using asymptote_export_S2_signals)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"asymptote_export_S2_signals(\"regression_data.asy\";\n    points = [ [x], data],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal]),\n    dot_size = 3.5, camera_position = (1.,.5,.5)\n)\nrender_asymptote(\"regression_data.asy\"; render = 2)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"(Image: The data of noisy versions of $x$)","category":"page"},{"location":"tutorials/GeodesicRegression.html#time-labelled-data-regression-1","page":"Do Geodesic regression","title":"Time labeled data","text":"","category":"section"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"if for each data item d_i we are also given a time point t_iinmathbb R, which are pairwise different.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"Then we can use the least squares error to state the objetive function as [Fletcher2013]","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"F(pX) = frac12sum_i=1^n d_mathcal M^2(γ_pX(t_i) d_i)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"where d_mathcal M is the Riemannian distance and γ_pX is the geodesic with γ(0) = p and dotgamma(0) = X.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"For the real-valued case mathcal M = mathbb R^m the solution (p^* X^*) is given in closed form as follows: with d^* = frac1ndisplaystylesum_i=1^nd_i and t^* = frac1ndisplaystylesum_i=1^n t_i we get","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":" X^* = fracsum_i=1^n (d_i-d^*)(t-t^*)sum_i=1^n (t_i-t^*)^2\nquadtext and quad\np^* = d^* - t^*X^*","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"and hence the linear regression result is the line γ_p^*x^*(t) = p^* + tX^*.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"On a Riemannian manifold we can phrase this as an optimization problem on the tangent bundle, i.e. the disjoiint union of all tangent spaces, as","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"operatorname*argmin_(pX) in mathrmTmathcal M F(pX)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"Due to linearity, the gradient of F(pX) is the sum of the single gradients of","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":" frac12d_mathcal M^2bigl(γ_pX(t_i)d_ibigr)\n = frac12d_mathcal M^2bigl(exp_p(t_iX)d_ibigr)\n quad i1ldotsn","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"which can be computed using a chain rule of the squared distance and the exponential map, see for example [BergmannGousenbourger2018] for details or Equations (7) and (8) of [Fletcher2013]:","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"M = TangentBundle(S)\n\nstruct RegressionCost{T,S}\n    data::T\n    times::S\nend\nRegressionCost(data::T, times::S) where {T,S} = RegressionCost{T,S}(data, times)\nfunction (a::RegressionCost)(M, x)\n    pts = [geodesic(M.manifold, x[M, :point], x[M, :vector], ti) for ti in a.times]\n    return 1 / 2 * sum(distance.(Ref(M.manifold), pts, a.data) .^ 2)\nend\nstruct RegressionGradient!{T,S}\n    data::T\n    times::S\nend\nRegressionGradient!(data::T, times::S) where {T,S} = RegressionGradient!{T,S}(data, times)\nfunction (a::RegressionGradient!)(M, Y, x)\n    pts = [geodesic(M.manifold, x[M, :point], x[M, :vector], ti) for ti in a.times]\n    gradients = grad_distance.(Ref(M.manifold), a.data, pts)\n    Y[M, :point] .= sum(\n        adjoint_differential_exp_basepoint.(\n            Ref(M.manifold),\n            Ref(x[M, :point]),\n            [ti * x[M, :vector] for ti in a.times],\n            gradients,\n        ),\n    )\n    Y[M, :vector] .= sum(\n        adjoint_differential_exp_argument.(\n            Ref(M.manifold),\n            Ref(x[M, :point]),\n            [ti * x[M, :vector] for ti in a.times],\n            gradients,\n        ),\n    )\n    return Y\nend","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"Now we need just a start point.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"For the Euclidean case, the result is given by the first principal component of a principal component analysis, see PCR, i.e. with p^* = frac1ndisplaystylesum_i=1^n d_i the direction X^* is obtained by defining the zero mean data matrix","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"D = bigl(d_1-p^* ldots d_n-p^*bigr) in mathbb R^mn","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"and taking X^* as an eigenvector to the larges eigenvalue of D^mathrmTD.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"We can do something similar, when considering the tangent space at the (Riemannian) mean of the data and then do a PCA on the coordinate coefficients with respect to a basis.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"m = mean(S, data)\nA = hcat(map(x -> get_coordinates(S, m, log(S, m, x), DefaultOrthonormalBasis()), data)...)\npca1 = get_vector(S, m, svd(A).U[:, 1], DefaultOrthonormalBasis())\nx0 = ProductRepr(m, pca1)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"The optimal “time labels” are then just the projections t_i = d_iX^*, i=1ldotsn.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"t = map(d -> inner(S, m, pca1, log(S, m, d)), data)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"And we can call the gradient descent. Note that since gradF! works in place of Y, we have to set the evalutation type accordingly.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"y = gradient_descent(\n    M,\n    RegressionCost(data, t),\n    RegressionGradient!(data, t),\n    x0;\n    evaluation=MutatingEvaluation(),\n    stepsize=ArmijoLinesearch(1.0, ExponentialRetraction(), 0.95, 0.1),\n    stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-8),\n    debug=[:Iteration, \" | \", :Cost, \"\\n\", :Stop, 50],\n);\nnothing #hide","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"And plot the result","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"dense_t = range(-0.5, 0.5; length=100)\ngeo = geodesic(S, y[M, :point], y[M, :vector], dense_t)\ninit_geo = geodesic(S, x0[M, :point], x0[M, :vector], dense_t)\ngeo_pts = geodesic(S, y[M, :point], y[M, :vector], t)\ngeo_conn_highlighted = shortest_geodesic(\n    S, data[highlighted], geo_pts[highlighted], 0.5 .+ dense_t\n)\nnothing #hide","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"asymptote_export_S2_signals(\nexport_folder * \"/regression_result1.asy\";\npoints=[data, [y[M, :point],], geo_pts],\ncurves=[init_geo, geo],\ntangent_vectors = [ [Tuple([y[M, :point], y[M, :vector]]),],],\ncolors=Dict(\n    :curves => [black,TolVibrantTeal],\n    :points => [TolVibrantBlue, TolVibrantOrange, TolVibrantTeal],\n    :tvectors => [TolVibrantOrange],\n),\ndot_sizes=[3.5, 3.5, 2],\nline_widths = [0.33, 0.66, 1.0],\ncamera_position=(1.0, 0.5, 0.5),\n)\nrender_asymptote(\"regression_result1.asy\"; render = 2)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"(Image: The result from doing a gradient descent on the tangent bundle)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"In this image, together with the blue data points, you see the geodesic of the initialization in black (evaluated on -frac12frac12), the final point on the tangent bundle in orange, as well as the resulting regression geodesic in teal, (on the same interval as the start) as well as small teal points indicating the time points on the geodesic corresponding to the data. Additionally, a thin blue line indicates the geodesic between a data point and its corresponding data point on the geodesic. While this would be the closest point in Euclidean space and hence the two directions (along the geodesic vs. to the data point) orthogonal, here we have","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"inner(\n    S,\n    geo_pts[highlighted],\n    log(S, geo_pts[highlighted], geo_pts[highlighted + 1]),\n    log(S, geo_pts[highlighted], data[highlighted]),\n)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"But we also started with one of the best scenarios, i.e. equally spaced points on a geodesic obstructed by noise","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"this gets worse if you start with less even distributed data","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"data2 = [exp(S, base, dir, t) for t in [-0.5, -0.49, -0.48, 0.1, 0.48, 0.49, 0.5]]\ndata2 = map(x -> exp(S, x, random_tangent(S, x, :Gaussian, σ / 2)), data2)\nm2 = mean(S, data2)\nA = hcat(map(x -> get_coordinates(S, m, log(S, m, x), DefaultOrthonormalBasis()), data2)...)\npca2 = get_vector(S, m, svd(A).U[:, 1], DefaultOrthonormalBasis())\nx1 = ProductRepr(m, pca2)\nt2 = map(d -> inner(S, m2, pca2, log(S, m2, d)), data2)\ny2 = gradient_descent(\n    M,\n    RegressionCost(data2, t2),\n    RegressionGradient!(data2, t2),\n    x1;\n    evaluation=MutatingEvaluation(),\n    stepsize=ArmijoLinesearch(1.0, ExponentialRetraction(), 0.95, 0.1),\n    stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-8),\n    debug=[:Iteration, \" | \", :Cost, \"\\n\", :Stop, 50],\n)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"(Image: The result from doing a gradient descent on the tangent bundle, unevenspaced noisy data)","category":"page"},{"location":"tutorials/GeodesicRegression.html#unlabeled-data-regression-1","page":"Do Geodesic regression","title":"Unlabeled data","text":"","category":"section"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"If we are not given time points t_i, then the optimization problem extends – informally speaking – to also finding the “best fitting” (in the sense of smallest error). To formalize, the objective function here reads","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"F(p X t) = frac12sum_i=1^n d_mathcal M^2(γ_pX(t_i) d_i)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"where t = (t_1ldotst_n) in mathbb R^n is now an additional parameter of the objective function. We write F_1(p X) to refer to the function on the tangent bundle for fixed values of t (as the one in the last part) and F_2(t) for the function F(p X t) as a function in t with fixed values (p X).","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"For the Euclidean case, there is no neccessity to optimize with respect to t, as we saw above for the initialisation of the fixed time points.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"On a Riemannian manifold this can be stated as a problem on the product manifold mathcal N = mathrmTmathcal M times mathbb R^n, i.e.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"N = M × Euclidean(length(t2))","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"  operatorname*argmin_bigl((pX)tbigr)inmathcal N F(p X t)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"In this tutorial we present an approach to solve this using an alternating gradient descent scheme. To be precise, we define the cost funcion now on the product manifold","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"struct RegressionCost2{T}\n    data::T\nend\nRegressionCost2(data::T) where {T} = RegressionCost2{T}(data)\nfunction (a::RegressionCost2)(N, x)\n    TM = N[1]\n    pts = [\n        geodesic(TM.manifold, x[N, 1][TM, :point], x[N, 1][TM, :vector], ti) for\n        ti in x[N, 2]\n    ]\n    return 1 / 2 * sum(distance.(Ref(TM.manifold), pts, a.data) .^ 2)\nend","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"The gradient in two parts, namely (a) the same gradient as before w.r.t. (pX)  Tmathcal M just now with a fixed t in mind for the second component of the product manifold mathcal N","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"struct RegressionGradient2a!{T}\n    data::T\nend\nRegressionGradient2a!(data::T) where {T} = RegressionGradient2a!{T}(data)\nfunction (a::RegressionGradient2a!)(N, Y, x)\n    TM = N[1]\n    p = x[N, 1]\n    pts = [geodesic(TM.manifold, p[TM, :point], p[TM, :vector], ti) for ti in x[N, 2]]\n    gradients = grad_distance.(Ref(TM.manifold), a.data, pts)\n    Y[TM, :point] .= sum(\n        adjoint_differential_exp_basepoint.(\n            Ref(TM.manifold),\n            Ref(p[TM, :point]),\n            [ti * p[TM, :vector] for ti in x[N, 2]],\n            gradients,\n        ),\n    )\n    Y[TM, :vector] .= sum(\n        adjoint_differential_exp_argument.(\n            Ref(TM.manifold),\n            Ref(p[TM, :point]),\n            [ti * p[TM, :vector] for ti in x[N, 2]],\n            gradients,\n        ),\n    )\n    return Y\nend","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"Finally we addionally look for a fixed point x=(pX)  mathrmTmathcal M at the gradient with respect to tmathbb R^n, i.e. the second component, which is given by","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"  (operatornamegradF_2(t))_i\n  = - dot γ_pX(t_i) log_γ_pX(t_i)d_i_γ_pX(t_i) i = 1 ldots n","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"struct RegressionGradient2b!{T}\n    data::T\nend\nRegressionGradient2b!(data::T) where {T} = RegressionGradient2b!{T}(data)\nfunction (a::RegressionGradient2b!)(N, Y, x)\n    TM = N[1]\n    p = x[N, 1]\n    pts = [geodesic(TM.manifold, p[TM, :point], p[TM, :vector], ti) for ti in x[N, 2]]\n    logs = log.(Ref(TM.manifold), pts, a.data)\n    pt = map(d -> vector_transport_to(TM.manifold, p[TM, :point], p[TM, :vector], d), pts)\n    Y .= -inner.(Ref(TM.manifold), pts, logs, pt)\n    return Y\nend","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"We can reuse the computed initial values from before, just that now we are on a product manifold","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"x2 = ProductRepr(x1, t2)\nF3 = RegressionCost2(data2)\ngradF3_vector = [RegressionGradient2a!(data2), RegressionGradient2b!(data2)]\ny3 = alternating_gradient_descent(\n    N,\n    F3,\n    gradF3_vector,\n    x2;\n    evaluation=MutatingEvaluation(),\n    debug=[:Iteration, \" | \", :Cost, \"\\n\", :Stop, 50],\n    stepsize=ArmijoLinesearch(),\n    inner_iterations=1,\n)\nnothing #hide","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"(Image: The result from doing a gradient descent on the tangent bundle, unevenspaced noisy data)","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"Note that the geodesics from the data to the regression geodesic meet at an nearly orthogonal angle.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"Acknowledgement. Parts of this tutorial are based on the bachelor thesis of Jeremias Arf.","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics, 2018. doi: 10.3389/fams.2018.00059, arXiv: 1807.10090","category":"page"},{"location":"tutorials/GeodesicRegression.html#","page":"Do Geodesic regression","title":"Do Geodesic regression","text":"[Fletcher2013]: Fletcher, P. T., Geodesic regression and the theory of least squares on Riemannian manifolds, International Journal of Computer Vision(105), 2, pp. 171–185, 2013. doi: 10.1007/s11263-012-0591-y","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/MeanAndMedian.jl\"","category":"page"},{"location":"tutorials/MeanAndMedian.html#Optimize-1","page":"get Started: Optimize!","title":"Get started: Optimize!","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"This example illustrates how to set up and solve optimization problems and how to further get data from the algorithm using DebugOptions and RecordOptions. We will use the Riemannian mean and median as simple examples.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"To start from the quite general case: A Solver is an algorithm that aims to solve","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"operatorname*argmin_xmathcal M f(x)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"where mathcal M is a Manifold and fmathcal M  ℝ is the cost function.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"In Manopt.jl a Solver is an algorithm that requires a Problem p and Options o. While former contains static data, most prominently the manifold mathcal M (usually as p.M) and the cost function f (usually as x->get_cost(p, x)), the latter contains dynamic data, i.e. things that usually change during the algorithm, are allowed to change, or specify the details of the algorithm to use. Together they form a plan. A plan uniquely determines the algorithm to use and provide all necessary information to run the algorithm.","category":"page"},{"location":"tutorials/MeanAndMedian.html#Example-1","page":"get Started: Optimize!","title":"Example","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"A gradient plan consists of a GradientProblem with the fields M, cost function f as well as gradient storing the gradient function corresponding to f. Accessing both functions can be done directly but should be encapsulated using get_cost(p,x) and get_gradient(p,x), where in both cases x is a point on the Manifold M. Second, the GradientDescentOptions specify that the algorithm to solve the GradientProblem will be the gradient descent algorithm. It requires an initial value o.x0, a StoppingCriterion o.stop, a Stepsize o.stepsize and a retraction o.retraction and it internally stores the last evaluation of the gradient at o.gradient for convenience. The only mandatory parameter is the initial value x0, though the defaults for both the stopping criterion (StopAfterIteration(100)) as well as the stepsize (ConstantStepsize(1.) are quite conservative, but are chosen to be as simple as possible.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"With these two at hand, running the algorithm just requires to call x_opt = solve(p,o).","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"In the following two examples we will see, how to use a higher level interface that allows to more easily activate for example a debug output or record values during the iterations","category":"page"},{"location":"tutorials/MeanAndMedian.html#The-given-Dataset-1","page":"get Started: Optimize!","title":"The given Dataset","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"using Manopt, Manifolds\nusing Random, Colors","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"For a persistent random set we use","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"n = 100\nσ = π / 8\nM = Sphere(2)\nx = 1 / sqrt(2) * [1.0, 0.0, 1.0]\nRandom.seed!(42)\ndata = [exp(M, x, random_tangent(M, x, Val(:Gaussian), σ)) for i in 1:n]\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"black = RGBA{Float64}(colorant\"#000000\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\")\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\")\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\")\nTolVibrantMagenta = RGBA{Float64}(colorant\"#EE3377\")\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"Then our data rendered using asymptote_export_S2_signals looks like","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"asymptote_export_S2_signals(\"startDataAndCenter.asy\";\n    points = [ [x], data],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal]),\n    dot_size = 3.5, camera_position = (1.,.5,.5)\n)\nrender_asymptote(\"startDataAndCenter.asy\"; render = 2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"(Image: The data of noisy versions of $x$)","category":"page"},{"location":"tutorials/MeanAndMedian.html#Computing-the-Mean-1","page":"get Started: Optimize!","title":"Computing the Mean","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"To compute the mean on the manifold we use the characterization, that the Euclidean mean minimizes the sum of squared distances, and end up with the following cost function. Its minimizer is called Riemannian Center of Mass.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"note: Note\nThere are more sophisticated methods tailored for the specific manifolds available in Manifolds.jl see mean.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"F(M, y) = sum(1 / (2 * n) * distance.(Ref(M), Ref(y), data) .^ 2)\ngradF(M, y) = sum(1 / n * grad_distance.(Ref(M), data, Ref(y)))\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"note that the grad_distance defaults to the case p=2, i.e. the gradient of the squared distance. For details on convergence of the gradient descent for this problem, see [Afsari, Tron, Vidal, 2013]","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"The easiest way to call the gradient descent is now to call gradient_descent","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"xMean = gradient_descent(M, F, gradF, data[1])\nnothing; #hide\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"but in order to get more details, we further add the debug= options, which act as a decorator pattern using the DebugOptions and DebugActions. The latter store values if that's necessary, for example for the DebugChange that prints the change during the last iteration. The following debug prints","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"# i | x: | Last Change: | F(x):`","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"as well as the reason why the algorithm stopped at the end. Here, the format shorthand and the [DebugFactory] are used, which returns a DebugGroup of DebugAction performed each iteration and the stop, respectively.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"xMean = gradient_descent(\n    M,\n    F,\n    gradF,\n    data[1];\n    debug=[:Iteration, \" | \", :x, \" | \", :Change, \" | \", :Cost, \"\\n\", :Stop],\n)\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"A way to get better performance and for convex and coercive costs a guaranteed convergence is to switch the default ConstantStepsize(1.0) with a step size that performs better, for example the ArmijoLinesearch(). We can tweak the default values for the contractionFactor and the sufficientDecrease beyond constant step size which is already quite fast.  This gives","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"xMean2 = gradient_descent(\n    M,\n    F,\n    gradF,\n    data[1];\n    stepsize=ArmijoLinesearch(1.0, ExponentialRetraction(), 0.99, 0.5),\n    debug=[:Iteration, \" | \", :x, \" | \", :Change, \" | \", :Cost, \"\\n\", :Stop],\n)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"which finishes in 5 steaps, just slightly better than the previous computation.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"F(M, xMean) - F(M, xMean2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"Note that other optimization tasks may have other speedup opportunities.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"For even more precision, we can further require a smaller gradient norm. This is done by changing the StoppingCriterion used, where several criteria can be combined using & and/or |.  If we want to decrease the final gradient (from less that 1e-8) norm but keep the maximal number of iterations to be 200, we can run","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"xMean3 = gradient_descent(\n    M,\n    F,\n    gradF,\n    data[1];\n    stepsize=ArmijoLinesearch(1.0, ExponentialRetraction(), 0.99, 0.5),\n    stopping_criterion=StopAfterIteration(200) | StopWhenGradientNormLess(1e-15),\n    debug=[:Iteration, \" | \", :x, \" | \", :Change, \" | \", :Cost, \"\\n\", :Stop],\n)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"which takes 10 iterations but gets a very small gradient, and not much is gained in the cost itself","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"F(M, xMean2) - F(M, xMean3)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"asymptote_export_S2_signals(\"startDataCenterMean.asy\";\n    points = [ [x], data, [xMean] ],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal, TolVibrantOrange]),\n    dot_size = 3.5, camera_position = (1.,.5,.5)\n)\nrender_asymptote(\"startDataCenterMean.asy\"; render = 2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"(Image: The resulting mean (orange))","category":"page"},{"location":"tutorials/MeanAndMedian.html#Computing-the-Median-1","page":"get Started: Optimize!","title":"Computing the Median","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"note: Note\nThere are more sophisticated methods tailored for the specific manifolds available in Manifolds.jl see median.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"Similar to the mean you can also define the median as the minimizer of the distances, see for example [Bačák, 2014], but since this problem is not differentiable, we employ the Cyclic Proximal Point (CPP) algorithm, described in the same reference. We define","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"F2(M, y) = sum(1 / (2 * n) * distance.(Ref(M), Ref(y), data))\nproxes = Function[(M, λ, y) -> prox_distance(M, λ / n, di, y, 1) for di in data]\nnothing #hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"where the Function is a helper for global scope to infer the correct type.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"We then call the cyclic_proximal_point as","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"o = cyclic_proximal_point(\n    M,\n    F2,\n    proxes,\n    data[1];\n    debug=[:Iteration, \" | \", :x, \" | \", :Change, \" | \", :Cost, \"\\n\", 50, :Stop],\n    record=[:Iteration, :Change, :Cost],\n    return_options=true,\n)\nxMedian = get_solver_result(o)\nvalues = get_record(o)\nnothing # hide","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"where the differences to gradient_descent are as follows","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"the third parameter is now an Array of proximal maps\ndebug is reduces to only every 50th iteration\nwe further activated a RecordAction using the record= optional parameter. These work very similar to those in debug, but they collect their data in an array. The high level interface then returns two variables; the values do contain an array of recorded datum per iteration. Here a Tuple containing the iteration, last change and cost respectively; see RecordGroup, RecordIteration, RecordChange, RecordCost as well as the RecordFactory for details. The values contains hence a tuple per iteration, that itself consists of (by order of specification) the iteration number, the last change and the cost function value.","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"These recorded entries read","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"values","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"The resulting median and mean for the data hence are","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"asymptote_export_S2_signals(\"startDataCenterMean.asy\";\n    points = [ [x], data, [xMean], [xMedian] ],\n    colors=Dict(:points => [TolVibrantBlue, TolVibrantTeal, TolVibrantOrange, TolVibrantMagenta]),\n    dot_size = 3.5, camera_position = (1.,.5,.5)\n)\nrender_asymptote(\"startDataCenterMedianAndMean.asy\"; render = 2)","category":"page"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"(Image: The resulting mean (orange) and median (magenta))","category":"page"},{"location":"tutorials/MeanAndMedian.html#Literature-1","page":"get Started: Optimize!","title":"Literature","text":"","category":"section"},{"location":"tutorials/MeanAndMedian.html#","page":"get Started: Optimize!","title":"get Started: Optimize!","text":"<ul>\n<li id=\"Bačák2014\">[<a>Bačák, 2014</a>]\n  Bačák, M: <emph>Computing Medians and Means in Hadamard Spaces.</emph>,\n  SIAM Journal on Optimization, Volume 24, Number 3, pp. 1542–1566,\n  doi: <a href=\"https://doi.org/10.1137/140953393\">10.1137/140953393</a>,\n  arxiv: <a href=\"https://arxiv.org/abs/1210.2145\">1210.2145</a>.</li>\n  <li id=\"AfsariTronVidal2013\">[<a>Afsari, Tron, Vidal, 2013</a>]\n   Afsari, B; Tron, R.; Vidal, R.: <emph>On the Convergence of Gradient\n   Descent for Finding the Riemannian Center of Mass</emph>,\n   SIAM Journal on Control and Optimization, Volume 51, Issue 3,\n   pp. 2230–2260.\n   doi: <a href=\"https://doi.org/10.1137/12086282X\">10.1137/12086282X</a>,\n   arxiv: <a href=\"https://arxiv.org/abs/1201.0925\">1201.0925</a></li>\n</ul>","category":"page"},{"location":"helpers/data.html#Data-1","page":"Data","title":"Data","text":"","category":"section"},{"location":"helpers/data.html#","page":"Data","title":"Data","text":"For some manifolds there are artificial or real application data available that can be loaded using the following data functions","category":"page"},{"location":"helpers/data.html#","page":"Data","title":"Data","text":"Modules = [Manopt]\nPages   = [\"artificialDataFunctions.jl\"]","category":"page"},{"location":"helpers/data.html#Manopt.artificialIn_SAR_image-Tuple{Integer}","page":"Data","title":"Manopt.artificialIn_SAR_image","text":"artificialIn_SAR_image([pts=500])\n\ngenerate an artificial InSAR image, i.e. phase valued data, of size pts x pts points.\n\nThis data set was introduced for the numerical examples in\n\nBergmann, R., Laus, F., Steidl, G., Weinmann, A.: Second Order Differences of Cyclic Data and Applications in Variational Denoising SIAM J. Imaging Sci., 7(4), 2916–2953, 2014. doi: 10.1137/140969993 arxiv: 1405.5349\n\n\n\n\n\n","category":"method"},{"location":"helpers/data.html#Manopt.artificial_S1_signal","page":"Data","title":"Manopt.artificial_S1_signal","text":"artificial_S1_signal([pts=500])\n\ngenerate a real-valued signal having piecewise constant, linear and quadratic intervals with jumps in between. If the resulting manifold the data lives on, is the Circle the data is also wrapped to -pipi).\n\nOptional\n\npts – (500) number of points to sample the function\n\nBergmann, R., Laus, F., Steidl, G., Weinmann, A.: Second Order Differences of Cyclic Data and Applications in Variational Denoising SIAM J. Imaging Sci., 7(4), 2916–2953, 2014. doi: 10.1137/140969993 arxiv: 1405.5349\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S1_signal-Tuple{Real}","page":"Data","title":"Manopt.artificial_S1_signal","text":"artificial_S1_signal(x)\n\nevaluate the example signal f(x) x   01, of phase-valued data introduces in Sec. 5.1 of\n\nBergmann, R., Laus, F., Steidl, G., Weinmann, A.: Second Order Differences of Cyclic Data and Applications in Variational Denoising SIAM J. Imaging Sci., 7(4), 2916–2953, 2014. doi: 10.1137/140969993 arxiv: 1405.5349\n\nfor values outside that intervall, this Signal is missing.\n\n\n\n\n\n","category":"method"},{"location":"helpers/data.html#Manopt.artificial_S1_slope_signal","page":"Data","title":"Manopt.artificial_S1_slope_signal","text":"artificial_S1_slope_signal([pts=500, slope=4.])\n\nCreates a Signal of (phase-valued) data represented on the CircleManifold with increasing slope.\n\nOptional\n\npts – (500) number of points to sample the function.\nslope – (4.0) initial slope that gets increased afterwards\n\nThis data set was introduced for the numerical examples in\n\nBergmann, R., Laus, F., Steidl, G., Weinmann, A.: Second Order Differences of Cyclic Data and Applications in Variational Denoising SIAM J. Imaging Sci., 7(4), 2916–2953, 2014. doi: 10.1137/140969993 arxiv: 1405.5349\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_composite_bezier_curve-Tuple{}","page":"Data","title":"Manopt.artificial_S2_composite_bezier_curve","text":"artificial_S2_composite_bezier_curve()\n\nCreate the artificial curve in the Sphere(2) consisting of 3 segments between the four points\n\np_0 = beginbmatrix001endbmatrix^mathrmT\np_1 = beginbmatrix0-10endbmatrix^mathrmT\np_2 = beginbmatrix-100endbmatrix^mathrmT\np_3 = beginbmatrix00-1endbmatrix^mathrmT\n\nwhere each segment is a cubic Bezér curve, i.e. each point, except p_3 has a first point within the following segment b_i^+, i=012 and a last point within the previous segment, except for p_0, which are denoted by b_i^-, i=123. This curve is differentiable by the conditions b_i^- = gamma_b_i^+p_i(2), i=12, where gamma_ab is the shortest_geodesic connecting a and b. The remaining points are defined as\n\nbeginaligned\n    b_0^+ = exp_p_0fracpi8sqrt2beginpmatrix1-10endpmatrix^mathrmT\n    b_1^+ = exp_p_1-fracpi4sqrt2beginpmatrix-101endpmatrix^mathrmT\n    b_2^+ = exp_p_2fracpi4sqrt2beginpmatrix01-1endpmatrix^mathrmT\n    b_3^- = exp_p_3-fracpi8sqrt2beginpmatrix-110endpmatrix^mathrmT\nendaligned\n\nThis example was used within minimization of acceleration of the paper\n\nBergmann, R., Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve, Front. Appl. Math. Stat. 12, 2018. doi: 10.3389/fams.2018.00059 arxiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"helpers/data.html#Manopt.artificial_S2_lemniscate","page":"Data","title":"Manopt.artificial_S2_lemniscate","text":"artificial_S2_lemniscate(p,t; a=π/2)\n\ngenerate a point from the signal on the Sphere mathbb S^2 by creating the Lemniscate of Bernoulli in the tangent space of p sampled at t and use èxp` to obtain a point on the Sphere.\n\nInput\n\np – the tangent space the Lemniscate is created in\nt – value to sample the Lemniscate at\n\nOptional Values\n\na – (π/2) defines a half axis of the Lemniscate to cover a half sphere.\n\nThis dataset was used in the numerical example of Section 5.1 of\n\nBačák, M., Bergmann, R., Steidl, G., Weinmann, A.: A Second Order Non-Smooth Variational Model for Restoring Manifold-Valued Images SIAM J. Sci. Comput. 38(1), A567–A597, 2016. doi: 10.1137/15M101988X arxiv: 1506.02409\n\n\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_lemniscate","page":"Data","title":"Manopt.artificial_S2_lemniscate","text":"artificial_S2_lemniscate(p [,pts=128,a=π/2,interval=[0,2π])\n\ngenerate a Signal on the Sphere mathbb S^2 by creating the Lemniscate of Bernoulli in the tangent space of p sampled at pts points and use exp to get a signal on the Sphere.\n\nInput\n\np – the tangent space the Lemniscate is created in\npts – (128) number of points to sample the Lemniscate\na – (π/2) defines a half axis of the Lemniscate to cover a  half sphere.\ninterval – ([0,2*π]) range to sample the lemniscate at, the default value refers to one closed curve\n\nThis dataset was used in the numerical example of Section 5.1 of\n\nBačák, M., Bergmann, R., Steidl, G., Weinmann, A.: A Second Order Non-Smooth Variational Model for Restoring Manifold-Valued Images SIAM J. Sci. Comput. 38(1), A567–A597, 2016. doi: 10.1137/15M101988X arxiv: 1506.02409\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_rotation_image","page":"Data","title":"Manopt.artificial_S2_rotation_image","text":"artificial_S2_rotation_image([pts=64, rotations=(.5,.5)])\n\ncreates an image with a rotation on each axis as a parametrization.\n\nOptional Parameters\n\npts – (64) number of pixels along one dimension\nrotations – ((.5,.5)) number of total rotations performed on the axes.\n\nThis dataset was used in the numerical example of Section 5.1 of\n\nBačák, M., Bergmann, R., Steidl, G., Weinmann, A.: A Second Order Non-Smooth Variational Model for Restoring Manifold-Valued Images SIAM J. Sci. Comput. 38(1), A567–A597, 2016. doi: 10.1137/15M101988X arxiv: 1506.02409\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_whirl_image","page":"Data","title":"Manopt.artificial_S2_whirl_image","text":"artificial_S2_whirl_image([pts=64])\n\ngenerate an artificial image of data on the 2 sphere,\n\nArguments\n\npts – (64) size of the image in ptstimespts pixel.\n\nThis example dataset was used in the numerical example in Section 5.5 of\n\nLaus, F., Nikolova, M., Persch, J., Steidl, G.: A Nonlocal Denoising Algorithm for Manifold-Valued Images Using Second Order Statistics, SIAM J. Imaging Sci., 10(1), 416–448, 2017. doi:  10.1137/16M1087114 arxiv: 1607.08481\n\nIt is based on artificial_S2_rotation_image extended by small whirl patches.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_S2_whirl_patch","page":"Data","title":"Manopt.artificial_S2_whirl_patch","text":"artificial_S2_whirl_patch([pts=5])\n\ncreate a whirl within the ptstimespts patch of Sphere(@ref)(2)-valued image data.\n\nThese patches are used within artificial_S2_whirl_image.\n\nOptional Parameters\n\npts – (5) size of the patch. If the number is odd, the center is the north pole.\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_SPD_image","page":"Data","title":"Manopt.artificial_SPD_image","text":"artificial_SPD_image([pts=64, stepsize=1.5])\n\ncreate an artificial image of symmetric positive definite matrices of size ptstimespts pixel with a jump of size stepsize.\n\nThis dataset was used in the numerical example of Section 5.2 of\n\nBačák, M., Bergmann, R., Steidl, G., Weinmann, A.: A Second Order Non-Smooth Variational Model for Restoring Manifold-Valued Images SIAM J. Sci. Comput. 38(1), A567–A597, 2016. doi: 10.1137/15M101988X arxiv: 1506.02409\n\n\n\n\n\n","category":"function"},{"location":"helpers/data.html#Manopt.artificial_SPD_image2","page":"Data","title":"Manopt.artificial_SPD_image2","text":"artificial_SPD_image2([pts=64, fraction=.66])\n\ncreate an artificial image of symmetric positive definite matrices of size ptstimespts pixel with right hand side fraction is moved upwards.\n\nThis data set was introduced in the numerical examples of Section of\n\nBergmann, R., Persch, J., Steidl, G.: A Parallel Douglas Rachford Algorithm for Minimizing ROF-like Functionals on Images with Values in Symmetric Hadamard Manifolds SIAM J. Imaging. Sci. 9(3), pp. 901-937, 2016. doi: 10.1137/15M1052858 arxiv: 1512.02814\n\n\n\n\n\n","category":"function"},{"location":"solvers/alternating_gradient_descent.html#AlternatingGradientDescentSolver-1","page":"Alternating Gradient Descent","title":"Alternating Gradient Descent","text":"","category":"section"},{"location":"solvers/alternating_gradient_descent.html#","page":"Alternating Gradient Descent","title":"Alternating Gradient Descent","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/alternating_gradient_descent.html#","page":"Alternating Gradient Descent","title":"Alternating Gradient Descent","text":"alternating_gradient_descent\nalternating_gradient_descent!","category":"page"},{"location":"solvers/alternating_gradient_descent.html#Manopt.alternating_gradient_descent","page":"Alternating Gradient Descent","title":"Manopt.alternating_gradient_descent","text":"alternating_gradient_descent(M, F, gradF, x)\n\nperform an alternating gradient descent\n\nInput\n\nM – the product manifold mathcal M = mathcal M_1  mathcal M_2   mathcal M_n\nF – the objective function (cost) defined on M.\ngradF – a gradient, that can be of two cases\nis a single function returning a ProductRepr or\nis a vector functions each returning a component part of the whole gradient\nx – an initial value x  mathcal M\n\nOptional\n\nevaluation – (AllocatingEvaluation) specify whether the gradient(s) works by  allocation (default) form gradF(M, x) or MutatingEvaluation in place, i.e.  is of the form gradF!(M, X, x) (elementwise).\nevaluation_order – (:Linear) – whether to use a randomly permuted sequence (:FixedRandom), a per cycle permuted sequence (:Random) or the default :Linear one.\ninner_iterations– (5) how many gradient steps to take in a component before alternating to the next\nstopping_criterion (StopAfterIteration(1000))– a StoppingCriterion\nstepsize (ArmijoLinesearch()) a Stepsize\norder - ([1:n]) the initial permutation, where n is the number of gradients in gradF.\nretraction_method – (default_retraction_method(M)) a retraction(M,x,ξ) to use.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\nnote: Note\nThis Problem requires the ProductManifold from Manifolds.jl, so Manifolds.jl to be loaded.\n\nnote: Note\nThe input of each of the (component) gradients is still the whole vector x, just that all other then the ith input component are assumed to be fixed and just the ith components gradient is computed / returned.\n\n\n\n\n\n","category":"function"},{"location":"solvers/alternating_gradient_descent.html#Manopt.alternating_gradient_descent!","page":"Alternating Gradient Descent","title":"Manopt.alternating_gradient_descent!","text":"alternating_gradient_descent!(M, F, gradF, x)\n\nperform a alternating gradient descent in place of x.\n\nInput\n\nM a manifold mathcal M\nF – the objective functioN (cost)\ngradF – a gradient function, that either returns a vector of the subgradients or is a vector of gradients\nx – an initial value x  mathcal M\n\nfor all optional parameters, see alternating_gradient_descent.\n\n\n\n\n\n","category":"function"},{"location":"solvers/alternating_gradient_descent.html#Problem-1","page":"Alternating Gradient Descent","title":"Problem","text":"","category":"section"},{"location":"solvers/alternating_gradient_descent.html#","page":"Alternating Gradient Descent","title":"Alternating Gradient Descent","text":"AlternatingGradientProblem","category":"page"},{"location":"solvers/alternating_gradient_descent.html#Manopt.AlternatingGradientProblem","page":"Alternating Gradient Descent","title":"Manopt.AlternatingGradientProblem","text":"AlternatingGradientProblem <: Problem\n\nAn alternating gradient problem consists of\n\na ProductManifold M =mathcal M = mathcal M_1    M_n\na cost function F(x)\na gradient operatornamegradF that is either\ngiven as one function operatornamegradF returning a tangent vector X on M or\nan array of gradient functions operatornamegradF_i, ì=1,…,n s each returning a component of the gradient\nwhich might be allocating or mutating variants, but not a mix of both.\n\nnote: Note\nThis Problem requires the ProductManifold from Manifolds.jl, so Manifolds.jl to be loaded.\n\nnote: Note\nThe input of each of the (component) gradients is still the whole vector x, just that all other then the ith input component are assumed to be fixed and just the ith components gradient is computed / returned.\n\nConstructors\n\nAlternatingGradientProblem(M::ProductManifold, F, gradF::Function;\n    evaluation=AllocatingEvaluation()\n)\nAlternatingGradientProblem(M::ProductManifold, F, gradF::AbstractVector{<:Function};\n    evaluation=AllocatingEvaluation()\n)\n\nCreate a alternating gradient problem with an optional cost and the gradient either as one function (returning an array) or a vector of functions.\n\n\n\n\n\n","category":"type"},{"location":"solvers/alternating_gradient_descent.html#Options-1","page":"Alternating Gradient Descent","title":"Options","text":"","category":"section"},{"location":"solvers/alternating_gradient_descent.html#","page":"Alternating Gradient Descent","title":"Alternating Gradient Descent","text":"AlternatingGradientDescentOptions","category":"page"},{"location":"solvers/alternating_gradient_descent.html#Manopt.AlternatingGradientDescentOptions","page":"Alternating Gradient Descent","title":"Manopt.AlternatingGradientDescentOptions","text":"AlternatingGradientDescentOptions <: AbstractGradientDescentOptions\n\nStore the fields for an alternating gradient descent algorithm, see also AlternatingGradientProblem and alternating_gradient_descent.\n\nFields\n\nx the current iterate\nstopping_criterion (StopAfterIteration(1000))– a StoppingCriterion\nstepsize (ConstantStepsize(1.0)) a Stepsize\ninner_iterations– (5) how many gradient steps to take in a component before alternating to the next\nevaluation_order – (:Linear) – whether to use a randomly permuted sequence (:FixedRandom), a per cycle newly permuted sequence (:Random) or the default :Linear evaluation order.\norder the current permutation\nretraction_method – (ExponentialRetraction()) a retraction(M,x,ξ) to use.\n\nConstructor\n\nAlternatingGradientDescentOptions(x)\n\nCreate a AlternatingGradientDescentOptions with start point x. all other fields are optional keyword arguments.\n\n\n\n\n\n","category":"type"},{"location":"solvers/alternating_gradient_descent.html#","page":"Alternating Gradient Descent","title":"Alternating Gradient Descent","text":"Additionally, the options share a DirectionUpdateRule, which chooses the current component, so they can be decorated further; The most inner one should always be the following one though.","category":"page"},{"location":"solvers/alternating_gradient_descent.html#","page":"Alternating Gradient Descent","title":"Alternating Gradient Descent","text":"AlternatingGradient","category":"page"},{"location":"solvers/alternating_gradient_descent.html#Manopt.AlternatingGradient","page":"Alternating Gradient Descent","title":"Manopt.AlternatingGradient","text":"AlternatingGradient <: DirectionUpdateRule\n\nThe default gradient processor, which just evaluates the (alternating) gradient on one of the components\n\n\n\n\n\n","category":"type"},{"location":"functions/manifold.html#Specific-manifold-functions-1","page":"Specific Manifold Functions","title":"Specific manifold functions","text":"","category":"section"},{"location":"functions/manifold.html#","page":"Specific Manifold Functions","title":"Specific Manifold Functions","text":"This small section extends the functions available from ManifoldsBase.jl and Manifolds.jl, espcially a few random generators, that are simpler than the functions available.","category":"page"},{"location":"functions/manifold.html#","page":"Specific Manifold Functions","title":"Specific Manifold Functions","text":"Modules = [Manopt]\nPages   = [\"manifold_functions.jl\"]","category":"page"},{"location":"functions/manifold.html#ManifoldsBase.mid_point-Tuple{AbstractManifold, Any, Any, Any}","page":"Specific Manifold Functions","title":"ManifoldsBase.mid_point","text":"mid_point(M, p, q, x)\nmid_point!(M, y, p, q, x)\n\nCompute the mid point between p and q. If there is more than one mid point of (not neccessarily minimizing) geodesics (e.g. on the sphere), the one nearest to x is returned (in place of y).\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.reflect-Tuple{AbstractManifold, Any, Any}","page":"Specific Manifold Functions","title":"Manopt.reflect","text":"reflect(M, p, x)\nreflect!(M, q, p, x)\n\nreflect the point x from the manifold M at point p, i.e.\n\n    operatornamerefl_p(x) = exp_p(-log_p x)\n\nwhere exp and log denote the exponential and logarithmic map on M. This can also be done in place of q.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.reflect-Tuple{AbstractManifold, Function, Any}","page":"Specific Manifold Functions","title":"Manopt.reflect","text":"reflect(M, f, x)\nreflect!(M, q, f, x)\n\nreflect the point x from the manifold M at the point f(x) of the function f mathcal M  mathcal M, i.e.,\n\n    operatornamerefl_f(x) = operatornamerefl_f(x)(x)\n\nCompute the result in q.\n\nsee also reflect(M,p,x).\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Simplified-random-functions-1","page":"Specific Manifold Functions","title":"Simplified random functions","text":"","category":"section"},{"location":"functions/manifold.html#","page":"Specific Manifold Functions","title":"Specific Manifold Functions","text":"While statistics are available in Manifolds.jl, the following functions provide default random points and vectors on manifolds.","category":"page"},{"location":"functions/manifold.html#","page":"Specific Manifold Functions","title":"Specific Manifold Functions","text":"Modules = [Manopt]\nPages   = [\"random.jl\"]","category":"page"},{"location":"functions/manifold.html#Manopt.random_point","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Sphere, :Gaussian[, σ=1.0])\n\nreturn a random point on the Sphere by projecting a normal distributed vector from within the embedding to the sphere.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_point","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Rotations, :Gaussian [, σ=1.0])\n\nreturn a random point p on the manifold Rotations by generating a (Gaussian) random orthogonal matrix with determinant +1. Let QR = A be the QR decomposition of a random matrix A, then the formula reads p = QD where D is a diagonal matrix with the signs of the diagonal entries of R, i.e.\n\nD_ij=begincases\noperatornamesgn(R_ij)  textif  i=j \n0   textotherwise\nendcases\n\nIt can happen that the matrix gets -1 as a determinant. In this case, the first and second columns are swapped.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{AbstractGroupManifold, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::AbstractGroupManifold, options...)\n\nOn an abstract group manifold, the random point is taken from the internally stored M.manifold.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{AbstractManifold, Symbol, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::AbstractManifold, s::Symbol, options...)\n\ngenerate a random point using a noise model given by s with its additional options just passed on.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{AbstractManifold}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::AbstractManifold)\n\ngenerate a random point on a manifold. By default it uses random_point(M,:Gaussian).\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{Circle, Val{:Uniform}}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Circle, :Uniform)\n\nreturn a random point on the Circle mathbb S^1 by picking a random element from -pipi) uniformly.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{Euclidean}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Euclidean[,:Gaussian, σ::Float64=1.0])\n\ngenerate a random point on the Euclidean manifold M, where the optional parameter determines the type of the entries of the resulting point on the Euclidean space d.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{ProductManifold, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::ProductManifold, options...)\n\nreturn a random point x on Grassmannian manifold M by generating a random (Gaussian) matrix with standard deviation σ in matching size, which is orthonormal.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{TangentBundle{𝔽, M, TVT} where {𝔽, M<:AbstractManifold{𝔽}, TVT<:Manifolds.VectorBundleVectorTransport}, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::TangentBundle, options...)\n\ngenerate a random point on the tangent bundle by calling a random_point and a random_tangent with the given options...\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Tuple{TangentSpaceAtPoint{𝔽, M, TX} where {𝔽, M<:AbstractManifold{𝔽}, TX}, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::TangentSpaceAtPoint, options...)\n\ngenerate a random point in the the tangent space of M.point with the given options....\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{Mt}, Tuple{𝔽}, Tuple{AbstractPowerManifold{𝔽, Mt, NestedPowerRepresentation}, Vararg{Any, N} where N}} where {𝔽, Mt}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::AbstractPowerManifold, options...)\n\ngenerate a random point on the AbstractPowerManifold M given options that are passed on.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{N}, Tuple{SymmetricPositiveDefinite{N}, Val{:Gaussian}}, Tuple{SymmetricPositiveDefinite{N}, Val{:Gaussian}, Float64}} where N","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::SymmetricPositiveDefinite, :Gaussian[, σ=1.0])\n\ngenerate a random symmetric positive definite matrix on the SymmetricPositiveDefinite manifold M.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{k}, Tuple{n}, Tuple{m}, Tuple{FixedRankMatrices{m, n, k, 𝔽} where 𝔽, Vararg{Any, N} where N}} where {m, n, k}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::FixedRankMatrices, options...)\n\nreturn a random point on the FixedRankMatrices manifold. The orthogonal matrices are sampled from the Stiefel manifold and the singular values are sampled uniformly at random.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{𝔽}, Tuple{k}, Tuple{n}, Tuple{Grassmann{n, k, 𝔽}, Val{:Gaussian}}, Tuple{Grassmann{n, k, 𝔽}, Val{:Gaussian}, Float64}} where {n, k, 𝔽}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Grassmannian, :Gaussian [, σ=1.0])\n\nreturn a random point x on Grassmannian manifold M by generating a random (Gaussian) matrix with standard deviation σ in matching size, which is orthonormal.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_point-Union{Tuple{𝔽}, Tuple{k}, Tuple{n}, Tuple{Stiefel{n, k, 𝔽}, Val{:Gaussian}}, Tuple{Stiefel{n, k, 𝔽}, Val{:Gaussian}, Float64}} where {n, k, 𝔽}","page":"Specific Manifold Functions","title":"Manopt.random_point","text":"random_point(M::Stiefel, :Gaussian[, σ=1.0])\n\nreturn a random (Gaussian) point x on the Stiefel manifold M by generating a (Gaussian) matrix with standard deviation σ and return the orthogonalized version, i.e. return ​​the Q component of the QR decomposition of the random matrix of size nk.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Rotations, p[, type=:Gaussian, σ=1.0])\n\nreturn a random tangent vector in the tangent space T_xmathrmSO(n) of the point x on the Rotations manifold M by generating a random skew-symmetric matrix. The function takes the real upper triangular matrix of a (Gaussian) random matrix A with dimension ntimes n and subtracts its transposed matrix. Finally, the matrix is normalized.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Sphere, p[, :Gaussian, σ=1.0])\n\nreturn a random tangent vector in the tangent space of p on the Sphere M.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::SymmetricPositiveDefinite, p, :Rician [,σ = 0.01])\n\ngenerate a random tangent vector in the tangent space of p on the SymmetricPositiveDefinite manifold M by using a Rician distribution with standard deviation σ.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::SymmetricPositiveDefinite, p[, :Gaussian, σ = 1.0])\n\ngenerate a random tangent vector in the tangent space of the point p on the SymmetricPositiveDefinite manifold M by using a Gaussian distribution with standard deviation σ on an ONB of the tangent space.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Stiefel, p[,type=:Gaussian, σ=1.0])\n\nreturn a (Gaussian) random vector from the tangent space T_pmathrmSt(nk) with mean zero and standard deviation σ by projecting a random Matrix onto the  p.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Grassmann, p[,type=:Gaussian, σ=1.0])\n\nreturn a (Gaussian) random vector from the tangent space T_pmathrmGr(nk) with mean zero and standard deviation σ by projecting a random Matrix onto the  p.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Hyperbolic, p, :Gaussian [, σ=1.0])\n\ngenerate a random point on the Hyperbolic manifold by projecting a point from the embedding with respect to the Minkowski metric.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::Circle, p [, :Gaussian, σ=1.0])\n\nreturn a random tangent vector from the tangent space of the point p on the Circle mathbb S^1 by using a normal distribution with mean 0 and standard deviation 1.\n\n\n\n\n\n","category":"function"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{AbstractGroupManifold, Any, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::AbstractGroupManifold, p, options...)\n\nOn an abstract group manifold, the random tangent is taken from the internally stored M.manifolds tangent space at p.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{AbstractManifold, Any, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M, p, options...)\n\ngenerate a random tangent vector in the tangent space of p on M. By default this is a :Gaussian distribution.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{ProductManifold, Any, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::ProductManifold, p)\n\ngenerate a random tangent vector in the tangent space of the point p on the ProductManifold M.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{TangentBundle{𝔽, M, TVT} where {𝔽, M<:AbstractManifold{𝔽}, TVT<:Manifolds.VectorBundleVectorTransport}, Any, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::TangentBundle, p, options...)\n\ngenerate a random tangent vector at p on the tangent bundle by calling random_tangent with the given options... twice.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent-Tuple{TangentSpaceAtPoint{𝔽, M, TX} where {𝔽, M<:AbstractManifold{𝔽}, TX}, Any, Vararg{Any, N} where N}","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::TangentSpaceAtPoint, _, options...)\n\ngenerate a random tangent vector from the tangent space of M.point with the given options..., which is the same as generating a point in the tangent space at M.point.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Manopt.random_tangent-Union{Tuple{k}, Tuple{n}, Tuple{m}, Tuple{FixedRankMatrices{m, n, k, 𝔽} where 𝔽, Any, Vararg{Any, N} where N}} where {m, n, k}","page":"Specific Manifold Functions","title":"Manopt.random_tangent","text":"random_tangent(M::FixedRankMatrices, p, options...)\n\ngenerate a random tangent vector in the tangent space of the point p on the FixedRankMatrices manifold M.\n\n\n\n\n\n","category":"method"},{"location":"functions/manifold.html#Initialize-data-1","page":"Specific Manifold Functions","title":"Initialize data","text":"","category":"section"},{"location":"functions/manifold.html#","page":"Specific Manifold Functions","title":"Specific Manifold Functions","text":"Modules = [Manopt]\nPages   = [\"initialize_data.jl\"]","category":"page"},{"location":"pluto/AutomaticDifferentiation.html#","page":"AD in Manopt","title":"AD in Manopt","text":"<iframe style=\"border:none; width:100%; height: 370rem;\" src=\"AutomaticDifferentiation_pluto.html\"></iframe>","category":"page"},{"location":"solvers/subgradient.html#SubgradientSolver-1","page":"Subgradient method","title":"Subgradient Method","text":"","category":"section"},{"location":"solvers/subgradient.html#","page":"Subgradient method","title":"Subgradient method","text":"subgradient_method\nsubgradient_method!","category":"page"},{"location":"solvers/subgradient.html#Manopt.subgradient_method","page":"Subgradient method","title":"Manopt.subgradient_method","text":"subgradient_method(M, F, ∂F, x)\n\nperform a subgradient method x_k+1 = mathrmretr(x_k s_kF(x_k)),\n\nwhere mathrmretr is a retraction, s_k can be specified as a function but is usually set to a constant value. Though the subgradient might be set valued, the argument ∂F should always return one element from the subgradient, but not necessarily deterministic.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fmathcal Mℝ to minimize\n∂F– the (sub)gradient partial F mathcal M Tmathcal M of F restricted to always only returning one value/element from the subgradient. This function can be passed as an allocation function (M, y) -> X or a mutating function (M, X, y) -> X, see evaluation.\nx – an initial value x  mathcal M\n\nOptional\n\nevaluation – (AllocatingEvaluation) specify whether the subgradient works by  allocation (default) form ∂F(M, y) or MutatingEvaluation in place, i.e. is  of the form ∂F!(M, X, x).\nstepsize – (ConstantStepsize(1.)) specify a Stepsize\nretraction – (default_retraction_method(M)) a retraction(M,x,ξ) to use.\nstopping_criterion – (StopAfterIteration(5000)) a functor, seeStoppingCriterion, indicating when to stop.\nreturn_options – (false) – if activated, the extended result, i.e. the complete Options re returned. This can be used to access recorded values. If set to false (default) just the optimal value x_opt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of the subgradient method\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/subgradient.html#Manopt.subgradient_method!","page":"Subgradient method","title":"Manopt.subgradient_method!","text":"subgradient_method!(M, F, ∂F, x)\n\nperform a subgradient method x_k+1 = mathrmretr(x_k s_kF(x_k)) in place of x\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fmathcal Mℝ to minimize\n∂F- the (sub)gradient partial Fmathcal M Tmathcal M of F restricted to always only returning one value/element from the subgradient. This function can be passed as an allocation function (M, y) -> X or a mutating function (M, X, y) -> X, see evaluation.\nx – an initial value x  mathcal M\n\nfor more details and all optional parameters, see subgradient_method.\n\n\n\n\n\n","category":"function"},{"location":"solvers/subgradient.html#Options-1","page":"Subgradient method","title":"Options","text":"","category":"section"},{"location":"solvers/subgradient.html#","page":"Subgradient method","title":"Subgradient method","text":"SubGradientMethodOptions","category":"page"},{"location":"solvers/subgradient.html#Manopt.SubGradientMethodOptions","page":"Subgradient method","title":"Manopt.SubGradientMethodOptions","text":"SubGradientMethodOptions <: Options\n\nstories option values for a subgradient_method solver\n\nFields\n\nretraction_method – the retration to use within\nstepsize – a Stepsize\nstop – a StoppingCriterion\nx – (initial or current) value the algorithm is at\nx_optimal – optimal value\n∂ the current element from the possible subgradients at x that is used\n\n\n\n\n\n","category":"type"},{"location":"solvers/subgradient.html#","page":"Subgradient method","title":"Subgradient method","text":"For DebugActions and RecordActions to record (sub)gradient, its norm and the step sizes, see the steepest Descent actions.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#tCG-1","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint Truncated Conjugate-Gradient Method","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"The aim is to solve the trust-region subproblem","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"operatorname*argmin_η    T_xmathcalM m_x(η) = F(x) +\noperatornamegradF(x) η_x + frac12 \noperatornameHessF(x)η η_x","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"textst  η η_x leq Δ^2","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"on a manifold by using the Steihaug-Toint truncated conjugate-gradient method. All terms involving the trust-region radius use an inner product w.r.t. the preconditioner; this is because the iterates grow in length w.r.t. the preconditioner, guaranteeing that we do not re-enter the trust-region.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Initialization-1","page":"Steihaug-Toint TCG Method","title":"Initialization","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"Initialize η_0 = η if using randomized approach and η the zero tangent vector otherwise, r_0 = operatornamegradF(x), z_0 = operatornameP(r_0), δ_0 = z_0 and k=0","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Iteration-1","page":"Steihaug-Toint TCG Method","title":"Iteration","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"Repeat until a convergence criterion is reached","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"Set κ = δ_k operatornameHessF(x)δ_k_x,  α =fracr_k z_k_xκ and  η_k η_k_x^* = η_k operatornameP(η_k)_x +  2α η_k operatornameP(δ_k)_x +  α^2  δ_k operatornameP(δ_k)_x.\nIf κ  0 or η_k η_k_x^*  Δ^2  return η_k+1 = η_k + τ δ_k and stop.\nSet η_k^*= η_k + α δ_k, if  η_k η_k_x + frac12 η_k  operatornameHessF (η_k)_x_x  η_k^*  η_k^*_x + frac12 η_k^*  operatornameHessF (η_k)_ x_x  set η_k+1 = η_k else set η_k+1 = η_k^*.\nSet r_k+1 = r_k + α operatornameHessF(δ_k)_x,   z_k+1 = operatornameP(r_k+1),   β = fracr_k+1  z_k+1_xr_k z_k _x and δ_k+1 = -z_k+1 + β δ_k.\nSet k=k+1.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Result-1","page":"Steihaug-Toint TCG Method","title":"Result","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"The result is given by the last computed η_k.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Remarks-1","page":"Steihaug-Toint TCG Method","title":"Remarks","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"The operatornameP() denotes the symmetric, positive deﬁnite preconditioner. It is required if a randomized approach is used i.e. using a random tangent vector η_0 as initial vector. The idea behind it is to avoid saddle points. Preconditioning is simply a rescaling of the variables and thus a redeﬁnition of the shape of the trust region. Ideally operatornameP() is a cheap, positive approximation of the inverse of the Hessian of F at x. On default, the preconditioner is just the identity.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"To step number 2: Obtain τ from the positive root of leftlVert η_k + τ δ_k rightrVert_operatornameP x = Δ what becomes after the conversion of the equation to","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":" τ = frac-η_k operatornameP(δ_k)_x +\n sqrtη_k operatornameP(δ_k)_x^2 +\n δ_k operatornameP(δ_k)_x ( Δ^2 -\n η_k operatornameP(η_k)_x)\n δ_k operatornameP(δ_k)_x","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"It can occur that δ_k operatornameHessF (δ_k)_x_x = κ  0 at iteration k. In this case, the model is not strictly convex, and the stepsize α =fracr_k z_k_x κ computed in step 1. does not give a reduction in the modelfunction m_x(). Indeed, m_x() is unbounded from below along the line η_k + α δ_k. If our aim is to minimize the model within the trust-region, it makes far more sense to reduce m_x() along η_k + α δ_k as much as we can while staying within the trust-region, and this means moving to the trust-region boundary along this line. Thus when κ  0 at iteration k, we replace α = fracr_k z_k_xκ with τ described as above. The other possibility is that η_k+1 would lie outside the trust-region at iteration k (i.e. η_k η_k_x^*   Δ^2 what can be identified with the norm of η_k+1). In particular, when operatornameHessF ()_x is positive deﬁnite and η_k+1 lies outside the trust region, the solution to the trust-region problem must lie on the trust-region boundary. Thus, there is no reason to continue with the conjugate gradient iteration, as it stands, as subsequent iterates will move further outside the trust-region boundary. A sensible strategy, just as in the case considered above, is to move to the trust-region boundary by ﬁnding τ.","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Interface-1","page":"Steihaug-Toint TCG Method","title":"Interface","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"  truncated_conjugate_gradient_descent\n  truncated_conjugate_gradient_descent!","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent","page":"Steihaug-Toint TCG Method","title":"Manopt.truncated_conjugate_gradient_descent","text":"truncated_conjugate_gradient_descent(M, F, gradF, x, η, HessF, trust_region_radius)\n\nsolve the trust-region subproblem\n\noperatorname*argmin_η  T_xM\nm_x(η) quadtextwhere \nm_x(η) = F(x) + operatornamegradF(x)η_x + frac12operatornameHessF(x)ηη_x\n\ntextsuch thatquad ηη_x  Δ^2\n\nwith the truncated_conjugate_gradient_descent. For a description of the algorithm and theorems offering convergence guarantees, see the reference:\n\nP.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: 10.1007/s10208-005-0179-9\nA. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: 10.1137/1.9780898719857\n\nInput\n\nM – a manifold mathcal M\nF – a cost function F mathcal M  ℝ to minimize\ngradF – the gradient operatornamegradF mathcal M  Tmathcal M of F\nHessF – the hessian operatornameHessF(x) T_xmathcal M  T_xmathcal M, X  operatonameHessF(x)X = _ξoperatornamegradf(x)\nx – a point on the manifold x  mathcal M\nη – an update tangential vector η  T_xmathcal M\ntrust_region_radius – a trust-region radius\n\nOptional\n\nevaluation – (AllocatingEvaluation) specify whether the gradient and hessian work by  allocation (default) or MutatingEvaluation in place\npreconditioner – a preconditioner for the hessian H\nθ – 1+θ is the superlinear convergence target rate. The algorithm will   terminate early if the residual was reduced by a power of 1+theta.\nκ – the linear convergence target rate: algorithm will terminate   early if the residual was reduced by a factor of kappa.\nrandomize – set to true if the trust-region solve is to be initiated with a   random tangent vector. If set to true, no preconditioner will be   used. This option is set to true in some scenarios to escape saddle   points, but is otherwise seldom activated.\nproject_vector! : (copyto!) specify a projection operation for tangent vectors   for numerical stability. A function (M, Y, p, X) -> ... working in place of Y.   per default, no projection is perfomed, set it to project! to activate projection.\nstopping_criterion – (StopWhenAny, StopAfterIteration,   StopIfResidualIsReducedByFactor, StopIfResidualIsReducedByPower,   StopWhenCurvatureIsNegative, StopWhenTrustRegionIsExceeded )   a functor inheriting from StoppingCriterion indicating when to stop,   where for the default, the maximal number of iterations is set to the dimension of the   manifold, the power factor is θ, the reduction factor is κ.   .\nreturn_options – (false) – if activated, the extended result, i.e. the   complete Options re returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt is returned\n\nand the ones that are passed to decorate_options for decorators.\n\nOutput\n\nη – an approximate solution of the trust-region subproblem in T_xmathcal M.\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\nsee also\n\ntrust_regions\n\n\n\n\n\n","category":"function"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.truncated_conjugate_gradient_descent!","page":"Steihaug-Toint TCG Method","title":"Manopt.truncated_conjugate_gradient_descent!","text":"truncated_conjugate_gradient_descent!(M, F, gradF, x, η, HessF, trust_region_radius; kwargs...)\n\nsolve the trust-region subproblem in place of x.\n\nInput\n\nInput\n\nM – a manifold mathcal M\nF – a cost function F mathcal M  ℝ to minimize\ngradF – the gradient operatornamegradF mathcal M  Tmathcal M of F\nHessF – the hessian operatornameHessF(x) T_xmathcal M  T_xmathcal M, X  operatonameHessF(x)X = _ξoperatornamegradf(x)\nx – a point on the manifold x  mathcal M\nη – an update tangential vector η  T_xmathcal M\ntrust_region_radius – a trust-region radius\n\nFor more details and all optional arguments, see truncated_conjugate_gradient_descent.\n\n\n\n\n\n","category":"function"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Options-1","page":"Steihaug-Toint TCG Method","title":"Options","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"TruncatedConjugateGradientOptions","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.TruncatedConjugateGradientOptions","page":"Steihaug-Toint TCG Method","title":"Manopt.TruncatedConjugateGradientOptions","text":"TruncatedConjugateGradientOptions <: AbstractHessianOptions\n\ndescribe the Steihaug-Toint truncated conjugate-gradient method, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx : a point, where the trust-region subproblem needs   to be solved\nstop : a function s,r = @(o,iter,ξ,x,xnew) returning a stop   indicator and a reason based on an iteration number, the gradient and the   last and current iterates\ngradient : the gradient at the current iterate\nη : a tangent vector (called update vector), which solves the   trust-region subproblem after successful calculation by the algorithm\nδ : search direction\ntrust_region_radius : the trust-region radius\nresidual : the gradient\nrandomize : indicates if the trust-region solve and so the algorithm is to be       initiated with a random tangent vector. If set to true, no       preconditioner will be used. This option is set to true in some       scenarios to escape saddle points, but is otherwise seldom activated.\nproject_vector! : (copyto!) specify a projection operation for tangent vectors   for numerical stability. A function (M, Y, p, X) -> ... working in place of Y.   per default, no projection is perfomed, set it to project! to activate projection.\n\nConstructor\n\nTruncatedConjugateGradientOptions(x, stop, eta, delta, Delta, res, uR)\n\nconstruct a truncated conjugate-gradient Option with the fields as above.\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Additional-Stopping-Criteria-1","page":"Steihaug-Toint TCG Method","title":"Additional Stopping Criteria","text":"","category":"section"},{"location":"solvers/truncated_conjugate_gradient_descent.html#","page":"Steihaug-Toint TCG Method","title":"Steihaug-Toint TCG Method","text":"StopIfResidualIsReducedByPower\nStopIfResidualIsReducedByFactor\nStopWhenTrustRegionIsExceeded\nStopWhenCurvatureIsNegative\nStopWhenModelIncreased","category":"page"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.StopIfResidualIsReducedByPower","page":"Steihaug-Toint TCG Method","title":"Manopt.StopIfResidualIsReducedByPower","text":"StopIfResidualIsReducedByPower <: StoppingCriterion\n\nA functor for testing if the norm of residual at the current iterate is reduced by a power of 1+θ compared to the norm of the initial residual, i.e. Vert r_k Vert_x leqq  Vert r_0 Vert_x^1+theta. In this case the algorithm reached superlinear convergence.\n\nFields\n\nθ – part of the reduction power\ninitialResidualNorm - stores the norm of the residual at the initial vector   η of the Steihaug-Toint tcg mehtod truncated_conjugate_gradient_descent\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\n\nConstructor\n\nStopIfResidualIsReducedByPower(iRN, θ)\n\ninitialize the StopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN to the power of 1+θ.\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.StopIfResidualIsReducedByFactor","page":"Steihaug-Toint TCG Method","title":"Manopt.StopIfResidualIsReducedByFactor","text":"StopIfResidualIsReducedByFactor <: StoppingCriterion\n\nA functor for testing if the norm of residual at the current iterate is reduced by a factor compared to the norm of the initial residual, i.e. Vert r_k Vert_x leqq κ Vert r_0 Vert_x. In this case the algorithm reached linear convergence.\n\nFields\n\nκ – the reduction factor\ninitialResidualNorm - stores the norm of the residual at the initial vector   η of the Steihaug-Toint tcg mehtod truncated_conjugate_gradient_descent\nreason – stores a reason of stopping if the stopping criterion has one be reached, see get_reason.\n\nConstructor\n\nStopIfResidualIsReducedByFactor(iRN, κ)\n\ninitialize the StopIfResidualIsReducedByFactor functor to indicate to stop after the norm of the current residual is lesser than the norm of the initial residual iRN times κ.\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.StopWhenTrustRegionIsExceeded","page":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenTrustRegionIsExceeded","text":"StopWhenTrustRegionIsExceeded <: StoppingCriterion\n\nA functor for testing if the norm of the next iterate in the  Steihaug-Toint tcg mehtod is larger than the trust-region radius, i.e. Vert η_k^* Vert_x  trust_region_radius. terminate the algorithm when the trust region has been left.\n\nFields\n\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\nstorage – stores the necessary parameters η, δ, residual to check the   criterion.\n\nConstructor\n\nStopWhenTrustRegionIsExceeded([a])\n\ninitialize the StopWhenTrustRegionIsExceeded functor to indicate to stop after the norm of the next iterate is greater than the trust-region radius using the StoreOptionsAction a, which is initialized to store :η, :δ, :residual by default.\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.StopWhenCurvatureIsNegative","page":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenCurvatureIsNegative","text":"StopWhenCurvatureIsNegative <: StoppingCriterion\n\nA functor for testing if the curvature of the model is negative, i.e. langle delta_k operatornameHessF(delta_k)rangle_x leqq 0. In this case, the model is not strictly convex, and the stepsize as computed does not give a reduction of the model.\n\nFields\n\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\n\nConstructor\n\nStopWhenCurvatureIsNegative()\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/truncated_conjugate_gradient_descent.html#Manopt.StopWhenModelIncreased","page":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenModelIncreased","text":"StopWhenModelIncreased <: StoppingCriterion\n\nA functor for testing if the curvature of the model value increased.\n\nFields\n\nreason – stores a reason of stopping if the stopping criterion has one be   reached, see get_reason.\n\nConstructor\n\nStopWhenModelIncreased()\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/particle_swarm.html#ParticleSwarmSolver-1","page":"Particle Swarm Optimization","title":"Particle Swarm Optimization","text":"","category":"section"},{"location":"solvers/particle_swarm.html#","page":"Particle Swarm Optimization","title":"Particle Swarm Optimization","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/particle_swarm.html#","page":"Particle Swarm Optimization","title":"Particle Swarm Optimization","text":"  particle_swarm\n  particle_swarm!","category":"page"},{"location":"solvers/particle_swarm.html#Manopt.particle_swarm","page":"Particle Swarm Optimization","title":"Manopt.particle_swarm","text":"patricle_swarm(M, F)\n\nperform the particle swarm optimization algorithm (PSO), starting with the initial particle positions x_0[Borckmans2010]. The aim of PSO is to find the particle position g on the Manifold M that solves\n\nmin_x mathcalM F(x)\n\nTo this end, a swarm of particles is moved around the Manifold M in the following manner. For every particle k we compute the new particle velocities v_k^(i) in every step i of the algorithm by\n\nv_k^(i) = ω  operatornameT_x_k^(i)gets x_k^(i-1)v_k^(i-1) + c   r_1  operatornameretr_x_k^(i)^-1(p_k^(i)) + s   r_2 operatornameretr_x_k^(i)^-1(g)\n\nwhere x_k^(i) is the current particle position, ω denotes the inertia, c and s are a cognitive and a social weight, respectively, r_j, j=12 are random factors which are computed new for each particle and step, operatornameretr^-1 denotes an inverse retraction on the Manifold M, and operatornameT is a vector transport.\n\nThen the position of the particle is updated as\n\nx_k^(i+1) = operatornameretr_x_k^(i)(v_k^(i))\n\nwhere operatornameretr denotes a retraction on the Manifold M. At the end of each step for every particle, we set\n\np_k^(i+1) = begincases\nx_k^(i+1)   textif  F(x_k^(i+1))F(p_k^(i))\np_k^(i)  textelse\nendcases\n\nand\n\ng_k^(i+1) =begincases\np_k^(i+1)   textif  F(p_k^(i+1))F(g_k^(i))\ng_k^(i)  textelse\nendcases\n\ni.e. p_k^(i) is the best known position for the particle k and g^(i) is the global best known position ever visited up to step i.\n\n[Borckmans2010]: P. B. Borckmans, M. Ishteva, P.-A. Absil, A Modified Particle Swarm Optimization Algorithm for the Best Low Multilinear Rank Approximation of Higher-Order Tensors, In: Dorigo M. et al. (eds) Swarm Intelligence. ANTS 2010. Lecture Notes in Computer Science, vol 6234. Springer, Berlin, Heidelberg, doi 10.1007/978-3-642-15461-4_2\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fmathcal Mℝ to minimize\n\nOptional\n\nn - (100) number of random initial positions of x0\nx0 – the initial positions of each particle in the swarm x_k^(0)  mathcal M for k = 1 dots n, per default these are n random_points\nvelocity – a set of tangent vectors (of type AbstractVector{T}) representing the velocities of the particles, per default a random_tangent per inital position\ninertia – (0.65) the inertia of the patricles\nsocial_weight – (1.4) a social weight factor\ncognitive_weight – (1.4) a cognitive weight factor\nretraction_method – (default_retraction_method(M)) a retraction(M,x,ξ) to use.\ninverse_retraction_method - (default_inverse_retraction_method(M)) an inverse_retraction(M,x,y) to use.\nvector_transport_mthod - (default_vector_transport_method(M)) a vector transport method to use.\nstopping_criterion – (StopWhenAny(StopAfterIteration(500), StopWhenChangeLess(10^{-4}))) a functor inheriting from StoppingCriterion indicating when to stop.\nreturn_options – (false) – if activated, the extended result, i.e. the   complete Options are returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt if returned\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\ng – the resulting point of PSO\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/particle_swarm.html#Manopt.particle_swarm!","page":"Particle Swarm Optimization","title":"Manopt.particle_swarm!","text":"patricle_swarm!(M, F; n=100, x0::AbstractVector=[random_point(M) for i in 1:n], kwargs...)\n\nperform the particle swarm optimization algorithm (PSO), starting with the initial particle positions x_0[Borckmans2010] in place of x0.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fmathcal Mℝ to minimize\n\nOptional\n\nn - (100) number of random initial positions of x0\nx0 – the initial positions of each particle in the swarm x_k^(0)  mathcal M for k = 1 dots n, per default these are n random_points\n\nfor more optional arguments, see particle_swarm.\n\n\n\n\n\n","category":"function"},{"location":"solvers/particle_swarm.html#Options-1","page":"Particle Swarm Optimization","title":"Options","text":"","category":"section"},{"location":"solvers/particle_swarm.html#","page":"Particle Swarm Optimization","title":"Particle Swarm Optimization","text":"ParticleSwarmOptions","category":"page"},{"location":"solvers/particle_swarm.html#Manopt.ParticleSwarmOptions","page":"Particle Swarm Optimization","title":"Manopt.ParticleSwarmOptions","text":"ParticleSwarmOptions{P,T} <: Options\n\nDescribes a particle swarm optimizing algorithm, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx0 – a set of points (of type AbstractVector{P}) on a manifold as initial particle positions\nvelocity – a set of tangent vectors (of type AbstractVector{T}) representing the velocities of the particles\ninertia – (0.65) the inertia of the patricles\nsocial_weight – (1.4) a social weight factor\ncognitive_weight – (1.4) a cognitive weight factor\nstopping_criterion – (StopWhenAny(StopAfterIteration(500), StopWhenChangeLess(10^{-4}))) a functor inheriting from StoppingCriterion indicating when to stop.\nretraction_method – (ExponentialRetraction) the rectraction to use\ninverse_retraction_method - (LogarithmicInverseRetraction) an inverse retraction to use.\n\nConstructor\n\nParticleSwarmOptions(x0, velocity, inertia, social_weight, cognitive_weight, stopping_criterion[, retraction_method=ExponentialRetraction(), inverse_retraction_method=LogarithmicInverseRetraction()])\n\nconstruct a particle swarm Option with the fields and defaults as above.\n\nSee also\n\nparticle_swarm\n\n\n\n\n\n","category":"type"},{"location":"solvers/particle_swarm.html#Literature-1","page":"Particle Swarm Optimization","title":"Literature","text":"","category":"section"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/Benchmark.jl\"","category":"page"},{"location":"tutorials/Benchmark.html#Mutations-1","page":"speed up! using gradF!","title":"Illustration how to use mutating gradient functions","text":"","category":"section"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"When it comes to time critital operations, a main ingredient in Julia are mutating functions, i.e. those that compute in place without additional Memory allocations. In the following the illustrate how to do this with Manopt.jl.","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"Let's start with the same function as in Get Started: Optimize! and compute the mean of some points. Just that here we use the sphere mathbb S^30 and n=800 points.","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"From the just mentioned example, the implementation looks like","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"using Manopt, Manifolds, Random, BenchmarkTools, Test\nRandom.seed!(42)\nm = 30\nM = Sphere(m)\nn = 800\nσ = π / 8\nx = zeros(Float64, m + 1)\nx[2] = 1.0\ndata = [exp(M, x, random_tangent(M, x, Val(:Gaussian), σ)) for i in 1:n];\nnothing #hide","category":"page"},{"location":"tutorials/Benchmark.html#Classical-definition-1","page":"speed up! using gradF!","title":"Classical definition","text":"","category":"section"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"The variant from the previous tutorial defines a cost F(x) and its gradient gradF(x)","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"F(x) = sum(1 / (2 * n) * distance.(Ref(M), Ref(x), data) .^ 2)\ngradF(M, x) = sum(1 / n * grad_distance.(Ref(M), data, Ref(x)))\nnothing #hide","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"we further set the stopping criterion to be a little more strict, then we obtain","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"sc = StopWhenGradientNormLess(1e-10)\nx0 = random_point(M)\nm1 = gradient_descent(M, F, gradF, x0; stopping_criterion=sc)\n\n@btime gradient_descent($M, $F, $gradF, $x0; stopping_criterion=$sc)\nnothing #hide","category":"page"},{"location":"tutorials/Benchmark.html#Inplace-computation-of-the-gradient-1","page":"speed up! using gradF!","title":"Inplace computation of the gradient","text":"","category":"section"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"We can reduce the memory allocations, by implementing the gradient as a functor. The motivation is twofold: On the one hand, we want to avoid variables from global scope, for example the manifold M or the data, to be used within the function For more complicated cost functions it might also be worth considering to do the same.","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"Here we store the data (as reference) and one temporary memory in order to avoid reallocation of memory per grad_distance computation. We have","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"struct grad!{TD,TTMP}\n    data::TD\n    tmp::TTMP\nend\nfunction (gradf!::grad!)(M, X, x)\n    fill!(X, 0)\n    for di in gradf!.data\n        grad_distance!(M, gradf!.tmp, di, x)\n        X .+= gradf!.tmp\n    end\n    X ./= length(gradf!.data)\n    return X\nend","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"Then we just have to initialize the gradient and perform our final benchmark. Note that we also have to interpolate all variables passed to the benchmark with a $.","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"gradF2! = grad!(data, similar(data[1]))\nm2 = deepcopy(x0)\ngradient_descent!(M, F, gradF2!, m2; evaluation=MutatingEvaluation(), stopping_criterion=sc)\n\n@btime gradient_descent!(\n    $M, $F, $gradF2!, m2; evaluation=$(MutatingEvaluation()), stopping_criterion=$sc\n) setup = (m2 = deepcopy($x0))\nnothing #hide","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"Mote that the results m1and m2 are of course the same.","category":"page"},{"location":"tutorials/Benchmark.html#","page":"speed up! using gradF!","title":"speed up! using gradF!","text":"@test distance(M, m1, m2) ≈ 0","category":"page"},{"location":"functions/index.html#Functions-1","page":"Introduction","title":"Functions","text":"","category":"section"},{"location":"functions/index.html#","page":"Introduction","title":"Introduction","text":"There are several functions required within optimization, most prominently costFunctions and gradients. This package includes several cost functions and corresponding gradients, but also corresponding proximal maps for variational methods manifold-valued data. Most of these functions require the evaluation of Differentials or their AdjointDifferentials as well as JacobiFields (e.g. easily to evaluate for symmetric manifolds).","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#StochasticGradientDescentSolver-1","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"","category":"section"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"stochastic_gradient_descent\nstochastic_gradient_descent!","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.stochastic_gradient_descent","page":"Stochastic Gradient Descent","title":"Manopt.stochastic_gradient_descent","text":"stochastic_gradient_descent(M, gradF, x)\n\nperform a stochastic gradient descent\n\nInput\n\nM a manifold mathcal M\ngradF – a gradient function, that either returns a vector of the subgradients or is a vector of gradients\nx – an initial value x  mathcal M\n\nOptional\n\ncost – (missing) you can provide a cost function for example to track the function value\nevaluation – (AllocatingEvaluation) specify whether the gradient(s) works by  allocation (default) form gradF(M, x) or MutatingEvaluation in place, i.e.  is of the form gradF!(M, X, x) (elementwise).\nevaluation_order – (:Random) – whether to use a randomly permuted sequence (:FixedRandom), a per cycle permuted sequence (:Linear) or the default :Random one.\nstopping_criterion (StopAfterIteration(1000))– a StoppingCriterion\nstepsize (ConstantStepsize(1.0)) a Stepsize\norder_type (:RandomOder) a type of ordering of gradient evaluations. values are :RandomOrder, a :FixedPermutation, :LinearOrder\norder - ([1:n]) the initial permutation, where n is the number of gradients in gradF.\nretraction_method – (default_retraction_method(M)) a retraction(M,x,ξ) to use.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.stochastic_gradient_descent!","page":"Stochastic Gradient Descent","title":"Manopt.stochastic_gradient_descent!","text":"stochastic_gradient_descent!(M, gradF, x)\n\nperform a stochastic gradient descent in place of x.\n\nInput\n\nM a manifold mathcal M\ngradF – a gradient function, that either returns a vector of the subgradients or is a vector of gradients\nx – an initial value x  mathcal M\n\nfor all optional parameters, see stochastic_gradient_descent.\n\n\n\n\n\n","category":"function"},{"location":"solvers/stochastic_gradient_descent.html#Options-1","page":"Stochastic Gradient Descent","title":"Options","text":"","category":"section"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"StochasticGradientDescentOptions","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.StochasticGradientDescentOptions","page":"Stochastic Gradient Descent","title":"Manopt.StochasticGradientDescentOptions","text":"StochasticGradientDescentOptions <: AbstractStochasticGradientDescentOptions\n\nStore the following fields for a default stochastic gradient descent algorithm, see also StochasticGradientProblem and stochastic_gradient_descent.\n\nFields\n\nx the current iterate\nstopping_criterion (StopAfterIteration(1000))– a StoppingCriterion\nstepsize (ConstantStepsize(1.0)) a Stepsize\nevaluation_order – (:Random) – whether to use a randomly permuted sequence (:FixedRandom), a per cycle permuted sequence (:Linear) or the default :Random one.\norder the current permutation\nretraction_method – (ExponentialRetraction()) a retraction(M,x,ξ) to use.\n\nConstructor\n\nStochasticGradientDescentOptions(x)\n\nCreate a StochasticGradientDescentOptions with start point x. all other fields are optional keyword arguments.\n\n\n\n\n\n","category":"type"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"Additionally, the options share a DirectionUpdateRule, so you can also apply MomentumGradient and AverageGradient here. The most inner one should always be.","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#","page":"Stochastic Gradient Descent","title":"Stochastic Gradient Descent","text":"AbstractStochasticGradientProcessor\nStochasticGradient","category":"page"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.AbstractStochasticGradientProcessor","page":"Stochastic Gradient Descent","title":"Manopt.AbstractStochasticGradientProcessor","text":"AbstractStochasticGradientDescentOptions <: Options\n\nA generic type for all options related to stochastic gradient descent methods\n\n\n\n\n\n","category":"type"},{"location":"solvers/stochastic_gradient_descent.html#Manopt.StochasticGradient","page":"Stochastic Gradient Descent","title":"Manopt.StochasticGradient","text":"StochasticGradient <: DirectionUpdateRule\n\nThe default gradient processor, which just evaluates the (stochastic) gradient or a subset thereof.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/JacobiFields.jl\"","category":"page"},{"location":"tutorials/JacobiFields.html#Illustration-of-Jacobi-Fields-1","page":"use Jacobi Fields","title":"Illustration of Jacobi Fields","text":"","category":"section"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"This tutorial illustrates the usage of Jacobi Fields within Manopt.jl. For this tutorial you should be familiar with the basic terminology on a manifold like the exponential and logarithmic map as well as shortest geodesics.","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"We first initialize the manifold","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"using Manopt, Manifolds","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"using Colors\nblack = RGBA{Float64}(colorant\"#000000\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\")\nTolVibrantCyan = RGBA{Float64}(colorant\"#33BBEE\")\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\")\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Assume we have two points on the equator of the Sphere mathcal M = mathbb S^2","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"M = Sphere(2)\np, q = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"their connecting shortest geodesic (sampled at 100 points)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"geodesicCurve = shortest_geodesic(M, p, q, [0:0.1:1.0...]);\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"looks as follows using the asymptote_export_S2_signals export","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"asymptote_export_S2_signals(\"jacobiGeodesic.asy\";\n    render = asyResolution,\n    curves=[geodesicCurve], points = [ [x,y] ],\n    colors=Dict(:curves => [black], :points => [TolVibrantOrange]),\n    dot_size = 3.5, line_width = 0.75, camera_position = (1.,1.,.5)\n)\nrender_asymptote(\"jacobiGeodesic.asy\"; render = 2)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"(Image: A geodesic connecting two points on the equator)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"where x is on the left. Then this tutorial solves the following task:","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Given a direction X_p T_xmathcal M, for example","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"X = [0.0, 0.4, 0.5]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"we move the start point x into, how does any point on the geodesic move?","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Or mathematically: Compute D_p g(t pq) for some fixed t01 and a given direction X_p. Of course two cases are quite easy: For t=0 we are in x and how x “moves” is already known, so D_x g(0pq) = X. On the other side, for t=1, g(1 pq) = q which is fixed, so D_p g(1 pq) is the zero tangent vector (in T_qmathcal M).","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"For all other cases we employ a jacobi_field, which is a (tangent) vector field along the shortest geodesic given as follows: The geodesic variation Gamma_gX(st) is defined for some varepsilon  0 as","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Gamma_gX(st)=expgamma_pX(s)tlog_g(spX)pqquad s(-varepsilonvarepsilon) t01","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Intuitively we make a small step s into direction ξ using the geodesic g( pX) and from r=g(s pX) we follow (in t) the geodesic g( rq). The corresponding Jacobi field J_gX along g( pq) is given by","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"J_gX(t)=fracDpartial sGamma_gX(st)Biglrvert_s=0","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"which is an ODE and we know the boundary conditions J_gX(0)=X and J_gX(t) = 0. In symmetric spaces we can compute the solution, since the system of ODEs decouples, see for example do Carmo, Chapter 4.2. Within Manopt.jl this is implemented as jacobi_field(M,p,q,t,X[,β]), where the optional parameter (function) β specifies, which Jacobi field we want to evaluate and the one used here is the default.","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"We can hence evaluate that on the points on the geodesic at","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"T = [0:0.1:1.0...]\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"namely","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"r = shortest_geodesic(M, p, q, T)\nnothing #hide","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"the geodesic moves as","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"W = jacobi_field.(Ref(M), Ref(p), Ref(q), T, Ref(X))","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"which can also be called using differential_geodesic_startpoint. We can add to the image above by creating extended tangent vectors the include their base points","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"V = [Tuple([a, b]) for (a, b) in zip(r, W)]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"and add that as one further set to the Asymptote export.","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"asymptote_export_S2_signals(\"jacobiGeodesicdifferential_geodesic_startpoint.asy\";\n    render = asyResolution,\n    curves=[geodesicCurve], points = [ [x,y], Z], tangent_vectors = [Vx],\n    colors=Dict(\n        :curves => [black],\n        :points => [TolVibrantOrange,TolVibrantCyan],\n        :tvectors => [TolVibrantCyan]\n    ),\n    dot_sizes = [3.5,2.], line_width = 0.75, camera_position = (1.,1.,.5)\n)\nrender_asymptote(\"jacobiGeodesicdifferential_geodesic_startpoint.asy\"; render = 2)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"(Image: A Jacobi field for $D_xg(t,x,y)[η]$)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"If we further move the end point, too, we can derive that Differential in direction","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"Xq = [0.2, 0.0, -0.5]\nW2 = differential_geodesic_endpoint.(Ref(M), Ref(p), Ref(q), T, Ref(Xq))\nV2 = [Tuple([a, b]) for (a, b) in zip(r, W2)]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"and we can combine both keeping the base point","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"V3 = [Tuple([a, b]) for (a, b) in zip(r, W2 + W)]","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"asymptote_export_S2_signals(\"jacobiGeodesicResult.asy\";\n   render = asyResolution,\n   curves=[geodesicCurve], points = [ [x,y], Z], tangent_vectors = [Vx,Vy,Vb],\n   colors=Dict(\n       :curves => [black],\n       :points => [TolVibrantOrange,TolVibrantCyan],\n       :tvectors => [TolVibrantCyan,TolVibrantCyan,TolVibrantTeal]\n  ),\n  dot_sizes = [3.5,2.], line_width = 0.75, camera_position = (1.,1.,0.)\n)\nrender_asymptote(\"jacobiGeodesicResult.asy\"; render = 2)","category":"page"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"(Image: A Jacobi field for the effect of two differentials (blue) in sum (teal))","category":"page"},{"location":"tutorials/JacobiFields.html#Literature-1","page":"use Jacobi Fields","title":"Literature","text":"","category":"section"},{"location":"tutorials/JacobiFields.html#","page":"use Jacobi Fields","title":"use Jacobi Fields","text":"<ul><li id=\"doCarmo1992\">[<a>doCarmo1992</a>] do Carmo, M. P.:\n   <emph>Riemannian Geometry</emph>, Mathematics: Theory & Applications,\n   Birkhäuser Basel, 1992, ISBN: 0-8176-3490-8</li>\n<li id=\"BergmannGousenbourger2018\">[<a>BergmannGousenbourger2018</a>]\n  Bergmann, R.; Gousenbourger, P.-Y.: <emph>A variational model for data\n  fitting on manifolds by minimizing the acceleration of a Bézier curve</emph>,\n  Frontiers in Applied Mathematics and Statistics, 2018.\n  doi: <a href=\"https://dx.doi.org/10.3389/fams.2018.00059\">10.3389/fams.2018.00059</a></li>\n</ul>","category":"page"},{"location":"functions/differentials.html#DifferentialFunctions-1","page":"Differentials","title":"Differentials","text":"","category":"section"},{"location":"functions/differentials.html#","page":"Differentials","title":"Differentials","text":"Modules = [Manopt]\nPages   = [\"functions/differentials.jl\"]","category":"page"},{"location":"functions/differentials.html#Manopt.differential_bezier_control-Tuple{AbstractManifold, AbstractVector{var\"#s53\"} where var\"#s53\"<:BezierSegment, AbstractVector{T} where T, AbstractVector{var\"#s52\"} where var\"#s52\"<:BezierSegment}","page":"Differentials","title":"Manopt.differential_bezier_control","text":"differential_bezier_control(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Ξ::AbstractVector{<:BezierSegment}\n)\ndifferential_bezier_control!(\n    M::AbstractManifold,\n    Θ::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Ξ::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors Ξ in the tangent spaces of the control points. The result is the “change” of the curve at the points in T, which are elementwise in 0N, and each depending the corresponding segment(s). Here, N is the length of B. For the mutating variant the result is computed in Θ.\n\nSee de_casteljau for more details on the curve and [BergmannGousenbourger2018].\n\n[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics, 2018. doi: 10.3389/fams.2018.00059, arXiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_bezier_control-Tuple{AbstractManifold, AbstractVector{var\"#s53\"} where var\"#s53\"<:BezierSegment, Any, AbstractVector{var\"#s52\"} where var\"#s52\"<:BezierSegment}","page":"Differentials","title":"Manopt.differential_bezier_control","text":"differential_bezier_control(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n)\ndifferential_bezier_control!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n)\n\nevaluate the differential of the composite Bézier curve with respect to its control points B and tangent vectors Ξ in the tangent spaces of the control points. The result is the “change” of the curve at t0N, which depends only on the corresponding segment. Here, N is the length of B. The computation can be done in place of Y.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_bezier_control-Tuple{AbstractManifold, BezierSegment, AbstractVector{T} where T, BezierSegment}","page":"Differentials","title":"Manopt.differential_bezier_control","text":"differential_bezier_control(\n    M::AbstractManifold,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n)\ndifferential_bezier_control!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X in the tangent spaces of the control points. The result is the “change” of the curve at the points T, elementwise in t01. The computation can be done in place of Y.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_bezier_control-Tuple{AbstractManifold, BezierSegment, Any, BezierSegment}","page":"Differentials","title":"Manopt.differential_bezier_control","text":"differential_bezier_control(M::AbstractManifold, b::BezierSegment, t::Float, X::BezierSegment)\ndifferential_bezier_control!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    t,\n    X::BezierSegment\n)\n\nevaluate the differential of the Bézier curve with respect to its control points b and tangent vectors X given in the tangent spaces of the control points. The result is the “change” of the curve at t01. The computation can be done in place of Y.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_exp_argument-Tuple{AbstractManifold, Any, Any, Any}","page":"Differentials","title":"Manopt.differential_exp_argument","text":"differential_exp_argument(M, p, X, Y)\ndifferential_exp_argument!(M, Z, p, X, Y)\n\ncomputes D_Xexp_pXY (in place of Z). Note that X   T_X(T_pmathcal M) = T_pmathcal M is still a tangent vector.\n\nSee also\n\ndifferential_exp_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_exp_basepoint-Tuple{AbstractManifold, Any, Any, Any}","page":"Differentials","title":"Manopt.differential_exp_basepoint","text":"differential_exp_basepoint(M, p, X, Y)\ndifferential_exp_basepoint!(M, Z, p, X, Y)\n\nCompute D_pexp_p XY (in place of Z).\n\nSee also\n\ndifferential_exp_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_forward_logs-Tuple{PowerManifold, Any, Any}","page":"Differentials","title":"Manopt.differential_forward_logs","text":"Y = differential_forward_logs(M, p, X)\ndifferential_forward_logs!(M, Y, p, X)\n\ncompute the differential of forward_logs F on the PowerManifold manifold M at p and direction X , in the power manifold array, the differential of the function\n\nF_i(x) = sum_j  mathcal I_i log_p_i p_j quad i  mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM     – a PowerManifold manifold\np     – a point.\nX     – a tangent vector.\n\nOuput\n\nY – resulting tangent vector in T_xmathcal N representing the differentials of the   logs, where mathcal N is the power manifold with the number of dimensions added   to size(x). The computation can also be done in place.\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_geodesic_endpoint-Tuple{AbstractManifold, Any, Any, Any, Any}","page":"Differentials","title":"Manopt.differential_geodesic_endpoint","text":"differential_geodesic_endpoint(M, p, q, t, X)\ndifferential_geodesic_endpoint!(M, Y, p, q, t, X)\n\ncomputes D_qg(tpq)X (in place of Y).\n\nSee also\n\ndifferential_geodesic_startpoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_geodesic_startpoint-Tuple{AbstractManifold, Any, Any, Any, Any}","page":"Differentials","title":"Manopt.differential_geodesic_startpoint","text":"differential_geodesic_startpoint(M, p, q, t, X)\ndifferential_geodesic_startpoint!(M, Y, p, q, t, X)\n\ncomputes D_p g(tpq)η (in place of Y).\n\nSee also\n\ndifferential_geodesic_endpoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_log_argument-Tuple{AbstractManifold, Any, Any, Any}","page":"Differentials","title":"Manopt.differential_log_argument","text":"differential_log_argument(M, p, q, X)\ndifferential_log_argument(M, Y, p, q, X)\n\ncomputes D_qlog_pqX (in place of Y).\n\nSee also\n\ndifferential_log_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/differentials.html#Manopt.differential_log_basepoint-Tuple{AbstractManifold, Any, Any, Any}","page":"Differentials","title":"Manopt.differential_log_basepoint","text":"differential_log_basepoint(M, p, q, X)\ndifferential_log_basepoint!(M, Y, p, q, X)\n\ncomputes D_plog_pqX (in place of Y).\n\nSee also\n\ndifferential_log_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"helpers/errorMeasures.html#ErrorMeasures-1","page":"Error Measures","title":"Error Measures","text":"","category":"section"},{"location":"helpers/errorMeasures.html#","page":"Error Measures","title":"Error Measures","text":"meanSquaredError\nmeanAverageError","category":"page"},{"location":"helpers/errorMeasures.html#Manopt.meanSquaredError","page":"Error Measures","title":"Manopt.meanSquaredError","text":"meanSquaredError(M, p, q)\n\nCompute the (mean) squared error between the two points p and q on the (power) manifold M.\n\n\n\n\n\n","category":"function"},{"location":"helpers/errorMeasures.html#Manopt.meanAverageError","page":"Error Measures","title":"Manopt.meanAverageError","text":"meanSquaredError(M,x,y)\n\nCompute the (mean) squared error between the two points x and y on the PowerManifold manifold M.\n\n\n\n\n\n","category":"function"},{"location":"functions/proximal_maps.html#proximalMapFunctions-1","page":"Proximal Maps","title":"Proximal Maps","text":"","category":"section"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"For a function varphimathcal M ℝ the proximal map is defined as","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"displaystyleoperatornameprox_λvarphi(x)\n= operatorname*argmin_y  mathcal M d_mathcal M^2(xy) + varphi(y)\nquad λ  0","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"where d_mathcal M mathcal M times mathcal M  ℝ denotes the geodesic distance on (\\mathcal M). While it might still be difficult to compute the minimizer, there are several proximal maps known (locally) in closed form. Furthermore if x^star  mathcal M is a minimizer of varphi, then","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"displaystyleoperatornameprox_λvarphi(x^star) = x^star","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"i.e. a minimizer is a fixed point of the proximal map.","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"This page lists all proximal maps available within Manopt. To add you own, just extend the functions/proximal_maps.jl file.","category":"page"},{"location":"functions/proximal_maps.html#","page":"Proximal Maps","title":"Proximal Maps","text":"Modules = [Manopt]\nPages   = [\"proximal_maps.jl\"]","category":"page"},{"location":"functions/proximal_maps.html#Manopt.project_collaborative_TV","page":"Proximal Maps","title":"Manopt.project_collaborative_TV","text":"project_collaborative_TV(M, λ, x, Ξ[, p=2,q=1])\nproject_collaborative_TV!(M, Θ, λ, x, Ξ[, p=2,q=1])\n\ncompute the projection onto collaborative Norm unit (or α-) ball, i.e. of the function\n\nF^q(x) = sum_imathcal G\n  Bigl( sum_jmathcal I_i\n    sum_k=1^d lVert X_ijrVert_x^pBigr)^fracqp\n\nwhere mathcal G is the set of indices for xmathcal M and mathcal I_i is the set of its forward neighbors. The computation can also be done in place of Θ.\n\nThis is adopted from the paper by Duran, Möller, Sbert, Cremers: Collaborative Total Variation: A General Framework for Vectorial TV Models (arxiv: 1508.01308), where the most inner norm is not on a manifold but on a vector space, see their Example 3 for details.\n\n\n\n\n\n","category":"function"},{"location":"functions/proximal_maps.html#Manopt.prox_TV","page":"Proximal Maps","title":"Manopt.prox_TV","text":"ξ = prox_TV(M,λ,x [,p=1])\n\ncompute the proximal maps operatornameprox_λvarphi of all forward differences occurring in the power manifold array, i.e. varphi(xixj) = d_mathcal M^p(xixj) with xi and xj are array elemets of x and j = i+e_k, where e_k is the kth unitvector. The parameter λ is the prox parameter.\n\nInput\n\nM – a Manifold\nλ – a real value, parameter of the proximal map\nx – a point.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\ny – resulting  point containing with all mentioned proximal points evaluated (in a cyclic order). The computation can also be done in place\n\n\n\n\n\n","category":"function"},{"location":"functions/proximal_maps.html#Manopt.prox_TV-Union{Tuple{T}, Tuple{AbstractManifold, Number, Tuple{T, T}}, Tuple{AbstractManifold, Number, Tuple{T, T}, Int64}} where T","page":"Proximal Maps","title":"Manopt.prox_TV","text":"[y1,y2] = prox_TV(M, λ, [x1,x2] [,p=1])\nprox_TV!(M, [y1,y2] λ, [x1,x2] [,p=1])\n\nCompute the proximal map operatornameprox_λvarphi of φ(xy) = d_mathcal M^p(xy) with parameter λ.\n\nInput\n\nM – a Manifold\nλ – a real value, parameter of the proximal map\n(x1,x2) – a tuple of two points,\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\n(y1,y2) – resulting tuple of points of the operatornameprox_λφ((x1,x2)). The result can also be computed in place.\n\n\n\n\n\n","category":"method"},{"location":"functions/proximal_maps.html#Manopt.prox_TV2-Union{Tuple{T}, Tuple{AbstractManifold, Any, Tuple{T, T, T}}, Tuple{AbstractManifold, Any, Tuple{T, T, T}, Int64}} where T","page":"Proximal Maps","title":"Manopt.prox_TV2","text":"(y1,y2,y3) = prox_TV2(M,λ,(x1,x2,x3),[p=1], kwargs...)\nprox_TV2!(M, y, λ,(x1,x2,x3),[p=1], kwargs...)\n\nCompute the proximal map operatornameprox_λvarphi of varphi(x_1x_2x_3) = d_mathcal M^p(c(x_1x_3)x_2) with parameter λ>0, where c(xz) denotes the mid point of a shortest geodesic from x1 to x3 that is closest to x2. The result can be computed in place of y.\n\nInput\n\nM          – a manifold\nλ          – a real value, parameter of the proximal map\n(x1,x2,x3) – a tuple of three points\np – (1) exponent of the distance of the TV term\n\nOptional\n\nkwargs... – parameters for the internal subgradient_method     (if M is neither Euclidean nor Circle, since for these a closed form     is given)\n\nOutput\n\n(y1,y2,y3) – resulting tuple of points of the proximal map. The computation can also be done in place.\n\n\n\n\n\n","category":"method"},{"location":"functions/proximal_maps.html#Manopt.prox_TV2-Union{Tuple{T}, Tuple{N}, Tuple{PowerManifold{N, T, TSize, TPR} where {TSize, TPR<:AbstractPowerRepresentation}, Any, Any}, Tuple{PowerManifold{N, T, TSize, TPR} where {TSize, TPR<:AbstractPowerRepresentation}, Any, Any, Int64}} where {N, T}","page":"Proximal Maps","title":"Manopt.prox_TV2","text":"y = prox_TV2(M, λ, x[, p=1])\nprox_TV2!(M, y, λ, x[, p=1])\n\ncompute the proximal maps operatornameprox_λvarphi of all centered second order differences occuring in the power manifold array, i.e. varphi(x_kx_ix_j) = d_2(x_kx_ix_j), where kj are backward and forward neighbors (along any dimension in the array of x). The parameter λ is the prox parameter.\n\nInput\n\nM – a Manifold\nλ – a real value, parameter of the proximal map\nx – a points.\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOutput\n\ny – resulting point with all mentioned proximal points evaluated (in a cylic order). The computation can also be done in place.\n\n\n\n\n\n","category":"method"},{"location":"functions/proximal_maps.html#Manopt.prox_distance","page":"Proximal Maps","title":"Manopt.prox_distance","text":"y = prox_distance(M,λ,f,x [, p=2])\nprox_distance!(M, y, λ, f, x [, p=2])\n\ncompute the proximal map operatornameprox_λvarphi with parameter λ of φ(x) = frac1pd_mathcal M^p(fx). For the mutating variant the computation is done in place of y.\n\nInput\n\nM – a Manifold mathcal M\nλ – the prox parameter\nf – a point f  mathcal M (the data)\nx – the argument of the proximal map\n\nOptional argument\n\np – (2) exponent of the distance.\n\nOuput\n\ny – the result of the proximal map of φ\n\n\n\n\n\n","category":"function"},{"location":"functions/proximal_maps.html#Manopt.prox_parallel_TV","page":"Proximal Maps","title":"Manopt.prox_parallel_TV","text":"y = prox_parallel_TV(M, λ, x [,p=1])\nprox_parallel_TV!(M, y, λ, x [,p=1])\n\ncompute the proximal maps operatornameprox_λφ of all forward differences occurring in the power manifold array, i.e. φ(x_ix_j) = d_mathcal M^p(x_ix_j) with xi and xj are array elements of x and j = i+e_k, where e_k is the kth unit vector. The parameter λ is the prox parameter.\n\nInput\n\nM     – a PowerManifold manifold\nλ     – a real value, parameter of the proximal map\nx     – a point\n\nOptional\n\n(default is given in brackets)\n\np – (1) exponent of the distance of the TV term\n\nOuput\n\ny  – resulting Array of points with all mentioned proximal points evaluated (in a parallel within the arrays elements). The computation can also be done in place.\n\nSee also prox_TV\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#planSection-1","page":"Plans","title":"Plans for solvers","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"CurrentModule = Manopt","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"In order to start a solver, both a Problem and Options are required. Together they form a plan and these are stored in this folder. For sub-problems there are maybe also only Options, since they than refer to the same problem.","category":"page"},{"location":"plans/index.html#Options-1","page":"Plans","title":"Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"For most algorithms a certain set of options can either be generated beforehand of the function with keywords can be used. Generally the type","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Options\nget_options","category":"page"},{"location":"plans/index.html#Manopt.Options","page":"Plans","title":"Manopt.Options","text":"Options\n\nA general super type for all options.\n\nFields\n\nThe following fields are assumed to be default. If you use different ones, provide the access functions accordingly\n\nx a point with the current iterate\nstop a StoppingCriterion.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_options","page":"Plans","title":"Manopt.get_options","text":"get_options(o::Options)\n\nreturn the undecorated Options of the (possibly) decorated o. As long as your decorated options store the options within o.options and the dispatch_options_decorator is set to Val{true}, the internal options are extracted.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Since the Options directly relate to a solver, they are documented with the corresponding Solvers. You can always access the options (since they might be decorated) by calling get_options.","category":"page"},{"location":"plans/index.html#Decorators-for-Options-1","page":"Plans","title":"Decorators for Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Options can be decorated using the following trait and function to initialize","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"dispatch_options_decorator\nis_options_decorator\ndecorate_options","category":"page"},{"location":"plans/index.html#Manopt.dispatch_options_decorator","page":"Plans","title":"Manopt.dispatch_options_decorator","text":"dispatch_options_decorator(o::Options)\n\nIndicate internally, whether an Options o to be of decorating type, i.e. it stores (encapsulates) options in itself, by default in the field o. options.\n\nDecorators indicate this by returning Val{true} for further dispatch.\n\nThe default is Val{false}, i.e. by default an options is not decorated.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.is_options_decorator","page":"Plans","title":"Manopt.is_options_decorator","text":"is_options_decorator(o::Options)\n\nIndicate, whether Options o are of decorator type.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.decorate_options","page":"Plans","title":"Manopt.decorate_options","text":"decorate_options(o)\n\ndecorate the Optionso with specific decorators.\n\nOptional Arguments\n\noptional arguments provide necessary details on the decorators. A specific one is used to activate certain decorators.\n\ndebug – (Array{Union{Symbol,DebugAction,String,Int},1}()) a set of symbols representing DebugActions, Strings used as dividers and a subsampling integer. These are passed as a DebugGroup within :All to the DebugOptions decorator dictionary. Only excention is :Stop that is passed to :Stop.\nrecord – (Array{Union{Symbol,RecordAction,Int},1}()) specify recordings by using Symbols or RecordActions directly. The integer can again be used for only recording every ith iteration.\n\nSee also\n\nDebugOptions, RecordOptions\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"In general decorators often perform actions so we introduce","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"AbstractOptionsAction","category":"page"},{"location":"plans/index.html#Manopt.AbstractOptionsAction","page":"Plans","title":"Manopt.AbstractOptionsAction","text":"AbstractOptionsAction\n\na common Type for AbstractOptionsActions that might be triggered in decoraters, for example DebugOptions or RecordOptions.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"as well as a helper for storing values using keys, i.e.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"StoreOptionsAction\nget_storage\nhas_storage\nupdate_storage!","category":"page"},{"location":"plans/index.html#Manopt.StoreOptionsAction","page":"Plans","title":"Manopt.StoreOptionsAction","text":"StoreTupleAction <: AbstractOptionsAction\n\ninternal storage for AbstractOptionsActions to store a tuple of fields from an Optionss\n\nThis functor posesses the usual interface of functions called during an iteration, i.e. acts on (p,o,i), where p is a Problem, o is an Options and i is the current iteration.\n\nFields\n\nvalues – a dictionary to store interims values based on certain Symbols\nkeys – an NTuple of Symbols to refer to fields of Options\nonce – whether to update the internal values only once per iteration\nlastStored – last iterate, where this AbstractOptionsAction was called (to determine once\n\nConstructiors\n\nStoreOptionsAction([keys=(), once=true])\n\nInitialize the Functor to an (empty) set of keys, where once determines whether more that one update per iteration are effective\n\nStoreOptionsAction(keys, once=true])\n\nInitialize the Functor to a set of keys, where the dictionary is initialized to be empty. Further, once determines whether more that one update per iteration are effective, otherwise only the first update is stored, all others are ignored.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_storage","page":"Plans","title":"Manopt.get_storage","text":"get_storage(a,key)\n\nreturn the internal value of the StoreOptionsAction a at the Symbol key.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.has_storage","page":"Plans","title":"Manopt.has_storage","text":"get_storage(a,key)\n\nreturn whether the StoreOptionsAction a has a value stored at the Symbol key.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.update_storage!","page":"Plans","title":"Manopt.update_storage!","text":"update_storage!(a,o)\n\nupdate the StoreOptionsAction a internal values to the ones given on the Options o.\n\n\n\n\n\nupdate_storage!(a,o)\n\nupdate the StoreOptionsAction a internal values to the ones given in the dictionary d. The values are merged, where the values from d are preferred.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#DebugOptions-1","page":"Plans","title":"Debug Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Modules = [Manopt]\nPages = [\"plans/debug_options.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"plans/index.html#Manopt.DebugAction","page":"Plans","title":"Manopt.DebugAction","text":"DebugAction\n\nA DebugAction is a small functor to print/issue debug output. The usual call is given by (p,o,i) -> s that performs the debug based on a Problem p, Options o and the current iterate i.\n\nBy convention i=0 is interpreted as \"For Initialization only\", i.e. only debug info that prints initialization reacts, i<0 triggers updates of variables internally but does not trigger any output. Finally typemin(Int) is used to indicate a call from stop_solver! that returns true afterwards.\n\nFields (assumed by subtypes to exist)\n\nprint method to perform the actual print. Can for example be set to a file export,\n\nor to @info. The default is the print function on the default Base.stdout.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugChange","page":"Plans","title":"Manopt.DebugChange","text":"DebugChange()\n\ndebug for the amount of change of the iterate (stored in o.x of the Options) during the last iteration. See DebugEntryChange for the general case\n\nKeyword Parameters\n\nstorage – (StoreOptionsAction( (:x,) )) – (eventually shared) the storage of the previous action\nprefix – (\"Last Change:\") prefix of the debug output (ignored if you set format)\nio – (stdout) default steream to print the debug to.\nformat - ( \"$prefix %f\") format to print the output using an sprintf format.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugCost","page":"Plans","title":"Manopt.DebugCost","text":"DebugCost <: DebugAction\n\nprint the current cost function value, see get_cost.\n\nConstructors\n\nDebugCost()\n\nParameters\n\nformat - (\"$prefix %f\") format to print the output using sprintf and a prefix (see long).\nio – (stdout) default steream to print the debug to.\nlong - (false) short form to set the format to F(x): (default) or current cost: and the cost\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugDivider","page":"Plans","title":"Manopt.DebugDivider","text":"DebugDivider <: DebugAction\n\nprint a small divider (default \" | \").\n\nConstructor\n\nDebugDivider(div,print)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugEntry","page":"Plans","title":"Manopt.DebugEntry","text":"DebugEntry <: RecordAction\n\nprint a certain fields entry of type {T} during the iterates, where a format can be specified how to print the entry.\n\nAddidtional Fields\n\nfield – Symbol the entry can be accessed with within Options\n\nConstructor\n\nDebugEntry(f[, prefix=\"$f:\", format = \"$prefix %s\", io=stdout])\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugEntryChange","page":"Plans","title":"Manopt.DebugEntryChange","text":"DebugEntryChange{T} <: DebugAction\n\nprint a certain entries change during iterates\n\nAdditional Fields\n\nprint – (print) function to print the result\nprefix – (\"Change of :x\") prefix to the print out\nformat – (\"$prefix %e\") format to print (uses the `prefix by default and scientific notation)\nfield – Symbol the field can be accessed with within Options\ndistance – function (p,o,x1,x2) to compute the change/distance between two values of the entry\nstorage – a StoreOptionsAction to store the previous value of :f\n\nConstructors\n\nDebugEntryChange(f,d)\n\nKeyword arguments\n\nio (stdout) an IOStream\nprefix (\"Change of $f\")\nstorage (StoreOptionsAction((f,))) a StoreOptionsAction\ninitial_value an initial value for the change of o.field.\nformat – (\"$prefix %e\") format to print the change\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugEvery","page":"Plans","title":"Manopt.DebugEvery","text":"DebugEvery <: DebugAction\n\nevaluate and print debug only every ith iteration. Otherwise no print is performed. Whether internal variables are updates is determined by alwaysUpdate.\n\nThis method does not perform any print itself but relies on it's childrens print.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugGroup","page":"Plans","title":"Manopt.DebugGroup","text":"DebugGroup <: DebugAction\n\ngroup a set of DebugActions into one action, where the internal prints are removed by default and the resulting strings are concatenated\n\nConstructor\n\nDebugGroup(g)\n\nconstruct a group consisting of an Array of DebugActions g, that are evaluated en bloque; the method does not perform any print itself, but relies on the internal prints. It still concatenates the result and returns the complete string\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugIterate","page":"Plans","title":"Manopt.DebugIterate","text":"DebugIterate <: DebugAction\n\ndebug for the current iterate (stored in o.x).\n\nConstructor\n\nDebugIterate()\n\nParameters\n\nio – (stdout) default steream to print the debug to.\nlong::Bool whether to print x: or current iterate\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugIteration","page":"Plans","title":"Manopt.DebugIteration","text":"DebugIteration <: DebugAction\n\nConstructor\nDebugIteration()\n\nKeyword parameters\n\nformat - (\"# %-6d\") format to print the output using an sprintf format.\nio – (stdout) default steream to print the debug to.\n\ndebug for the current iteration (prefixed with # by )\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugOptions","page":"Plans","title":"Manopt.DebugOptions","text":"DebugOptions <: Options\n\nThe debug options append to any options a debug functionality, i.e. they act as a decorator pattern. Internally a Dictionary is kept that stores a DebugAction for several occasions using a Symbol as reference. The default occasion is :All and for example solvers join this field with :Start, :Step and :Stop at the beginning, every iteration or the end of the algorithm, respectively\n\nThe original options can still be accessed using the get_options function.\n\nFields (defaults in brackets)\n\noptions – the options that are extended by debug information\ndebugDictionary – a Dict{Symbol,DebugAction} to keep track of Debug for different actions\n\nConstructors\n\nDebugOptions(o,dA)\n\nconstruct debug decorated options, where dD can be\n\na DebugAction, then it is stored within the dictionary at :All\nan Array of DebugActions, then it is stored as a debugDictionary within :All.\na Dict{Symbol,DebugAction}.\nan Array of Symbols, String and an Int for the DebugFactory\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugStoppingCriterion","page":"Plans","title":"Manopt.DebugStoppingCriterion","text":"DebugStoppingCriterion <: DebugAction\n\nprint the Reason provided by the stopping criterion. Usually this should be empty, unless the algorithm stops.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DebugActionFactory-Tuple{String}","page":"Plans","title":"Manopt.DebugActionFactory","text":"DebugActionFactory(s)\n\ncreate a DebugAction where\n\na Stringyields the correspoinding divider\na DebugAction is passed through\na [Symbol] creates DebugEntry of that symbol, with the exceptions of :Change, :Iterate, :Iteration, and :Cost.\na Tuple{Symbol,String} creates a DebugEntry of that symbol where the String specifies the format.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.DebugFactory-Tuple{Vector{var\"#s22\"} where var\"#s22\"}","page":"Plans","title":"Manopt.DebugFactory","text":"DebugFactory(a)\n\ngiven an array of Symbols, Strings DebugActions and Ints\n\nThe symbol :Stop creates an entry of to display the stopping criterion at the end (:Stop => DebugStoppingCriterion())\nThe symbol :Cost creates a DebugCost\nThe symbol :iteration creates a DebugIteration\nThe symbol :Change creates a DebugChange\nany other symbol creates debug output of the corresponding field in Options\nany string creates a DebugDivider\nany DebugAction is directly included\nan Integer kintroduces that debug is only printed every kth iteration\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"see DebugSolver for details on the decorated solver.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Further specific DebugActions can be found at the specific Options.","category":"page"},{"location":"plans/index.html#RecordOptions-1","page":"Plans","title":"Record Options","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Modules = [Manopt]\nPages = [\"plans/record_options.jl\"]\nOrder = [:type, :function]\nPrivate = false","category":"page"},{"location":"plans/index.html#Manopt.RecordAction","page":"Plans","title":"Manopt.RecordAction","text":"RecordAction\n\nA RecordAction is a small functor to record values. The usual call is given by (p,o,i) -> s that performs the record based on a Problem p, Options o and the current iterate i.\n\nBy convention i<=0 is interpreted as \"For Initialization only\", i.e. only initialize internal values, but not trigger any record, the same holds for i=typemin(Inf) which is used to indicate stop, i.e. that the record is called from within stop_solver! which returns true afterwards.\n\nFields (assumed by subtypes to exist)\n\nrecorded_values an Array of the recorded values.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordChange","page":"Plans","title":"Manopt.RecordChange","text":"RecordChange <: RecordAction\n\ndebug for the amount of change of the iterate (stored in o.x of the Options) during the last iteration.\n\nAdditional Fields\n\nstorage a StoreOptionsAction to store (at least) o.x to use this as the last value (to compute the change)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordCost","page":"Plans","title":"Manopt.RecordCost","text":"RecordCost <: RecordAction\n\nrecord the current cost function value, see get_cost.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordEntry","page":"Plans","title":"Manopt.RecordEntry","text":"RecordEntry{T} <: RecordAction\n\nrecord a certain fields entry of type {T} during the iterates\n\nFields\n\nrecorded_values – the recorded Iterates\nfield – Symbol the entry can be accessed with within Options\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordEntryChange","page":"Plans","title":"Manopt.RecordEntryChange","text":"RecordEntryChange{T} <: RecordAction\n\nrecord a certain entries change during iterates\n\nAdditional Fields\n\nrecorded_values – the recorded Iterates\nfield – Symbol the field can be accessed with within Options\ndistance – function (p,o,x1,x2) to compute the change/distance between two values of the entry\nstorage – a StoreOptionsAction to store (at least) getproperty(o, d.field)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordEvery","page":"Plans","title":"Manopt.RecordEvery","text":"RecordEvery <: RecordAction\n\nrecord only every ith iteration. Otherwise (optionally, but activated by default) just update internal tracking values.\n\nThis method does not perform any record itself but relies on it's childrens methods\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordGroup","page":"Plans","title":"Manopt.RecordGroup","text":"RecordGroup <: RecordAction\n\ngroup a set of RecordActions into one action, where the internal RecordActions act independently, but the results can be collected in a grouped fashion, i.e. tuples per calls of this group. The enries can be later addressed either by index or semantic Symbols\n\nConstructors\n\nRecordGroup(g::Array{<:RecordAction, 1})\n\nconstruct a group consisting of an Array of RecordActions g,\n\nRecordGroup(g, symbols)\n\nExamples\n\nr = RecordGroup([RecordIteration(), RecordCost()])\n\nA RecordGroup to record the current iteration and the cost. The cost can then be accessed using get_record(r,2) or r[2].\n\nr = RecordGroup([RecordIteration(), RecordCost()], Dict(:Cost => 2))\n\nA RecordGroup to record the current iteration and the cost, wich can then be accesed using get_record(:Cost) or r[:Cost].\n\nr = RecordGroup([RecordIteration(), :Cost => RecordCost()])\n\nA RecordGroup identical to the previous constructor, just a little easier to use.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordIterate","page":"Plans","title":"Manopt.RecordIterate","text":"RecordIterate <: RecordAction\n\nrecord the iterate\n\nConstructors\n\nRecordIterate(x0)\n\ninitialize the iterate record array to the type of x0, e.g. your initial data.\n\nRecordIterate(P)\n\ninitialize the iterate record array to the data type T.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordIteration","page":"Plans","title":"Manopt.RecordIteration","text":"RecordIteration <: RecordAction\n\nrecord the current iteration\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordOptions","page":"Plans","title":"Manopt.RecordOptions","text":"RecordOptions <: Options\n\nappend to any Options the decorator with record functionality, Internally a Dictionary is kept that stores a RecordAction for several concurrent modes using a Symbol as reference. The default mode is :Iteration, which is used to store information that is recorded during the iterations. RecordActions might be added to :Start or :Stop to record values at the beginning or for the stopping time point, respectively\n\nThe original options can still be accessed using the get_options function.\n\nFields\n\noptions – the options that are extended by debug information\nrecordDictionary – a Dict{Symbol,RecordAction} to keep track of all different recorded values\n\nConstructors\n\nRecordOptions(o,dR)\n\nconstruct record decorated Options, where dR can be\n\na RecordAction, then it is stored within the dictionary at :Iteration\nan Array of RecordActions, then it is stored as a recordDictionary(@ref) within the dictionary at :All.\na Dict{Symbol,RecordAction}.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.RecordActionFactory-Tuple{Options, RecordAction}","page":"Plans","title":"Manopt.RecordActionFactory","text":"RecordActionFactory(s)\n\ncreate a RecordAction where\n\na RecordAction is passed through\na [Symbol] creates RecordEntry of that symbol, with the exceptions of :Change, :Iterate, :Iteration, and :Cost.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.RecordFactory-Tuple{Options, Vector{var\"#s10\"} where var\"#s10\"}","page":"Plans","title":"Manopt.RecordFactory","text":"RecordFactory(o::Options, a)\n\ngiven an array of Symbols and RecordActions and Ints\n\nThe symbol :Cost creates a RecordCost\nThe symbol :iteration creates a RecordIteration\nThe symbol :Change creates a RecordChange\nany other symbol creates a RecordEntry of the corresponding field in Options\nany RecordAction is directly included\nan semantic pair :symbol => RecordAction is directly included\nan Integer k introduces that record is only performed every kth iteration\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.get_record","page":"Plans","title":"Manopt.get_record","text":"get_record(o::Options, [,s=:Iteration])\nget_record(o::RecordOptions, [,s=:Iteration])\n\nreturn the recorded values from within the RecordOptions o that where recorded with respect to the Symbol s as an Array. The default refers to any recordings during an :Iteration.\n\nWhen called with arbitrary Options, this method looks for the RecordOptions decorator and calls get_record on the decorator.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.get_record-Tuple{RecordAction, Any}","page":"Plans","title":"Manopt.get_record","text":"get_record(r::RecordAction)\n\nreturn the recorded values stored within a RecordAction r.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.get_record-Tuple{RecordGroup}","page":"Plans","title":"Manopt.get_record","text":"get_record(r::RecordGroup)\n\nreturn an array of tuples, where each tuple is a recorded set, e.g. per iteration / record call.\n\nget_record(r::RecordGruop, i::Int)\n\nreturn an array of values corresponding to the ith entry in this record group\n\nget_record(r::RecordGruop, s::Symbol)\n\nreturn an array of recorded values with respect to the s, see RecordGroup.\n\nget_record(r::RecordGroup, s1::Symbol, s2::Symbol,...)\n\nreturn an array of tuples, where each tuple is a recorded set corresponding to the symbols s1, s2,... per iteration / record call.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.get_record_action","page":"Plans","title":"Manopt.get_record_action","text":"get_record_action(o::Options, s::Symbol)\n\nreturn the action contained in the (first) RecordOptions decorator within the Options o.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.get_record_options-Tuple{Options}","page":"Plans","title":"Manopt.get_record_options","text":"get_record_options(o::Options)\n\nreturn the RecordOptions among the decorators from the Options o\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.has_record-Tuple{RecordOptions}","page":"Plans","title":"Manopt.has_record","text":"has_record(o::Options)\n\ncheck whether the Optionso are decorated with RecordOptions\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"getindex(ro::RecordOptions, s::Symbol)\ngetindex(::RecordGroup,::Any...)","category":"page"},{"location":"plans/index.html#Base.getindex-Tuple{RecordOptions, Symbol}","page":"Plans","title":"Base.getindex","text":"get_index(ro::RecordOptions, s::Symbol)\nro[s]\n\nGet the recorded values for reording type s, see get_record for details.\n\nget_index(ro::RecordOptions, s::Symbol, i...)\nro[s, i...]\n\nAcces the recording type of type s and call its RecordAction with [i...].\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Base.getindex-Tuple{RecordGroup, Vararg{Any, N} where N}","page":"Plans","title":"Base.getindex","text":"getindex(r::RecordGroup, s::Symbol)\nr[s]\ngetindex(r::RecordGroup, sT::NTuple{N,Symbol})\nr[sT]\ngetindex(r::RecordGroup, i)\nr[i]\n\nreturn an array of recorded values with respect to the s, the symbols from the tuple sT or the index i. See get_record for details.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"see RecordSolver for details on the decorated solver.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Further specific RecordActions can be found at the specific Options.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"there's one internal helper that might be useful for you own actions, namely","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"record_or_reset!","category":"page"},{"location":"plans/index.html#Manopt.record_or_reset!","page":"Plans","title":"Manopt.record_or_reset!","text":"record_or_reset!(r,v,i)\n\neither record (i>0 and not Inf) the value v within the RecordAction r or reset (i<0) the internal storage, where v has to match the internal value type of the corresponding Recordaction.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Stepsize-1","page":"Plans","title":"Stepsize and Linesearch","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"The step size determination is implemented as a Functor based on","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Stepsize","category":"page"},{"location":"plans/index.html#Manopt.Stepsize","page":"Plans","title":"Manopt.Stepsize","text":"Stepsize\n\nAn abstract type for the functors representing step sizes, i.e. they are callable structures. The naming scheme is TypeOfStepSize, e.g. ConstantStepsize.\n\nEvery Stepsize has to provide a constructor and its function has to have the interface (p,o,i) where a Problem as well as Options and the current number of iterations are the arguments and returns a number, namely the stepsize to use.\n\nSee also\n\nLinesearch\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"in general there are","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Modules = [Manopt]\nPages = [\"plans/stepsize.jl\"]\nOrder = [:type,:function]","category":"page"},{"location":"plans/index.html#Manopt.ArmijoLinesearch","page":"Plans","title":"Manopt.ArmijoLinesearch","text":"ArmijoLineseach <: Linesearch\n\nA functor representing Armijo line seach including the last runs state, i.e. a last step size.\n\nFields\n\ninitialStepsize – (1.0) and initial step size\nretraction_method – (ExponentialRetraction()) the rectraction to use, defaults to the exponential map\ncontractionFactor – (0.95) exponent for line search reduction\nsufficientDecrease – (0.1) gain within Armijo's rule\nlastStepSize – (initialstepsize) the last step size we start the search with\nlinesearch_stopsize - (0.0) a safeguard when to stop the line search   before the step is numerically zero. This should be combined with StopWhenStepSizeLess\n\nConstructor\n\nArmijoLineSearch()\n\nwith the Fields above in their order as optional arguments.\n\nThis method returns the functor to perform Armijo line search, where two inter faces are available:\n\nbased on a tuple (p,o,i) of a GradientProblem p, Options o and a current iterate i.\nwith (M, x, F, gradFx[,η=-gradFx]) -> s where Manifold M, a current point x a function F, that maps from the manifold to the reals, its gradient (a tangent vector) gradFx=operatornamegradF(x) at  x and an optional search direction tangent vector η=-gradFx are the arguments.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.ConstantStepsize","page":"Plans","title":"Manopt.ConstantStepsize","text":"ConstantStepsize <: Stepsize\n\nA functor that always returns a fixed step size.\n\nFields\n\nlength – constant value for the step size.\n\nConstructor\n\nConstantStepSize(s)\n\ninitialize the stepsize to a constant s\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.DecreasingStepsize","page":"Plans","title":"Manopt.DecreasingStepsize","text":"DecreasingStepsize()\n\nA functor that represents several decreasing step sizes\n\nFields\n\nlength – (1) the initial step size l.\nfactor – (1) a value f to multiply the initial step size with every iteration\nsubtrahend – (0) a value a that is subtracted every iteration\nexponent – (1) a value e the current iteration numbers eth exponential is taken of\nshift – (0) shift the denominator iterator i by s`.\n\nIn total the complete formulae reads for the ith iterate as\n\ns_i = frac(l - i a)f^i(i-s)^e\n\nand hence the default simplifies to just s_i = fracli\n\nConstructor\n\nDecreasingStepsize(l=1,f=1,a=0,e=1,s=0)\n\nAlternatively one can also use the following keyword Alternatively\n\nDecreasingStepSize(;length=1.0, multiplier=1.0, subtrahend=0.0, exponent=1.0, shift=0)\n\ninitialiszes all fields above, where none of them is mandatory.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.Linesearch","page":"Plans","title":"Manopt.Linesearch","text":"Linesearch <: Stepsize\n\nAn abstract functor to represent line search type step size deteminations, see Stepsize for details. One example is the ArmijoLinesearch functor.\n\nCompared to simple step sizes, the linesearch functors provide an interface of the form (p,o,i,η) -> s with an additional (but optional) fourth parameter to provide a search direction; this should default to something reasonable, e.g. the negative gradient.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.NonmonotoneLinesearch","page":"Plans","title":"Manopt.NonmonotoneLinesearch","text":"NonmonotoneLinesearch <: Linesearch\n\nA functor representing a nonmonotone line search using the Barzilai-Borwein step size[Iannazzo2018]. Together with a gradient descent algorithm this line search represents the Riemannian Barzilai-Borwein with nonmonotone line-search (RBBNMLS) algorithm. We shifted the order of the algorithm steps from the paper by Iannazzo and Porcelli so that in each iteration we first find\n\ny_k = operatornamegradF(x_k) - operatornameT_x_k-1  x_k(operatornamegradF(x_k-1))\n\nand\n\ns_k = - α_k-1 * operatornameT_x_k-1  x_k(operatornamegradF(x_k-1))\n\nwhere α_k-1 is the step size computed in the last iteration and operatornameT is a vector transport. We then find the Barzilai–Borwein step size\n\nα_k^textBB = begincases\nmin(α_textmax max(α_textmin τ_k))   textif  s_k y_k_x_k  0\nα_textmax  textelse\nendcases\n\nwhere\n\nτ_k = fracs_k s_k_x_ks_k y_k_x_k\n\nif the direct strategy is chosen,\n\nτ_k = fracs_k y_k_x_ky_k y_k_x_k\n\nin case of the inverse strategy and an alternation between the two in case of the alternating strategy. Then we find the smallest h = 0 1 2  such that\n\nF(operatornameretr_x_k(- σ^h α_k^textBB operatornamegradF(x_k)))\nleq\nmax_1  j  min(k+1m) F(x_k+1-j) - γ σ^h α_k^textBB operatornamegradF(x_k) operatornamegradF(x_k)_x_k\n\nwhere σ is a step length reduction factor  (01), m is the number of iterations after which the function value has to be lower than the current one and γ is the sufficient decrease parameter (01). We can then find the new stepsize by\n\nα_k = σ^h α_k^textBB\n\n[Iannazzo2018]: B. Iannazzo, M. Porcelli, The Riemannian Barzilai–Borwein Method with Nonmonotone Line Search and the Matrix Geometric Mean Computation, In: IMA Journal of Numerical Analysis. Volume 38, Issue 1, January 2018, Pages 495–517, doi 10.1093/imanum/drx015\n\nFields\n\ninitial_stepsize – (1.0) the step size we start the search with\nlinesearch_stopsize - (0.0) a safeguard when to stop the line search   before the step is numerically zero. This should be combined with StopWhenStepSizeLess\nmemory_size – (10) number of iterations after which the cost value needs to be lower than the current one\nmin_stepsize – (1e-3) lower bound for the Barzilai-Borwein step size greater than zero\nmax_stepsize – (1e3) upper bound for the Barzilai-Borwein step size greater than min_stepsize\nretraction_method – (ExponentialRetraction()) the rectraction to use\nstrategy – (direct) defines if the new step size is computed using the direct, indirect or alternating strategy\nstorage – (x, gradient) a StoreOptionsAction to store old_x and old_gradient, the x-value and corresponding gradient of the previous iteration\nstepsize_reduction – (0.5) step size reduction factor contained in the interval (0,1)\nsufficient_decrease – (1e-4) sufficient decrease parameter contained in the interval (0,1)\nvector_transport_method – (ParallelTransport()) the vector transport method to use\n\nConstructor\n\nNonmonotoneLinesearch()\n\nwith the Fields above in their order as optional arguments.\n\nThis method returns the functor to perform nonmonotone line search.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.WolfePowellBinaryLinesearch","page":"Plans","title":"Manopt.WolfePowellBinaryLinesearch","text":"WolfePowellBinaryLinesearch <: Linesearch\n\nA Linesearch method that determines a step size t fulfilling the Wolfe conditions\n\nbased on a binary chop. Let η be a search direction and c_1c_20 be two constants. Then with\n\nA(t) = f(x_+)  c_1 t operatornamegradf(x) η_x\nquadtextandquad\nW(t) = operatornamegradf(x_+) textV_x_+gets xη_x_+  c_2 η operatornamegradf(x)_x\n\nwhere x_+ = operatornameretr_x(tη) is the current trial point, and textV is a vector transport, we perform the following Algorithm similar to Algorithm 7 from [Huang2014]\n\nset α=0, β= and t=1.\nWhile either A(t) does not hold or W(t) does not hold do steps 3-5.\nIf A(t) fails, set β=t.\nIf A(t) holds but W(t) fails, set α=t.\nIf β set t=fracα+β2, otherwise set t=2α.\n\nConstructor\n\nWolfePowellBinaryLinesearch(\n    retr::AbstractRetractionMethod=ExponentialRetraction(),\n    vtr::AbstractVectorTransportMethod=ParallelTransport(),\n    c_1::Float64=10^(-4),\n    c_2::Float64=0.999\n)\n\n[Huang2014]: Huang, W.: Optimization algorithms on Riemannian manifolds with applications, Dissertation, Flordia State University, 2014. pdf\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.WolfePowellLineseach","page":"Plans","title":"Manopt.WolfePowellLineseach","text":"WolfePowellLineseach <: Linesearch\n\nDo a backtracking linesearch to find a step size α that fulfils the Wolfe conditions along a search direction η starting from x, i.e.\n\nfbigl( operatornameretr_x(αη) bigr)  f(x_k) + c_1 α_k operatornamegradf(x) η_x\nquadtextandquad\nfracmathrmdmathrmdt fbigr(operatornameretr_x(tη)bigr)\nBigvert_t=α\n c_2 fracmathrmdmathrmdt fbigl(operatornameretr_x(tη)bigr)Bigvert_t=0\n\nConstructor\n\nWolfePowellLinesearch(\n    retr::AbstractRetractionMethod=ExponentialRetraction(),\n    vtr::AbstractVectorTransportMethod=ParallelTransport(),\n    c_1::Float64=10^(-4),\n    c_2::Float64=0.999\n)\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_stepsize-Tuple{Problem, Options, Vararg{Any, N} where N}","page":"Plans","title":"Manopt.get_stepsize","text":"get_stepsize(p::Problem, o::Options, vars...)\n\nreturn the stepsize stored within Options o when solving Problem p. This method also works for decorated options and the Stepsize function within the options, by default stored in o.stepsize.\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Manopt.linesearch_backtrack-Union{Tuple{T}, Tuple{TF}, Tuple{AbstractManifold, TF, Any, T, Any, Any, Any}, Tuple{AbstractManifold, TF, Any, T, Any, Any, Any, AbstractRetractionMethod}, Tuple{AbstractManifold, TF, Any, T, Any, Any, Any, AbstractRetractionMethod, T}, Tuple{AbstractManifold, TF, Any, T, Any, Any, Any, AbstractRetractionMethod, T, Any}} where {TF, T}","page":"Plans","title":"Manopt.linesearch_backtrack","text":"linesearch_backtrack(M, F, x, gradFx, s, decrease, contract, retr, η = -gradFx, f0 = F(x); stop_step=0.)\n\nperform a linesearch for\n\na manifold M\na cost function F,\nan iterate x\nthe gradient operatornamegradF(x)\nan initial stepsize s usually called γ\na sufficient decrease\na contraction factor σ\na retraction, which defaults to the ExponentialRetraction()\na search direction η = -operatornamegradF(x)\nan offset, f_0 = F(x)\na keyword stop_step as a minimal step size when to stop\n\n\n\n\n\n","category":"method"},{"location":"plans/index.html#Problems-1","page":"Plans","title":"Problems","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"A problem usually contains its cost function and provides and implementation to access the cost","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"Problem\nget_cost","category":"page"},{"location":"plans/index.html#Manopt.Problem","page":"Plans","title":"Manopt.Problem","text":"Problem{T}\n\nDescribe the problem that should be optimized by stating all properties, that do not change during an optimization or that are dependent of a certain solver.\n\nThe parameter T can be used to distinguish problems with different representations or implementations. The default parameter AllocatingEvaluation, which might be slower but easier to use. The usually faster parameter value is MutatingEvaluation\n\nSee Options for the changing and solver dependent properties.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_cost","page":"Plans","title":"Manopt.get_cost","text":"get_cost(p, x)\n\nevaluate the cost function F stored within a Problem at the point x.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"A problem can be of different type, more specifically, whether its containing functions, for example to compute the gradient work with allocation or without. To be precise, an allocation function X = gradF(x) allocates memory for its result X, while gradF!(X,x) does not.","category":"page"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"AbstractEvaluationType\nAllocatingEvaluation\nMutatingEvaluation","category":"page"},{"location":"plans/index.html#Manopt.AbstractEvaluationType","page":"Plans","title":"Manopt.AbstractEvaluationType","text":"AbstractEvaluationType\n\nAn abstract type to specify the kind of evaluation a Problem supports.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.AllocatingEvaluation","page":"Plans","title":"Manopt.AllocatingEvaluation","text":"AllocatingEvaluation <: AbstractEvaluationType\n\nA parameter for a Problem indicating that the problem uses functions that allocate memory for their result, i.e. they work out of place.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.MutatingEvaluation","page":"Plans","title":"Manopt.MutatingEvaluation","text":"MutatingEvaluation\n\nA parameter for a Problem indicating that the problem uses functions that do not allocate memory but work on their input, i.e. in place.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Cost-based-problem-1","page":"Plans","title":"Cost based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"CostProblem","category":"page"},{"location":"plans/index.html#Manopt.CostProblem","page":"Plans","title":"Manopt.CostProblem","text":"CostProblem{T, Manifold, TCost} <: Problem{T}\n\nspeficy a problem for solvers just based on cost functions, i.e. gradient free ones.\n\nFields\n\nM            – a manifold mathcal M\ncost – a function F mathcal M  ℝ to minimize\n\nConstructors\n\nCostProblem(M, cost; evaluation=AllocatingEvaluation())\n\nGenerate a problem. While this Problem does not have any allocating functions, the type T can be set for consistency reasons with other problems.\n\nSee also\n\nNelderMead\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Gradient-based-problem-1","page":"Plans","title":"Gradient based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"AbstractGradientProblem\nGradientProblem\nStochasticGradientProblem\nget_gradient\nget_gradients","category":"page"},{"location":"plans/index.html#Manopt.AbstractGradientProblem","page":"Plans","title":"Manopt.AbstractGradientProblem","text":"AbstractGradientProblem{T} <: Problem{T}\n\nAn abstract type for all functions that provide a (full) gradient, where T is a AbstractEvaluationType for the gradient function.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.GradientProblem","page":"Plans","title":"Manopt.GradientProblem","text":"GradientProblem{T} <: AbstractGradientProblem{T}\n\nspecify a problem for gradient based algorithms.\n\nFields\n\nM        – a manifold mathcal M\ncost     – a function F mathcal M  ℝ to minimize\ngradient!! – the gradient operatornamegradFmathcal M  mathcal Tmathcal M of the cost function F.\n\nDepending on the AbstractEvaluationType T the gradient has to be provided\n\nas a function x -> X that allocates memory for X itself for an AllocatingEvaluation\nas a function (X,x) -> X that work in place of X for an MutatingEvaluation\n\nConstructors\n\nGradientProblem(M, cost, gradient; evaluation=AllocatingEvaluation())\n\nSee also\n\ngradient_descent, GradientDescentOptions\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.StochasticGradientProblem","page":"Plans","title":"Manopt.StochasticGradientProblem","text":"StochasticGradientProblem <: Problem\n\nA stochastic gradient problem consists of\n\na Manifold M\na(n optional) cost function ``f(x) = \\displaystyle\\sum{i=1}^n fi(x)\nan array of gradients, i.e. a function that returns and array or an array of functions\n\noperatornamegradf_i_i=1^n, where both variants can be given in the allocating variant and the array of function may also be provided as mutating functions (X,x) -> X.\n\nConstructors\n\nStochasticGradientProblem(M::AbstractManifold, gradF::Function;\n    cost=Missing(), evaluation=AllocatingEvaluation()\n)\nStochasticGradientProblem(M::AbstractManifold, gradF::AbstractVector{<:Function};\n    cost=Missing(), evaluation=AllocatingEvaluation()\n)\n\nCreate a Stochastic gradient problem with an optional cost and the gradient either as one function (returning an array) or a vector of functions.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_gradient","page":"Plans","title":"Manopt.get_gradient","text":"get_gradient(p::AbstractGradientProblem{T},x)\nget_gradient!(p::AbstractGradientProblem{T}, X, x)\n\nevaluate the gradient of a AbstractGradientProblem{T}p at the point x.\n\nThe evaluation is done in place of X for the !-variant. The T=AllocatingEvaluation problem might still allocate memory within. When the non-mutating variant is called with a T=MutatingEvaluation memory for the result is allocated.\n\n\n\n\n\nget_gradient(p::StochasticGradientProblem, k, x)\nget_gradient!(p::StochasticGradientProblem, Y, k, x)\n\nEvaluate one of the summands gradients operatornamegradf_k, k1n, at x (in place of Y).\n\nNote that for the MutatingEvaluation based problem and a single function for the stochastic gradient mutating variant is not available, since it would require too many allocations.\n\n\n\n\n\nget_gradient(P::AlternatingGradientProblem, x)\nget_gradient!(P::AlternatingGradientProblem, Y, x)\n\nEvaluate all summands gradients at a point x on the ProductManifold M (in place of Y)\n\n\n\n\n\nget_gradient(p::AlternatingGradientProblem, k, x)\nget_gradient!(p::AlternatingGradientProblem, Y, k, x)\n\nEvaluate one of the component gradients operatornamegradf_k, k1n, at x (in place of Y).\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.get_gradients","page":"Plans","title":"Manopt.get_gradients","text":"get_gradients(P::StochasticGradientProblem, x)\nget_gradients!(P::StochasticGradientProblem, Y, x)\n\nEvaluate all summands gradients operatornamegradf_i_i=1^n at x (in place of Y).\n\nNote that for the MutatingEvaluation based problem and a single function for the stochastic gradient, the allocating variant is not available.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Subgradient-based-problem-1","page":"Plans","title":"Subgradient based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"SubGradientProblem\nget_subgradient","category":"page"},{"location":"plans/index.html#Manopt.SubGradientProblem","page":"Plans","title":"Manopt.SubGradientProblem","text":"SubGradientProblem <: Problem\n\nA structure to store information about a subgradient based optimization problem\n\nFields\n\nmanifold – a Manifold\ncost – the function F to be minimized\nsubgradient – a function returning a subgradient partial F of F\n\nConstructor\n\nSubGradientProblem(M, f, ∂f)\n\nGenerate the [Problem] for a subgradient problem, i.e. a function f on the manifold M and a function ∂f that returns an element from the subdifferential at a point.\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_subgradient","page":"Plans","title":"Manopt.get_subgradient","text":"get_subgradient(p, q)\nget_subgradient!(p, X, q)\n\nEvaluate the (sub)gradient of a SubGradientProblemp at the point q.\n\nThe evaluation is done in place of X for the !-variant. The T=AllocatingEvaluation problem might still allocate memory within. When the non-mutating variant is called with a T=MutatingEvaluation memory for the result is allocated.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#ProximalProblem-1","page":"Plans","title":"Proximal Map(s) based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"ProximalProblem\nget_proximal_map","category":"page"},{"location":"plans/index.html#Manopt.ProximalProblem","page":"Plans","title":"Manopt.ProximalProblem","text":"ProximalProblem <: Problem\n\nspecify a problem for solvers based on the evaluation of proximal map(s).\n\nFields\n\nM - a Riemannian manifold\ncost - a function Fmathcal Mℝ to minimize\nproxes - proximal maps operatornameprox_λvarphimathcal Mmathcal M as functions (λ,x) -> y, i.e. the prox parameter λ also belongs to the signature of the proximal map.\nnumber_of_proxes - (length(proxes)) number of proximal Maps, e.g. if one of the maps is a combined one such that the proximal Maps functions return more than one entry per function\n\nSee also\n\ncyclic_proximal_point, get_cost, get_proximal_map\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_proximal_map","page":"Plans","title":"Manopt.get_proximal_map","text":"get_proximal_map(p,λ,x,i)\n\nevaluate the ith proximal map of ProximalProblem p at the point x of p.M with parameter λ0.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#HessianProblem-1","page":"Plans","title":"Hessian based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"HessianProblem\nget_hessian\nget_preconditioner","category":"page"},{"location":"plans/index.html#Manopt.HessianProblem","page":"Plans","title":"Manopt.HessianProblem","text":"HessianProblem <: Problem\n\nspecify a problem for hessian based algorithms.\n\nFields\n\nM            : a manifold mathcal M\ncost : a function Fmathcal Mℝ to minimize\ngradient     : the gradient operatornamegradFmathcal M  mathcal Tmathcal M of the cost function F\nhessian      : the hessian operatornameHessF(x) mathcal T_x mathcal M  mathcal T_x mathcal M of the cost function F\nprecon       : the symmetric, positive definite   preconditioner (approximation of the inverse of the Hessian of F)\n\nSee also\n\ntruncated_conjugate_gradient_descent, trust_regions\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_hessian","page":"Plans","title":"Manopt.get_hessian","text":"get_hessian(p::HessianProblem{T}, q, X)\nget_hessian!(p::HessianProblem{T}, Y, q, X)\n\nevaluate the Hessian of a HessianProblem p at the point q applied to a tangent vector X, i.e. operatornameHessf(q)X.\n\nThe evaluation is done in place of Y for the !-variant. The T=AllocatingEvaluation problem might still allocate memory within. When the non-mutating variant is called with a T=MutatingEvaluation memory for the result is allocated.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.get_preconditioner","page":"Plans","title":"Manopt.get_preconditioner","text":"get_preconditioner(p,x,ξ)\n\nevaluate the symmetric, positive definite preconditioner (approximation of the inverse of the Hessian of the cost function F) of a HessianProblem p at the point xapplied to a tangent vector ξ.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#PrimalDualProblem-1","page":"Plans","title":"Primal dual based problem","text":"","category":"section"},{"location":"plans/index.html#","page":"Plans","title":"Plans","text":"PrimalDualProblem\nget_primal_prox\nget_dual_prox\nforward_operator\nlinearized_forward_operator\nadjoint_linearized_operator","category":"page"},{"location":"plans/index.html#Manopt.PrimalDualProblem","page":"Plans","title":"Manopt.PrimalDualProblem","text":"PrimalDualProblem {T, mT <: Manifold, nT <: Manifold} <: PrimalDualProblem} <: Problem{T}\n\nDescribes a Problem for the linearized or exact Chambolle-Pock algorithm.[BergmannHerzogSilvaLouzeiroTenbrinckVidalNunez2020][ChambollePock2011]\n\nFields\n\nAll fields with !! can either be mutating or nonmutating functions, which should be set depenting on the parameter T <: AbstractEvaluationType.\n\nM, N – two manifolds mathcal M, mathcal N\ncost F + G(Λ()) to evaluate interims cost function values\nlinearized_forward_operator!! linearized operator for the forward operation in the algorithm DΛ\nlinearized_adjoint_operator!! The adjoint differential (DΛ)^*  mathcal N  Tmathcal M\nprox_F!! the proximal map belonging to f\nprox_G_dual!! the proximal map belonging to g_n^*\nΛ!! – (fordward_operator) the  forward operator (if given) Λ mathcal M  mathcal N\n\nEither DΛ (for the linearized) or Λ are required usually.\n\nConstructor\n\nLinearizedPrimalDualProblem(M, N, cost, prox_F, prox_G_dual, adjoint_linearized_operator;\n    linearized_forward_operator::Union{Function,Missing}=missing,\n    Λ::Union{Function,Missing}=missing,\n    evaluation::AbstractEvaluationType=AllocatingEvaluation()\n)\n\nThe last optional argument can be used to provide the 4 or 5 functions as allocating or mutating (in place computation) ones. Note that the first argument is always the manifold under consideration, the mutated one is the second.\n\n[BergmannHerzogSilvaLouzeiroTenbrinckVidalNunez2020]: R. Bergmann, R. Herzog, M. Silva Louzeiro, D. Tenbrinck, J. Vidal-Núñez: Fenchel Duality Theory and a Primal-Dual Algorithm on Riemannian Manifolds, Foundations of Computational Mathematics, 2021. doi: 10.1007/s10208-020-09486-5 arXiv: 1908.02022\n\n[ChambollePock2011]: A. Chambolle, T. Pock: A first-order primal-dual algorithm for convex problems with applications to imaging, Journal of Mathematical Imaging and Vision 40(1), 120–145, 2011. doi: 10.1007/s10851-010-0251-1\n\n\n\n\n\n","category":"type"},{"location":"plans/index.html#Manopt.get_primal_prox","page":"Plans","title":"Manopt.get_primal_prox","text":"y = get_primal_prox(p::PrimalDualProblem, σ, x)\nget_primal_prox!(p::PrimalDualProblem, y, σ, x)\n\nEvaluate the proximal map of F stored within PrimalDualProblem\n\noperatornameprox_σF(x)\n\nwhich can also be computed in place of y.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.get_dual_prox","page":"Plans","title":"Manopt.get_dual_prox","text":"y = get_dual_prox(p::PrimalDualProblem, n, τ, ξ)\nget_dual_prox!(p::PrimalDualProblem, y, n, τ, ξ)\n\nEvaluate the proximal map of G_n^* stored within PrimalDualProblem\n\noperatornameprox_τG_n^*(ξ)\n\nwhich can also be computed in place of y.\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.forward_operator","page":"Plans","title":"Manopt.forward_operator","text":"y = forward_operator(p::PrimalDualProblem, x)\nforward_operator!(p::PrimalDualProblem, y, x)\n\nEvaluate the forward operator of Λ(x) stored within the PrimalDualProblem (in place of y).\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.linearized_forward_operator","page":"Plans","title":"Manopt.linearized_forward_operator","text":"Y = linearized_forward_operator(p::PrimalDualProblem, m X, n)\nlinearized_forward_operator!(p::PrimalDualProblem, Y, m, X, n)\n\nEvaluate the linearized operator (differential) DΛ(m)X stored within the PrimalDualProblem (in place of Y), where n = Λ(m).\n\n\n\n\n\n","category":"function"},{"location":"plans/index.html#Manopt.adjoint_linearized_operator","page":"Plans","title":"Manopt.adjoint_linearized_operator","text":"X = adjoint_linearized_operator(p::PrimalDualProblem, m, n, Y)\nadjoint_linearized_operator(p::PrimalDualProblem, X, m, n, Y)\n\nEvaluate the adjoint of the linearized forward operator of (DΛ(m))^*Y stored within the PrimalDualProblem (in place of X). Since YT_nmathcal N, both m and n=Λ(m) are necessary arguments, mainly because the forward operator Λ might be missing in p.\n\n\n\n\n\n","category":"function"},{"location":"functions/Jacobi_fields.html#JacobiFieldFunctions-1","page":"Jacobi Fields","title":"Jacobi Fields","text":"","category":"section"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"A smooth tangent vector field J 01  Tmathcal M along a geodesic g(xy) is called Jacobi field if it fulfills the ODE","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"displaystyle 0 = fracDdtJ + R(Jdot g)dot g","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"where R is the Riemannian curvature tensor. Such Jacobi fields can be used to derive closed forms for the exponential map, the logarithmic map and the geodesic, all of them with respect to both arguments: Let Fmathcal N  mathcal M be given (for the exp_x   we have mathcal N = T_xmathcal M, otherwise mathcal N=mathcal M) and denote by ξ_1ξ_d an orthonormal frame along g(xy) that diagonalizes the curvature tensor with corresponding eigenvalues κ_1κ_d. Note that on symmetric manifolds such a frame always exists.","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Then DF(x)η = sum_k=1^d langle ηξ_k(0)rangle_xβ(κ_k)ξ_k(T) holds, where T also depends on the function F as the weights β. The values stem from solving the corresponding system of (decoupled) ODEs.","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Note that in different references some factors might be a little different, for example when using unit speed geodesics.","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"The following weights functions are available","category":"page"},{"location":"functions/Jacobi_fields.html#","page":"Jacobi Fields","title":"Jacobi Fields","text":"Modules = [Manopt]\nPages   = [\"Jacobi_fields.jl\"]","category":"page"},{"location":"functions/Jacobi_fields.html#Manopt.adjoint_Jacobi_field","page":"Jacobi Fields","title":"Manopt.adjoint_Jacobi_field","text":"Y = adjoint_Jacobi_field(M, p, q, t, X, β)\nadjoint_Jacobi_field!(M, Y, p, q, t, X, β)\n\nCompute the AdjointJacobiField J along the geodesic γ_pq on the manifold mathcal M with initial conditions (depending on the application) X  T_γ_pq(t)mathcal M and weights β. The result is a vector Y  T_pmathcal M. The main difference to jacobi_field is the, that the input X and the output Y switched tangent spaces. The computation can be done in place of Y.\n\nFor details see jacobi_field\n\n\n\n\n\n","category":"function"},{"location":"functions/Jacobi_fields.html#Manopt.jacobi_field","page":"Jacobi Fields","title":"Manopt.jacobi_field","text":"Y = jacobi_field(M, p, q, t, X, β)\njacobi_field!(M, Y, p, q, t, X, β)\n\ncompute the Jacobi field J along the geodesic γ_pq on the manifold mathcal M with initial conditions (depending on the application) X  T_pmathcal M and weights β. The result is a tangent vector Y from T_γ_pq(t)mathcal M. The computation can be done in place of Y.\n\nSee also\n\nadjoint_Jacobi_field\n\n\n\n\n\n","category":"function"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_exp_argument-Tuple{Any, Number, Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_exp_argument","text":"βdifferential_exp_argument(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_X exp_p XY. They are\n\nβ(κ) = begincases\nfracsinh(dsqrt-κ)dsqrt-κtext if κ  0\n1  text if  κ = 0\nfracsin(dsqrtκ)dsqrtκtext if κ  0\nendcases\n\nSee also\n\ndifferential_exp_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_exp_basepoint-Tuple{Any, Number, Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_exp_basepoint","text":"βdifferential_exp_basepoint(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_p exp_p X Y. They are\n\nβ(κ) = begincases\ncosh(sqrt-κ)text if κ  0\n1  text if  κ = 0\ncos(sqrtκ) text if κ  0\nendcases\n\nSee also\n\ndifferential_exp_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_geodesic_startpoint-Tuple{Any, Any, Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_geodesic_startpoint","text":"βdifferential_geodesic_startpoint(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_x g(tpq)X. They are\n\nβ(κ) = begincases\nfracsinh(d(1-t)sqrt-κ)sinh(dsqrt-κ)\ntext if κ  0\n1-t  text if  κ = 0\nfracsin((1-t)dsqrtκ)sinh(dsqrtκ)\ntext if κ  0\nendcases\n\nDue to a symmetry argument, these are also used to compute D_q g(t pq)η\n\nSee also\n\ndifferential_geodesic_endpoint, differential_geodesic_startpoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_log_argument-Tuple{Any, Number, Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_log_argument","text":"βdifferential_log_argument(κ,t,d)\n\nweights for the JacobiField corresponding to the differential of the logarithmic map with respect to its argument D_q log_p qX. They are\n\nβ(κ) = begincases\nfrac dsqrt-κ sinh(dsqrt-κ)text if κ  0\n1  text if  κ = 0\nfrac dsqrtκ sin(dsqrtκ)text if κ  0\nendcases\n\nSee also\n\ndifferential_log_basepoint, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/Jacobi_fields.html#Manopt.βdifferential_log_basepoint-Tuple{Any, Number, Any}","page":"Jacobi Fields","title":"Manopt.βdifferential_log_basepoint","text":"βdifferential_log_basepoint(κ,t,d)\n\nweights for the jacobi_field corresponding to the differential of the geodesic with respect to its start point D_p log_p qX. They are\n\nβ(κ) = begincases\n-sqrt-κdfraccosh(dsqrt-κ)sinh(dsqrt-κ)text if κ  0\n-1  text if  κ = 0\n-sqrtκdfraccos(dsqrtκ)sin(dsqrtκ)text if κ  0\nendcases\n\nSee also\n\ndifferential_log_argument, differential_log_argument, jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Exports-1","page":"Exports","title":"Exports","text":"","category":"section"},{"location":"helpers/exports.html#","page":"Exports","title":"Exports","text":"Exports aim to provide a consistent generation of images of your results. For example if you record the trace your algorithm walks on the Sphere, you yan easily export this trace to a rendered image using asymptote_export_S2_signals and render the result with Asymptote. Despite these, you can always record values during your iterations, and export these, for example to csv.","category":"page"},{"location":"helpers/exports.html#Asymptote-1","page":"Exports","title":"Asymptote","text":"","category":"section"},{"location":"helpers/exports.html#","page":"Exports","title":"Exports","text":"The following functions provide exports both in graphics and/or raw data using Asymptote.","category":"page"},{"location":"helpers/exports.html#","page":"Exports","title":"Exports","text":"Modules = [Manopt]\nPages   = [\"Asymptote.jl\"]","category":"page"},{"location":"helpers/exports.html#Manopt.asymptote_export_S2_data-Tuple{String}","page":"Exports","title":"Manopt.asymptote_export_S2_data","text":"asymptote_export_S2_data(filename)\n\nExport given data as an array of points on the sphere, i.e. one-, two- or three-dimensional data with points on the Sphere mathbb S^2.\n\nInput\n\nfilename – a file to store the Asymptote code in.\n\nOptional Arguments (Data)\n\ndata – a point representing the 1-,2-, or 3-D array of points\nelevation_color_scheme - A ColorScheme for elevation\nscale_axes - ((1/3,1/3,1/3)) move spheres closer to each other by a factor per direction\n\nOptional Arguments (Asymptote)\n\narrow_head_size - (1.8) size of the arrowheads of the vectors (in mm)\ncamera_position - position of the camrea (default: centered above xy-plane) szene\ntarget - position the camera points at (default: center of xy-plane within data).\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Manopt.asymptote_export_S2_signals-Tuple{String}","page":"Exports","title":"Manopt.asymptote_export_S2_signals","text":"asymptote_export_S2_signals(filename; points, curves, tangent_vectors, colors, options...)\n\nExport given points, curves, and tangent_vectors on the sphere mathbb S^2 to Asymptote.\n\nInput\n\nfilename – a file to store the Asymptote code in.\n\nOptional Arguments (Data)\n\ncolors - dictionary of color arrays (indexed by symbols :points, :curves and :tvector) where each entry has to provide as least as many colors as the length of the corresponding sets.\ncurves – an Array of Arrays of points on the sphere, where each inner array is interpreted as a curve and is accompanied by an entry within colors\npoints – an Array of Arrays of points on the sphere where each inner array is itnerpreted as a set of points and is accompanied by an entry within colors\ntangent_vectors – an Array of Arrays of tuples, where the first is a points, the second a tangent vector and each set of vectors is accompanied by an entry from within colors\n\nOptional Arguments (Asymptote)\n\narrow_head_size - (6.0) size of the arrowheads of the tangent vectors\narrow_head_sizes – overrides the previous value to specify a value per tVector set.\ncamera_position - ((1., 1., 0.)) position of the camera in the Asymptote szene\nline_width – (1.0) size of the lines used to draw the curves.\nline_widths – overrides the previous value to specify a value per curve and tVector set.\ndot_size – (1.0) size of the dots used to draw the points.\ndot_sizes – overrides the previous value to specify a value per point set.\nsphere_color – (RGBA{Float64}(0.85, 0.85, 0.85, 0.6)) color of the sphere the data is drawn on\nsphere_line_color –  (RGBA{Float64}(0.75, 0.75, 0.75, 0.6)) color of the lines on the sphere\nsphere_line_width – (0.5) line width of the lines on the sphere\ntarget – ((0.,0.,0.)) position the camera points at\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Manopt.asymptote_export_SPD-Tuple{String}","page":"Exports","title":"Manopt.asymptote_export_SPD","text":"asymptote_export_SPD(filename)\n\nexport given data as a point on a Power{SPDPoint} manifold, i.e. one-, two- or three-dimensional data with points on the manifold of symmetric positive definite matrices.\n\nInput\n\nfilename – a file to store the Asymptote code in.\n\nOptional Arguments (Data)\n\ndata – a point representing the 1-,2-, or 3-D array of SPDPoints\ncolor_scheme - A ColorScheme for Geometric Anisotropy Index\nscale_axes - ((1/3,1/3,1/3)) move symmetric positive definite matrices closer to each other by a factor per direction compared to the distance esimated by the maximal eigenvalue of all involved SPD points\n\nOptional Arguments (Asymptote)\n\ncamera_position - position of the camrea (default: centered above xy-plane) szene.\ntarget - position the camera points at (default: center of xy-plane within data).\n\nBoth values camera_position and target are scaled by scaledAxes*EW, where EW is the maximal eigenvalue in the data.\n\n\n\n\n\n","category":"method"},{"location":"helpers/exports.html#Manopt.render_asymptote-Tuple{Any}","page":"Exports","title":"Manopt.render_asymptote","text":"render_asymptote(filename; render=4, format=\"png\", ...)\n\nrender an exported asymptote file specified in the filename, which can also be given as a relative or full path\n\nInput\n\nfilename – filename of the exported asy and rendered image\n\nKeyword Arguments\n\nthe default values are given in brackets\n\nrender – (4) render level of asymptote, i.e. its -render option\nformat – (\"png\") final rendered format, i.e. asymptote's -f option\nexport_file - (the filename with format as ending) specify the export filename\n\n\n\n\n\n","category":"method"},{"location":"solvers/gradient_descent.html#GradientDescentSolver-1","page":"Gradient Descent","title":"Gradient Descent","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"  gradient_descent\n  gradient_descent!","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.gradient_descent","page":"Gradient Descent","title":"Manopt.gradient_descent","text":"gradient_descent(M, F, gradF, x)\n\nperform a gradient descent\n\nx_k+1 = operatornameretr_x_kbigl( s_koperatornamegradf(x_k) bigr)\n\nwith different choices of s_k available (see stepsize option below).\n\nInput\n\nM – a manifold mathcal M\nF – a cost function F mathcal Mℝ to minimize\ngradF – the gradient operatornamegradF mathcal M  Tmathcal M of F\nx – an initial value x  mathcal M\n\nOptional\n\ndirection – IdentityUpdateRule perform a processing of the direction, e.g.\nevaluation – (AllocatingEvaluation) specify whether the gradient works by allocation (default) form gradF(M, x) or MutatingEvaluation in place, i.e. is of the form gradF!(M, X, x).\nretraction_method – (default_retraction_method(M)) a retraction(M,x,ξ) to use.\nreturn_options – (false) – if activated, the extended result, i.e. the   complete Options are returned. This can be used to access recorded values.   If set to false (default) just the optimal value x_opt if returned\nstepsize – (ConstantStepsize(1.)) specify a Stepsize functor.\nstopping_criterion – (StopWhenAny(StopAfterIteration(200),StopWhenGradientNormLess(10.0^-8))) a functor inheriting from StoppingCriterion indicating when to stop.\n\n... and the ones that are passed to decorate_options for decorators.\n\nOutput\n\nx_opt – the resulting (approximately critical) point of gradientDescent\n\nOR\n\noptions - the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/gradient_descent.html#Manopt.gradient_descent!","page":"Gradient Descent","title":"Manopt.gradient_descent!","text":"gradient_descent!(M, F, gradF, x)\n\nperform a gradient_descent\n\nx_k+1 = operatornameretr_x_kbigl( s_koperatornamegradf(x_k) bigr)\n\nin place of x with different choices of s_k available.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function Fmathcal Mℝ to minimize\ngradF – the gradient operatornamegradFmathcal M Tmathcal M of F\nx – an initial value x  mathcal M\n\nFor more options, especially Stepsizes for s_k, see gradient_descent\n\n\n\n\n\n","category":"function"},{"location":"solvers/gradient_descent.html#Options-1","page":"Gradient Descent","title":"Options","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"AbstractGradientOptions\nGradientDescentOptions","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.AbstractGradientOptions","page":"Gradient Descent","title":"Manopt.AbstractGradientOptions","text":"AbstractGradientOptions <: Options\n\nA generic Options type for gradient based options data.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.GradientDescentOptions","page":"Gradient Descent","title":"Manopt.GradientDescentOptions","text":"GradientDescentOptions{P,T} <: AbstractGradientOptions\n\nDescribes a Gradient based descent algorithm, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx0 – an a point (of type P) on a manifold as starting point\ngradient – the current gradient operatornamegradf(x)\nstopping_criterion – (StopAfterIteration(100)) a StoppingCriterion\nstepsize – (ConstantStepsize(1.))a Stepsize\ndirection - (IdentityUpdateRule) a processor to compute the gradient\nretraction_method – (ExponentialRetraction()) the retraction to use, defaults to the exponential map\n\nConstructor\n\nGradientDescentOptions(x, stop, s [, retr=ExponentialRetraction()])\nGradientDescentOptions(M, x, stop, s [, retr=ExponentialRetraction()])\nGradientDescentOptions(x, X, stop, s [, retr=ExponentialRetraction()])\n\nconstruct a Gradient Descent Option with the fields and defaults as above, where the first can be used if points (x)  and tangent vectors (gradient) have the same type, for example when they are matrices. The second uses the Manifold M to set gradient=zero_vector(M,x).\n\nSee also\n\ngradient_descent, GradientProblem\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Direction-Update-Rules-1","page":"Gradient Descent","title":"Direction Update Rules","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"A field of the options is the direction, a DirectionUpdateRule, which by default IdentityUpdateRule just evaluates the gradient but can be enhanced for example to","category":"page"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"DirectionUpdateRule\nIdentityUpdateRule\nMomentumGradient\nAverageGradient\nNesterov","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.DirectionUpdateRule","page":"Gradient Descent","title":"Manopt.DirectionUpdateRule","text":"DirectionUpdateRule\n\nA general functor, that handles direction update rules. It's field(s) is usually only a StoreOptionsAction by default initialized to the fields required for the specific coefficient, but can also be replaced by a (common, global) individual one that provides these values.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.IdentityUpdateRule","page":"Gradient Descent","title":"Manopt.IdentityUpdateRule","text":"IdentityUpdateRule <: DirectionUpdateRule\n\nThe default gradient direction update is the identity, i.e. it just evaluates the gradient.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.MomentumGradient","page":"Gradient Descent","title":"Manopt.MomentumGradient","text":"MomentumGradient <: DirectionUpdateRule\n\nAppend a momentum to a gradient processor, where the last direction and last iterate are stored and the new is composed as η_i = m*η_i-1 - s d_i, where sd_i is the current (inner) direction and η_i-1 is the vector transported last direction multiplied by momentum m.\n\nFields\n\ngradient – (zero_vector(M,x0)) the last gradient/direction update added as momentum\nlast_iterate - remember the last iterate for parallel transporting the last direction\nmomentum – (0.2) factor for momentum\ndirection – internal DirectionUpdateRule to determine directions to add the momentum to.\nvector_transport_method vector transport method to use\n\nConstructors\n\nMomentumGradient(\n    p::GradientProlem,\n    x0,\n    s::DirectionUpdateRule=Gradient();\n    gradient=zero_vector(p.M, o.x), momentum=0.2\n   vector_transport_method=ParallelTransport(),\n)\n\nAdd momentum to a gradient problem, where by default just a gradient evaluation is used Equivalently you can also use a Manifold M instead of the GradientProblem p.\n\nMomentumGradient(\n    p::StochasticGradientProblem\n    x0\n    s::DirectionUpdateRule=IdentityUpdateRule();\n    gradient=zero_vector(p.M, x0), momentum=0.2\n   vector_transport_method=ParallelTransport(),\n)\n\nAdd momentum to a stochastic gradient problem, where by default just a stochastic gradient evaluation is used\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.AverageGradient","page":"Gradient Descent","title":"Manopt.AverageGradient","text":"AverageGradient <: DirectionUpdateRule\n\nAdd an average of gradients to a gradient processor. A set of previous directions (from the inner processor) and the last iterate are stored, average is taken after vector transporting them to the current iterates tangent space.\n\nFields\n\ngradients – (fill(zero_vector(M,x0),n)) the last n gradient/direction updates\nlast_iterate – last iterate (needed to transport the gradients)\ndirection – internal DirectionUpdateRule to determine directions to apply the averaging to\nvector_transport_method - vector transport method to use\n\nConstructors\n\nAverageGradient(\n    p::GradientProlem,\n    x0,\n    n::Int=10\n    s::DirectionUpdateRule=IdentityUpdateRule();\n    gradients = fill(zero_vector(p.M, o.x),n),\n    last_iterate = deepcopy(x0),\n    vector_transport_method = ParallelTransport()\n)\n\nAdd average to a gradient problem, n determines the size of averaging and gradients can be prefilled with some history Equivalently you can also use a Manifold M instead of the GradientProblem p.\n\nAverageGradient(\n    p::StochasticGradientProblem\n    x0\n    n::Int=10\n    s::DirectionUpdateRule=IdentityUpdateRule();\n    gradients = fill(zero_vector(p.M, o.x),n),\n    last_iterate = deepcopy(x0),\n    vector_transport_method = ParallelTransport()\n)\n\nAdd average to a stochastic gradient problem, n determines the size of averaging and gradients can be prefilled with some history\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.Nesterov","page":"Gradient Descent","title":"Manopt.Nesterov","text":"Nesterov <: DirectionUpdateRule\n\nFields\n\nγ\nμ the strong convexity coefficient\nv (==v_k, v_0=x_0) an interims point to compute the next gradient evaluation point y_k\nshrinkage (= i -> 0.8) a function to compute the shrinkage β_k per iterate.\n\nLet's assume f is L-Lipschitz and μ-strongly convex. Given\n\na step size h_kfrac1L (from the GradientDescentOptions\na shrinkage parameter β_k\nand a current iterate x_k\nas well as the interims values γ_k and v_k from the previous iterate.\n\nThis compute a Nesterov type update using the following steps, see [ZhangSra2018]\n\nCopute the positive root, i.e. α_k(01) of α^2 = h_kbigl((1-α_k)γ_k+α_k μbigr).\nSet bar γ_k+1 = (1-α_k)γ_k + α_kμ\ny_k = operatornameretr_x_kBigl(fracα_kγ_kγ_k + α_kμoperatornameretr^-1_x_kv_k Bigr)\nx_k+1 = operatornameretr_y_k(-h_k operatornamegradf(y_k))\nv_k+1 = operatornameretr_y_kBigl(frac(1-α_k)γ_kbarγ_koperatornameretr_y_k^-1(v_k) - fracα_kbar γ_k+1operatornamegradf(y_k) Bigr)\nγ_k+1 = frac11+β_kbar γ_k+1\n\nThen the direction from x_k to x_k+1, i.e. d = operatornameretr^-1_x_kx_k+1 is returned.\n\nConstructor\n\nNesterov(x0::P, γ=0.001, μ=0.9, schrinkage = k -> 0.8;\n    inverse_retraction_method=LogarithmicInverseRetraction())\n\nInitialize the Nesterov acceleration, where x0 initializes v.\n\n[ZhangSra2018]: H. Zhang, S. Sra: Towards Riemannian Accelerated Gradient Methods, Preprint, 2018, arXiv: 1806.02812\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Debug-Actions-1","page":"Gradient Descent","title":"Debug Actions","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"DebugGradient\nDebugGradientNorm\nDebugStepsize","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.DebugGradient","page":"Gradient Descent","title":"Manopt.DebugGradient","text":"DebugGradient <: DebugAction\n\ndebug for the gradient evaluated at the current iterate\n\nConstructors\n\nDebugGradient(; long=false, prefix= , format= \"$prefix%s\", io=stdout)\n\ndisplay the short (false) or long (true) default text for the gradient, or set the prefix manually. Alternatively the complete format can be set.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.DebugGradientNorm","page":"Gradient Descent","title":"Manopt.DebugGradientNorm","text":"DebugGradientNorm <: DebugAction\n\ndebug for gradient evaluated at the current iterate.\n\nConstructors\n\nDebugGradientNorm([long=false,p=print])\n\ndisplay the short (false) or long (true) default text for the gradient norm.\n\nDebugGradientNorm(prefix[, p=print])\n\ndisplay the a prefix in front of the gradientnorm.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.DebugStepsize","page":"Gradient Descent","title":"Manopt.DebugStepsize","text":"DebugStepsize <: DebugAction\n\ndebug for the current step size.\n\nConstructors\n\nDebugStepsize(;long=false,prefix=\"step size:\", format=\"$prefix%s\", io=stdout)\n\ndisplay the a prefix in front of the step size.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Record-Actions-1","page":"Gradient Descent","title":"Record Actions","text":"","category":"section"},{"location":"solvers/gradient_descent.html#","page":"Gradient Descent","title":"Gradient Descent","text":"RecordGradient\nRecordGradientNorm\nRecordStepsize","category":"page"},{"location":"solvers/gradient_descent.html#Manopt.RecordGradient","page":"Gradient Descent","title":"Manopt.RecordGradient","text":"RecordGradient <: RecordAction\n\nrecord the gradient evaluated at the current iterate\n\nConstructors\n\nRecordGradient(ξ)\n\ninitialize the RecordAction to the corresponding type of the tangent vector.\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.RecordGradientNorm","page":"Gradient Descent","title":"Manopt.RecordGradientNorm","text":"RecordGradientNorm <: RecordAction\n\nrecord the norm of the current gradient\n\n\n\n\n\n","category":"type"},{"location":"solvers/gradient_descent.html#Manopt.RecordStepsize","page":"Gradient Descent","title":"Manopt.RecordStepsize","text":"RecordStepsize <: RecordAction\n\nrecord the step size\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Solvers-1","page":"Introduction","title":"Solvers","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"CurrentModule = Manopt","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Solvers can be applied to Problems with solver specific Options.","category":"page"},{"location":"solvers/index.html#List-of-Algorithms-1","page":"Introduction","title":"List of Algorithms","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The following algorithms are currently available","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Solver File Problem & Option\nAlternating Gradient Descent alterating_gradient_descent.jl AlternatingGradientProblem, AlternatingGradientDescentOptions\nChambolle-Pock Chambolle-Pock.jl PrimalDualProblem, ChambollePockOptions\nCyclic Proximal Point cyclic_proximal_point.jl ProximalProblem, CyclicProximalPointOptions\nDouglas–Rachford DouglasRachford.jl ProximalProblem, DouglasRachfordOptions\nGradient Descent gradient_descent.jl GradientProblem, GradientDescentOptions\nNelder-Mead NelderMead.jl CostProblem, NelderMeadOptions\nParticle Swarm particle_swarm.jl CostProblem, ParticleSwarmOptions\nQuasi-Newton Method quasi_newton.jl GradientProblem, QuasiNewtonOptions\nSubgradient Method subgradient_method.jl SubGradientProblem, SubGradientMethodOptions\nSteihaug-Toint Truncated Conjugate-Gradient Method truncated_conjugate_gradient_descent.jl HessianProblem, TruncatedConjugateGradientOptions\nThe Riemannian Trust-Regions Solver trust_regions.jl HessianProblem, TrustRegionsOptions","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Note that the solvers (or their Options to be precise) can also be decorated to enhance your algorithm by general additional properties, see Decorated Solvers.","category":"page"},{"location":"solvers/index.html#StoppingCriteria-1","page":"Introduction","title":"StoppingCriteria","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Stopping criteria are implemented as a functor, i.e. inherit from the base type","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"StoppingCriterion\nStoppingCriterionSet","category":"page"},{"location":"solvers/index.html#Manopt.StoppingCriterion","page":"Introduction","title":"Manopt.StoppingCriterion","text":"StoppingCriterion\n\nAn abstract type for the functors representing stopping criteria, i.e. they are callable structures. The naming Scheme follows functions, see for example StopAfterIteration.\n\nEvery StoppingCriterion has to provide a constructor and its function has to have the interface (p,o,i) where a Problem as well as Options and the current number of iterations are the arguments and returns a Bool whether to stop or not.\n\nBy default each StoppingCriterion should provide a fields reason to provide details when a criterion is met (and that is empty otherwise).\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StoppingCriterionSet","page":"Introduction","title":"Manopt.StoppingCriterionSet","text":"StoppingCriterionGroup <: StoppingCriterion\n\nAn abstract type for a Stopping Criterion that itself consists of a set of Stopping criteria. In total it acts as a stopping criterion itself. Examples are StopWhenAny and StopWhenAll that can be used to combine stopping criteria.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Modules = [Manopt]\nPages = [\"plans/stopping_criterion.jl\"]\nOrder = [:type]","category":"page"},{"location":"solvers/index.html#Manopt.StopAfter","page":"Introduction","title":"Manopt.StopAfter","text":"StopAfter <: StoppingCriterion\n\nstore a threshold when to stop looking at the complete runtime. It uses time_ns() to measure the time and you provide a Period as a time limit, i.e. Minute(15)\n\nConstructor\n\nStopAfter(t)\n\ninitialize the stopping criterion to a Period t to stop after.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopAfterIteration","page":"Introduction","title":"Manopt.StopAfterIteration","text":"StopAfterIteration <: StoppingCriterion\n\nA functor for an easy stopping criterion, i.e. to stop after a maximal number of iterations.\n\nFields\n\nmaxIter – stores the maximal iteration number where to stop at\nreason – stores a reason of stopping if the stopping criterion has one be reached, see get_reason.\n\nConstructor\n\nStopAfterIteration(maxIter)\n\ninitialize the stopafterIteration functor to indicate to stop after maxIter iterations.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenAll","page":"Introduction","title":"Manopt.StopWhenAll","text":"StopWhenAll <: StoppingCriterion\n\nstore an array of StoppingCriterion elements and indicates to stop, when all indicate to stop. The reason is given by the concatenation of all reasons.\n\nConstructor\n\nStopWhenAll(c::NTuple{N,StoppingCriterion} where N)\nStopWhenAll(c::StoppingCriterion,...)\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenAny","page":"Introduction","title":"Manopt.StopWhenAny","text":"StopWhenAny <: StoppingCriterion\n\nstore an array of StoppingCriterion elements and indicates to stop, when any single one indicates to stop. The reason is given by the concatenation of all reasons (assuming that all non-indicating return \"\").\n\nConstructor\n\nStopWhenAny(c::NTuple{N,StoppingCriterion} where N)\nStopWhenAny(c::StoppingCriterion...)\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenChangeLess","page":"Introduction","title":"Manopt.StopWhenChangeLess","text":"StopWhenChangeLess <: StoppingCriterion\n\nstores a threshold when to stop looking at the norm of the change of the optimization variable from within a Options, i.e o.x. For the storage a StoreOptionsAction is used\n\nConstructor\n\nStopWhenChangeLess(ε[, a])\n\ninitialize the stopping criterion to a threshold ε using the StoreOptionsAction a, which is initialized to just store :x by default.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenCostLess","page":"Introduction","title":"Manopt.StopWhenCostLess","text":"StopWhenCostLess <: StoppingCriterion\n\nstore a threshold when to stop looking at the cost function of the optimization problem from within a Problem, i.e get_cost(p,o.x).\n\nConstructor\n\nStopWhenCostLess(ε)\n\ninitialize the stopping criterion to a threshold ε.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenGradientNormLess","page":"Introduction","title":"Manopt.StopWhenGradientNormLess","text":"StopWhenGradientNormLess <: StoppingCriterion\n\nstores a threshold when to stop looking at the norm of the gradient from within a GradientProblem.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#Manopt.StopWhenStepSizeLess","page":"Introduction","title":"Manopt.StopWhenStepSizeLess","text":"StopWhenStepsizeLess <: StoppingCriterion\n\nstores a threshold when to stop looking at the last step size determined or found during the last iteration from within a Options.\n\nConstructor\n\nStopWhenStepsizeLess(ε)\n\ninitialize the stopping criterion to a threshold ε.\n\n\n\n\n\n","category":"type"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"as well as the functions","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Base.:&(::StoppingCriterion, ::StoppingCriterion)\nBase.:|(::StoppingCriterion, ::StoppingCriterion)\nget_reason\nget_stopping_criteria\nget_active_stopping_criteria\nare_these_stopping_critera_active","category":"page"},{"location":"solvers/index.html#Base.:&-Tuple{StoppingCriterion, StoppingCriterion}","page":"Introduction","title":"Base.:&","text":"&(s1,s2)\ns1 & s2\n\nCombine two StoppingCriterion within an StopWhenAll. If either s1 (or s2) is already an StopWhenAll, then s2 (or s1) is appended to the list of StoppingCriterion within s1 (or s2).\n\nExample\n\na = StopAfterIteration(200) & StopWhenChangeLess(1e-6)\nb = a & StopWhenGradientNormLess(1e-6)\n\nIs the same as\n\na = StopWhenAll(StopAfterIteration(200), StopWhenChangeLess(1e-6))\nb = StopWhenAll(StopAfterIteration(200), StopWhenChangeLess(1e-6), StopWhenGradientNormLess(1e-6))\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Base.:|-Tuple{StoppingCriterion, StoppingCriterion}","page":"Introduction","title":"Base.:|","text":"|(s1,s2)\ns1 | s2\n\nCombine two StoppingCriterion within an StopWhenAny. If either s1 (or s2) is already an StopWhenAny, then s2 (or s1) is appended to the list of StoppingCriterion within s1 (or s2)\n\nExample\n\na = StopAfterIteration(200) | StopWhenChangeLess(1e-6)\nb = a | StopWhenGradientNormLess(1e-6)\n\nIs the same as\n\na = StopWhenAny(StopAfterIteration(200), StopWhenChangeLess(1e-6))\nb = StopWhenAny(StopAfterIteration(200), StopWhenChangeLess(1e-6), StopWhenGradientNormLess(1e-6))\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.get_reason","page":"Introduction","title":"Manopt.get_reason","text":"get_reason(o)\n\nreturn the current reason stored within the StoppingCriterion from within the Options This reason is empty if the criterion has never been met.\n\n\n\n\n\nget_reason(c)\n\nreturn the current reason stored within a StoppingCriterion c. This reason is empty if the criterion has never been met.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.get_stopping_criteria","page":"Introduction","title":"Manopt.get_stopping_criteria","text":"get_stopping_criteria(c)\n\nreturn the array of internally stored StoppingCriterions for a StoppingCriterionSet c.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.get_active_stopping_criteria","page":"Introduction","title":"Manopt.get_active_stopping_criteria","text":"get_active_stopping_criteria(c)\n\nreturns all active stopping criteria, if any, that are within a StoppingCriterion c, and indicated a stop, i.e. their reason is nonempty. To be precise for a simple stopping criterion, this returns either an empty array if no stop is indicated or the stopping criterion as the only element of an array. For a StoppingCriterionSet all internal (even nested) criteria that indicate to stop are returned.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.are_these_stopping_critera_active","page":"Introduction","title":"Manopt.are_these_stopping_critera_active","text":"are_these_stopping_critera_active(c::StoppingCriterion, cond)\n\nReturn true if any criterion from the given set is both active and fulfils the given condition cond (cond(c) returns true).\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"further stopping criteria might be available for individual Solvers.","category":"page"},{"location":"solvers/index.html#DecoratedSolvers-1","page":"Introduction","title":"Decorated Solvers","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The following decorators are available.","category":"page"},{"location":"solvers/index.html#DebugSolver-1","page":"Introduction","title":"Debug Solver","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The decorator to print debug during the iterations can be activated by decorating the Options with DebugOptions and implementing your own DebugActions. For example printing a gradient from the GradientDescentOptions is automatically available, as explained in the gradient_descent solver.","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Modules = [Manopt]\nPages   = [\"debug_solver.jl\"]","category":"page"},{"location":"solvers/index.html#Manopt.get_solver_result-Tuple{DebugOptions}","page":"Introduction","title":"Manopt.get_solver_result","text":"get_solver_result(o)\n\nReturn the final result after all iterations that is stored within the (modified during the iterations) Options o.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.initialize_solver!-Tuple{Problem, DebugOptions}","page":"Introduction","title":"Manopt.initialize_solver!","text":"initialize_solver!(p,o)\n\nInitialize the solver to the optimization Problem by initializing all values in the DebugOptionso.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.step_solver!-Tuple{Problem, DebugOptions, Any}","page":"Introduction","title":"Manopt.step_solver!","text":"step_solver!(p,o,iter)\n\nDo one iteration step (the iterth) for Problemp by modifying the values in the Optionso.options and print Debug.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.stop_solver!-Tuple{Problem, DebugOptions, Int64}","page":"Introduction","title":"Manopt.stop_solver!","text":"stop_solver!(p,o,i)\n\ndetermine whether the solver for Problem p and the DebugOptions o should stop at iteration i. If so, print all debug from :All and :Final.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#RecordSolver-1","page":"Introduction","title":"Record Solver","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The decorator to record certain values during the iterations can be activated by decorating the Options with RecordOptions and implementing your own RecordActions. For example recording the gradient from the GradientDescentOptions is automatically available, as explained in the gradient_descent solver.","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"Modules = [Manopt]\nPages   = [\"record_solver.jl\"]","category":"page"},{"location":"solvers/index.html#Manopt.get_solver_result-Tuple{RecordOptions}","page":"Introduction","title":"Manopt.get_solver_result","text":"get_solver_result(o)\n\nReturn the final result after all iterations that is stored within the (modified during the iterations) Optionso.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.initialize_solver!-Tuple{Problem, RecordOptions}","page":"Introduction","title":"Manopt.initialize_solver!","text":"initialize_solver!(p,o)\n\nInitialize the solver to the optimization Problem by initializing the encapsulated options from within the RecordOptionso.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.step_solver!-Tuple{Problem, RecordOptions, Any}","page":"Introduction","title":"Manopt.step_solver!","text":"step_solver!(p,o,iter)\n\nDo one iteration step (the iterth) for Problemp by modifying the values in the Optionso.options and record the result(s).\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Manopt.stop_solver!-Tuple{Problem, RecordOptions, Int64}","page":"Introduction","title":"Manopt.stop_solver!","text":"stop_solver!(p,o,i)\n\ndetermine whether the solver for Problem p and the RecordOptions o should stop at iteration i. If so, do a (final) record to :All and :Stop.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#Technical-Details-1","page":"Introduction","title":"Technical Details","text":"","category":"section"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"The main function a solver calls is","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"solve(p::Problem, o::Options)","category":"page"},{"location":"solvers/index.html#Manopt.solve-Tuple{Problem, Options}","page":"Introduction","title":"Manopt.solve","text":"solve(p,o)\n\nrun the solver implemented for the Problemp and the Optionso employing initialize_solver!, step_solver!, as well as the stop_solver! of the solver.\n\n\n\n\n\n","category":"method"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"which is a framework, that you in general should not change or redefine. It uses the following methods, which also need to be implemented on your own algorithm, if you want to provide one.","category":"page"},{"location":"solvers/index.html#","page":"Introduction","title":"Introduction","text":"initialize_solver!\nstep_solver!\nget_solver_result\nstop_solver!(p::Problem, o::Options, i::Int)","category":"page"},{"location":"solvers/index.html#Manopt.initialize_solver!","page":"Introduction","title":"Manopt.initialize_solver!","text":"initialize_solver!(p,o)\n\nInitialize the solver to the optimization Problem by initializing all values in the Optionso.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.step_solver!","page":"Introduction","title":"Manopt.step_solver!","text":"step_solver!(p,o,iter)\n\nDo one iteration step (the iterth) for Problemp by modifying the values in the Options o.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.get_solver_result","page":"Introduction","title":"Manopt.get_solver_result","text":"get_solver_result(o)\n\nReturn the final result after all iterations that is stored within the (modified during the iterations) Options o.\n\n\n\n\n\n","category":"function"},{"location":"solvers/index.html#Manopt.stop_solver!-Tuple{Problem, Options, Int64}","page":"Introduction","title":"Manopt.stop_solver!","text":"stop_solver!(p,o,i)\n\ndepending on the current Problem p, the current state of the solver stored in Options o and the current iterate i this function determines whether to stop the solver by calling the StoppingCriterion.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#adjointDifferentialFunctions-1","page":"Adjoint Differentials","title":"Adjoint Differentials","text":"","category":"section"},{"location":"functions/adjointdifferentials.html#","page":"Adjoint Differentials","title":"Adjoint Differentials","text":"Modules = [Manopt]\nPages   = [\"adjoint_differentials.jl\"]","category":"page"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_bezier_control-Tuple{AbstractManifold, AbstractVector{var\"#s54\"} where var\"#s54\"<:BezierSegment, AbstractVector{T} where T, AbstractVector{T} where T}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_bezier_control","text":"adjoint_differential_bezier_control(\n    M::AbstractManifold,\n    T::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_bezier_control!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    T::AbstractVector,\n    X::AbstractVector,\n)\n\nEvaluate the adjoint of the differential with respect to the controlpoints at several times T. This can be computed in place of Y.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_bezier_control-Tuple{AbstractManifold, AbstractVector{var\"#s54\"} where var\"#s54\"<:BezierSegment, Any, Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_bezier_control","text":"adjoint_differential_bezier_control(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n)\nadjoint_differential_bezier_control!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n)\n\nevaluate the adjoint of the differential of a composite Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in t_i01 on the curve and given corresponding tangential vectors X = (η_i)_i=1^n, η_iT_β(t_i)mathcal M This can be computed in place of Y.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_bezier_control-Tuple{AbstractManifold, BezierSegment, AbstractVector{T} where T, AbstractVector{T} where T}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_bezier_control","text":"adjoint_differential_bezier_control(\n    M::AbstractManifold,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_bezier_control!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a points T=(t_i)_i=1^n that are pointwise in t_i01 on the curve and given corresponding tangential vectors X = (η_i)_i=1^n, η_iT_β(t_i)mathcal M This can be computed in place of Y.\n\nSee de_casteljau for more details on the curve and[BergmannGousenbourger2018].\n\n[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics, 2018. doi: 10.3389/fams.2018.00059, arXiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_bezier_control-Tuple{AbstractManifold, BezierSegment, Any, Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_bezier_control","text":"adjoint_differential_bezier_control(M::AbstractManifold, b::BezierSegment, t, η)\nadjoint_differential_bezier_control!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t,\n    η,\n)\n\nevaluate the adjoint of the differential of a Bézier curve on the manifold M with respect to its control points b based on a point t01 on the curve and a tangent vector ηT_β(t)mathcal M. This can be computed in place of Y.\n\nSee de_casteljau for more details on the curve.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_exp_argument-Tuple{AbstractManifold, Any, Any, Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_exp_argument","text":"adjoint_differential_exp_argument(M, p, X, Y)\nadjoint_differential_exp_argument!(M, Z, p, X, Y)\n\nCompute the adjoint of D_Xexp_p XY (in place of Z). Note that X   T_p(T_pmathcal M) = T_pmathcal M is still a tangent vector.\n\nSee also\n\ndifferential_exp_argument, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_exp_basepoint-Tuple{AbstractManifold, Any, Any, Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_exp_basepoint","text":"adjoint_differential_exp_basepoint(M, p, X, Y)\nadjoint_differential_exp_basepoint!(M, Z, p, X, Y)\n\nComputes the adjoint of D_p exp_p XY (in place of Z).\n\nSee also\n\ndifferential_exp_basepoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{PowerManifold{𝔽, TM, TSize, TPR}, Any, Any}} where {𝔽, TM, TSize, TPR}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_forward_logs","text":"Y = adjoint_differential_forward_logs(M, p, X)\nadjoint_differential_forward_logs!(M, Y, p, X)\n\nCompute the adjoint differential of forward_logs F orrucirng, in the power manifold array p, the differential of the function\n\nF_i(p) = sum_j  mathcal I_i log_p_i p_j\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i Let n be the number dimensions of the PowerManifold manifold (i.e. length(size(x))). Then the input tangent vector lies on the manifold mathcal M = mathcal M^n. The adjoint differential can be computed in place of Y.\n\nInput\n\nM     – a PowerManifold manifold\np     – an array of points on a manifold\nX     – a tangent vector to from the n-fold power of p, where n is the ndims of p\n\nOuput\n\nY – resulting tangent vector in T_pmathcal M representing the adjoint   differentials of the logs.\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_geodesic_endpoint-Tuple{AbstractManifold, Any, Any, Any, Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_geodesic_endpoint","text":"adjoint_differential_geodesic_endpoint(M, p, q, t, X)\nadjoint_differential_geodesic_endpoint!(M, Y, p, q, t, X)\n\nCompute the adjoint of D_q γ(t p q)X (in place of Y).\n\nSee also\n\ndifferential_geodesic_endpoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_geodesic_startpoint-Tuple{AbstractManifold, Any, Any, Any, Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_geodesic_startpoint","text":"adjoint_differential_geodesic_startpoint(M,p, q, t, X)\nadjoint_differential_geodesic_startpoint!(M, Y, p, q, t, X)\n\nCompute the adjoint of D_p γ(t p q)X (in place of Y).\n\nSee also\n\ndifferential_geodesic_startpoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_log_argument-Tuple{AbstractManifold, Any, Any, Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_log_argument","text":"adjoint_differential_log_argument(M, p, q, X)\nadjoint_differential_log_argument!(M, Y, p, q, X)\n\nCompute the adjoint of D_q log_p qX (in place of Y).\n\nSee also\n\ndifferential_log_argument, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"functions/adjointdifferentials.html#Manopt.adjoint_differential_log_basepoint-Tuple{AbstractManifold, Any, Any, Any}","page":"Adjoint Differentials","title":"Manopt.adjoint_differential_log_basepoint","text":"adjoint_differential_log_basepoint(M, p, q, X)\nadjoint_differential_log_basepoint!(M, Y, p, q, X)\n\ncomputes the adjoint of D_p log_p qX (in place of Y).\n\nSee also\n\ndifferential_log_basepoint, adjoint_Jacobi_field\n\n\n\n\n\n","category":"method"},{"location":"solvers/quasi_Newton.html#quasiNewton-1","page":"Quasi-Newton","title":"Riemannian quasi-Newton methods","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"    CurrentModule = Manopt","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"    quasi_Newton\n    quasi_Newton!","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.quasi_Newton","page":"Quasi-Newton","title":"Manopt.quasi_Newton","text":"quasi_Newton(M, F, gradF, x)\n\nPerform a quasi Newton iteration for F on the manifold M starting in the point x using a retraction R and a vector transport T\n\nThe kth iteration consists of\n\nCompute the search direction η_k = -mathcalB_k operatornamegradf (x_k) or solve mathcalH_k η_k = -operatornamegradf (x_k).\nDetermine a suitable stepsize α_k along the curve gamma(α) = R_x_k(α η_k) e.g. by using WolfePowellLineseach.\nCompute x_k+1 = R_x_k(α_k η_k).\nDefine s_k = T_x_k α_k η_k(α_k η_k) and y_k = operatornamegradf(x_k+1) - T_x_k α_k η_k(operatornamegradf(x_k)).\nCompute the new approximate Hessian H_k+1 or its inverse B_k.\n\nInput\n\nM – a manifold mathcalM.\nF – a cost function F  mathcalM ℝ to minimize.\ngradF– the gradient operatornamegradF  mathcalM T_xmathcal M of F.\nx – an initial value x  mathcalM.\nstoppingcriterion::StoppingCriterion=StopWhenAny(       StopAfterIteration(max(1000, memorysize)), StopWhenGradientNormLess(10^(-6))   ),   return_options=false,\n\nOptional\n\nbasis – (DefaultOrthonormalBasis()) basis within the tangent space(s) to represent the Hessian (inverse).\ncautious_update – (false) – whether or not to use a QuasiNewtonCautiousDirectionUpdate\ncautious_function – ((x) -> x*10^(-4)) – a monotone increasing function that is zero at 0 and strictly increasing at 0 for the cautious update.\ndirection_update – (InverseBFGS()) the update rule to use.\nevaluation – (AllocatingEvaluation) specify whether the gradient works by  allocation (default) form gradF(M, x) or MutatingEvaluation in place, i.e.  is of the form gradF!(M, X, x).\ninitial_operator – (Matrix{Float64}(I,n,n)) initial matrix to use die the approximation, where n=manifold_dimension(M), see also scale_initial_operator.\nmemory_size – (20) limited memory, number of s_k y_k to store. Set to a negative value to use a full memory representation\nretraction_method – (default_retraction_method(M)) a retraction method to use, by default the exponential map.\nscale_initial_operator - (true) scale initial operator with fracs_ky_k_x_klVert y_krVert_x_k in the computation\nstabilize – (true) stabilize the method numerically by projecting computed (Newton-) directions to the tangent space to reduce numerical errors\nstepsize – (WolfePowellLineseach(retraction_method, vector_transport_method)) specify a Stepsize.\nstopping_criterion - (StopWhenAny(StopAfterIteration(max(1000, memory_size)), StopWhenGradientNormLess(10^(-6))) specify a StoppingCriterion\nvector_transport_method – (default_vector_transport_method(M)) a vector transport to use.\nreturn_options – (false) – specify whether to return just the result x (default) or the complete Options, e.g. to access recorded values. if activated, the extended result, i.e. the\n\nOutput\n\nx_opt – the resulting (approximately critical) point of the quasi–Newton method\n\nOR\n\noptions – the options returned by the solver (see return_options)\n\n\n\n\n\n","category":"function"},{"location":"solvers/quasi_Newton.html#Manopt.quasi_Newton!","page":"Quasi-Newton","title":"Manopt.quasi_Newton!","text":"quasi_Newton!(M, F, gradF, x; options...)\n\nPerform a quasi Newton iteration for F on the manifold M starting in the point x using a retraction R and a vector transport T.\n\nInput\n\nM – a manifold mathcalM.\nF – a cost function F mathcalM ℝ to minimize.\ngradF– the gradient operatornamegradF  mathcalM  T_xmathcal M of F.\nx – an initial value x  mathcalM.\n\nFor all optional parameters, see quasi_Newton.\n\n\n\n\n\n","category":"function"},{"location":"solvers/quasi_Newton.html#Background-1","page":"Quasi-Newton","title":"Background","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"The aim is to minimize a real-valued function on a Riemannian manifold, i.e.","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"min f(x) quad x  mathcalM","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"Riemannian quasi-Newtonian methods are as generalizations of their Euclidean counterparts Riemannian line search methods. These methods determine a search direction η_k  T_x_k mathcalM at the current iterate x_k and a suitable stepsize α_k along gamma(α) = R_x_k(α η_k), where R T mathcalM mathcalM is a retraction. The next iterate is obtained by","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"x_k+1 = R_x_k(α_k η_k)","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"In quasi-Newton methods, the search direction is given by","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"η_k = -mathcalH_k^-1operatornamegradf (x_k) = -mathcalB_k operatornamegrad (x_k)","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"where mathcalH_k  T_x_k mathcalM T_x_k mathcalM is a positive definite self-adjoint operator, which approximates the action of the Hessian operatornameHess f (x_k) and mathcalB_k = mathcalH_k^-1. The idea of quasi-Newton methods is instead of creating a complete new approximation of the Hessian operator operatornameHess f(x_k+1) or its inverse at every iteration, the previous operator mathcalH_k or mathcalB_k is updated by a convenient formula using the obtained information about the curvature of the objective function during the iteration. The resulting operator mathcalH_k+1 or mathcalB_k+1 acts on the tangent space T_x_k+1 mathcalM of the freshly computed iterate x_k+1. In order to get a well-defined method, the following requirements are placed on the new operator mathcalH_k+1 or mathcalB_k+1 that is created by an update. Since the Hessian operatornameHess f(x_k+1) is a self-adjoint operator on the tangent space T_x_k+1 mathcalM, and mathcalH_k+1 approximates it, we require that mathcalH_k+1 or mathcalB_k+1 is also self-adjoint on T_x_k+1 mathcalM. In order to achieve a steady descent, we want η_k to be a descent direction in each iteration. Therefore we require, that mathcalH_k+1 or mathcalB_k+1 is a positive definite operator on T_x_k+1 mathcalM. In order to get information about the curvature of the objective function into the new operator mathcalH_k+1 or mathcalB_k+1, we require that it satisfies a form of a Riemannian quasi-Newton equation:","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"mathcalH_k+1 T_x_k rightarrow x_k+1(R_x_k^-1(x_k+1)) = operatornamegrad(x_k+1) - T_x_k rightarrow x_k+1(operatornamegradf(x_k))","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"or","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"mathcalB_k+1 operatornamegradf(x_k+1) - T_x_k rightarrow x_k+1(operatornamegradf(x_k)) = T_x_k rightarrow x_k+1(R_x_k^-1(x_k+1))","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"where T_x_k rightarrow x_k+1  T_x_k mathcalM T_x_k+1 mathcalM and the chosen retraction R is the associated retraction of T. We note that, of course, not all updates in all situations will meet these conditions in every iteration. For specific quasi-Newton updates, the fulfilment of the Riemannian curvature condition, which requires that","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"g_x_k+1(s_k y_k)  0","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"holds, is a requirement for the inheritance of the self-adjointness and positive definiteness of the mathcalH_k or mathcalB_k to the operator mathcalH_k+1 or mathcalB_k+1. Unfortunately, the fulfillment of the Riemannian curvature condition is not given by a step size alpha_k  0 that satisfies the generalised Wolfe conditions. However, in order to create a positive definite operator mathcalH_k+1 or mathcalB_k+1 in each iteration, in [HuangGallivanAbsil2015] the so-called locking condition was introduced, which requires that the isometric vector transport T^S, which is used in the update formula, and its associate retraction R fulfill","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"T^Sx ξ_x(ξ_x) = β T^Rx ξ_x(ξ_x) quad β = fraclVert ξ_x rVert_xlVert T^Rx ξ_x(ξ_x) rVert_R_x(ξ_x)","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"where T^R is the vector transport by differentiated retraction. With the requirement that the isometric vector transport T^S and its associated retraction R satisfies the locking condition and using the tangent vector","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"y_k = β_k^-1 operatornamegradf(x_k+1) - T^Sx_k α_k η_k(operatornamegradf(x_k))","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"where","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"β_k = fraclVert α_k η_k rVert_x_klVert T^Rx_k α_k η_k(α_k η_k) rVert_x_k+1","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"in the update, it can be shown that choosing a stepsize α_k  0 that satisfies the Riemannian Wolfe conditions leads to the fulfilment of the Riemannian curvature condition, which in turn implies that the operator generated by the updates is positive definite. In the following we denote the specific operators in matrix notation and hence use H_k and B_k, respectively.","category":"page"},{"location":"solvers/quasi_Newton.html#Direction-Updates-1","page":"Quasi-Newton","title":"Direction Updates","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"In general there are different ways to compute a fixed AbstractQuasiNewtonUpdateRule. In general these are represented by","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"AbstractQuasiNewtonDirectionUpdate\nQuasiNewtonMatrixDirectionUpdate\nQuasiNewtonLimitedMemoryDirectionUpdate\nQuasiNewtonCautiousDirectionUpdate","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.AbstractQuasiNewtonDirectionUpdate","page":"Quasi-Newton","title":"Manopt.AbstractQuasiNewtonDirectionUpdate","text":"AbstractQuasiNewtonDirectionUpdate\n\nAn abstract representation of an Quasi Newton Update rule to determine the next direction given current QuasiNewtonOptions.\n\nAll subtypes should be functors, i.e. one should be able to call them as H(M,x,d) to compute a new direction update.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.QuasiNewtonMatrixDirectionUpdate","page":"Quasi-Newton","title":"Manopt.QuasiNewtonMatrixDirectionUpdate","text":"QuasiNewtonMatrixDirectionUpdate <: AbstractQuasiNewtonDirectionUpdate\n\nThese AbstractQuasiNewtonDirectionUpdates represent any quasi-Newton update rule, where the operator is stored as a matrix. A distinction is made between the update of the approximation of the Hessian, H_k mapsto H_k+1, and the update of the approximation of the Hessian inverse, B_k mapsto B_k+1. For the first case, the coordinates of the search direction η_k with respect to a basis b_i^n_i=1 are determined by solving a linear system of equations, i.e.\n\ntextSolve quad hatη_k = - H_k widehatoperatornamegradf(x_k)\n\nwhere H_k is the matrix representing the operator with respect to the basis b_i^n_i=1 and widehatoperatornamegradf(x_k) represents the coordinates of the gradient of the objective function f in x_k with respect to the basis b_i^n_i=1. If a method is chosen where Hessian inverse is approximated, the coordinates of the search direction η_k with respect to a basis b_i^n_i=1 are obtained simply by matrix-vector multiplication, i.e.\n\nhatη_k = - B_k widehatoperatornamegradf(x_k)\n\nwhere B_k is the matrix representing the operator with respect to the basis b_i^n_i=1 and widehatoperatornamegradf(x_k) as above. In the end, the search direction η_k is generated from the coordinates hateta_k and the vectors of the basis b_i^n_i=1 in both variants. The AbstractQuasiNewtonUpdateRule indicates which quasi-Newton update rule is used. In all of them, the Euclidean update formula is used to generate the matrix H_k+1 and B_k+1, and the basis b_i^n_i=1 is transported into the upcoming tangent space T_x_k+1 mathcalM, preferably with an isometric vector transport, or generated there.\n\nFields\n\nbasis – the basis.\nmatrix – the matrix which represents the approximating operator.\nscale – indicates whether the initial matrix (= identity matrix) should be scaled before the first update.\nupdate – a AbstractQuasiNewtonUpdateRule.\nvector_transport_method – an AbstractVectorTransportMethod\n\nSee also\n\nQuasiNewtonLimitedMemoryDirectionUpdate QuasiNewtonCautiousDirectionUpdate AbstractQuasiNewtonDirectionUpdate\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.QuasiNewtonLimitedMemoryDirectionUpdate","page":"Quasi-Newton","title":"Manopt.QuasiNewtonLimitedMemoryDirectionUpdate","text":"QuasiNewtonLimitedMemoryDirectionUpdate <: AbstractQuasiNewtonDirectionUpdate\n\nThis AbstractQuasiNewtonDirectionUpdate represents the limited-memory Riemanian BFGS update, where the approximating  operator is represented by m stored pairs of tangent vectors  widetildes_i widetildey_i_i=k-m^k-1 in the k-th iteration. For the calculation of the search direction η_k, the generalisation of the two-loop recursion is used (see [HuangGallivanAbsil2015]), since it only requires inner products and linear combinations of tangent vectors in T_x_k mathcalM. For that the stored pairs of tangent vectors  widetildes_i widetildey_i_i=k-m^k-1, the gradient operatornamegradf(x_k) of the objective function f in x_k and the positive definite self-adjoint operator\n\nmathcalB^(0)_k = fracg_x_k(s_k-1 y_k-1)g_x_k(y_k-1 y_k-1)  mathrmid_T_x_k mathcalM\n\nare used. The two-loop recursion can be understood as that the InverseBFGS update is executed m times in a row on mathcalB^(0)_k using the tangent vectors  widetildes_i widetildey_i_i=k-m^k-1, and in the same time the resulting operator mathcalB^LRBFGS_k  is directly applied on operatornamegradf(x_k). When updating there are two cases: if there is still free memory, i.e. k  m, the previously stored vector pairs  widetildes_i widetildey_i_i=k-m^k-1 have to be transported into the upcoming tangent space T_x_k+1 mathcalM; if there is no free memory, the oldest pair  widetildes_km widetildey_km has to be discarded and then all the remaining vector pairs  widetildes_i widetildey_i_i=k-m+1^k-1 are transported into the tangent space T_x_k+1 mathcalM. After that we calculate and store s_k = widetildes_k = T^S_x_k α_k η_k(α_k η_k) and y_k = widetildey_k. This process ensures that new information about the objective function is always included and the old, probably no longer relevant, information is discarded.\n\nFields\n\nmethod – the maximum number of vector pairs stored.\nmemory_s – the set of the stored (and transported) search directions times step size  widetildes_i_i=k-m^k-1.\nmemory_y – set of the stored gradient differences  widetildey_i_i=k-m^k-1.\nξ – a variable used in the two-loop recursion.\nρ – a variable used in the two-loop recursion.\nscale –\nvector_transport_method – a AbstractVectorTransportMethod\n\nSee also\n\nInverseBFGS QuasiNewtonCautiousDirectionUpdate AbstractQuasiNewtonDirectionUpdate\n\n[HuangGallivanAbsil2015]: Huang, Wen and Gallivan, K. A. and Absil, P.-A., A Broyden Class of Quasi-Newton Methods for Riemannian Optimization, SIAM J. Optim., 25 (2015), pp. 1660-1685. doi: 10.1137/140955483\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.QuasiNewtonCautiousDirectionUpdate","page":"Quasi-Newton","title":"Manopt.QuasiNewtonCautiousDirectionUpdate","text":"QuasiNewtonCautiousDirectionUpdate <: AbstractQuasiNewtonDirectionUpdate\n\nThese AbstractQuasiNewtonDirectionUpdates represent any quasi-Newton update rule, which are based on the idea of a so-called cautious update. The search direction is calculated as given in QuasiNewtonMatrixDirectionUpdate or [LimitedMemoryQuasiNewctionDirectionUpdate]. But the update given in QuasiNewtonMatrixDirectionUpdate or [LimitedMemoryQuasiNewctionDirectionUpdate] is only executed if\n\nfracg_x_k+1(y_ks_k)lVert s_k rVert^2_x_k+1 geq theta(lVert operatornamegradf(x_k) rVert_x_k)\n\nis satisfied, where theta is a monotone increasing function satisfying theta(0) = 0 and theta is strictly increasing at 0. If this is not the case, the corresponding update will be skipped, which means that for QuasiNewtonMatrixDirectionUpdate the matrix H_k or B_k is not updated, but the basis b_i^n_i=1 is nevertheless transported into the upcoming tangent space T_x_k+1 mathcalM, and for [LimitedMemoryQuasiNewctionDirectionUpdate] neither the oldest vector pair  widetildes_km widetildey_km is discarded nor the newest vector pair  widetildes_k widetildey_k is added into storage, but all stored vector pairs  widetildes_i widetildey_i_i=k-m^k-1 are transported into the tangent space T_x_k+1 mathcalM. If InverseBFGS or InverseBFGS is chosen as update, then the resulting method follows the method of [HuangAbsilGallivan2018], taking into account that the corresponding step size is chosen.\n\nFields\n\nupdate – an AbstractQuasiNewtonDirectionUpdate\nθ – a monotone increasing function satisfying θ(0) = 0 and θ is strictly increasing at 0.\n\nSee also\n\nQuasiNewtonMatrixDirectionUpdate QuasiNewtonLimitedMemoryDirectionUpdate\n\n[HuangAbsilGallivan2018]: Huang, Wen and Absil, P.-A and Gallivan, Kyle, A Riemannian BFGS Method Without Differentiated Retraction for Nonconvex Optimization Problems, SIAM J. Optim., 28 (2018), pp. 470-495. doi: 10.1137/17M1127582\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Hessian-Update-Rules-1","page":"Quasi-Newton","title":"Hessian Update Rules","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"Using","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"update_hessian!","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.update_hessian!","page":"Quasi-Newton","title":"Manopt.update_hessian!","text":"update_hessian!(d, p, o, x_old, iter)\n\nupdate the hessian wihtin the QuasiNewtonOptions o given a Problem p as well as the an AbstractQuasiNewtonDirectionUpdate d and the last iterate x_old. Note that the current (iterth) iterate is already stored in o.x.\n\nSee also AbstractQuasiNewtonUpdateRule for the different rules that are available within d.\n\n\n\n\n\n","category":"function"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"the following update formulae for either H_k+1 or B_k+1 are available.","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"AbstractQuasiNewtonUpdateRule\nBFGS\nDFP\nBroyden\nSR1\nInverseBFGS\nInverseDFP\nInverseBroyden\nInverseSR1","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.AbstractQuasiNewtonUpdateRule","page":"Quasi-Newton","title":"Manopt.AbstractQuasiNewtonUpdateRule","text":"AbstractQuasiNewtonUpdateRule\n\nSpecify a type for the different AbstractQuasiNewtonDirectionUpdates.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.BFGS","page":"Quasi-Newton","title":"Manopt.BFGS","text":"BFGS <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the Riemanian BFGS update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeH_k^mathrmBFGS the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nH^mathrmBFGS_k+1 = widetildeH^mathrmBFGS_k  + fracy_k y^mathrmT_k s^mathrmT_k y_k - fracwidetildeH^mathrmBFGS_k s_k s^mathrmT_k widetildeH^mathrmBFGS_k s^mathrmT_k widetildeH^mathrmBFGS_k s_k\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\noperatornamegradf(x_k+1) - T^S_x_k α_k η_k(operatornamegradf(x_k))  T_x_k+1 mathcalM\n\nrespectively.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.DFP","page":"Quasi-Newton","title":"Manopt.DFP","text":"DFP <: AbstractQuasiNewtonUpdateRule\n\nindicates in an AbstractQuasiNewtonDirectionUpdate that the Riemanian DFP update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeH_k^mathrmDFP the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nH^mathrmDFP_k+1 = Bigl(\n  mathrmid_T_x_k+1 mathcalM - fracy_k s^mathrmT_ks^mathrmT_k y_k\nBigr)\nwidetildeH^mathrmDFP_k\nBigl(\n  mathrmid_T_x_k+1 mathcalM - fracs_k y^mathrmT_ks^mathrmT_k y_k\nBigr) + fracy_k y^mathrmT_ks^mathrmT_k y_k\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\noperatornamegradf(x_k+1) - T^S_x_k α_k η_k(operatornamegradf(x_k))  T_x_k+1 mathcalM\n\nrespectively.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.Broyden","page":"Quasi-Newton","title":"Manopt.Broyden","text":"Broyden <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the Riemanian Broyden update is used in the Riemannian quasi-Newton method, which is as a convex combination of BFGS and DFP.\n\nWe denote by widetildeH_k^mathrmBr the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nH^mathrmBr_k+1 = widetildeH^mathrmBr_k\n  - fracwidetildeH^mathrmBr_k s_k s^mathrmT_k widetildeH^mathrmBr_ks^mathrmT_k widetildeH^mathrmBr_k s_k + fracy_k y^mathrmT_ks^mathrmT_k y_k\n  + φ_k s^mathrmT_k widetildeH^mathrmBr_k s_k\n  Bigl(\n        fracy_ks^mathrmT_k y_k - fracwidetildeH^mathrmBr_k s_ks^mathrmT_k widetildeH^mathrmBr_k s_k\n  Bigr)\n  Bigl(\n        fracy_ks^mathrmT_k y_k - fracwidetildeH^mathrmBr_k s_ks^mathrmT_k widetildeH^mathrmBr_k s_k\n  Bigr)^mathrmT\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\noperatornamegradf(x_k+1) - T^S_x_k α_k η_k(operatornamegradf(x_k))  T_x_k+1 mathcalM\n\nrespectively, and φ_k is the Broyden factor which is :constant by default but can also be set to :Davidon.\n\nConstructor\n\nBroyden(φ, update_rule::Symbol = :constant)\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.SR1","page":"Quasi-Newton","title":"Manopt.SR1","text":"SR1 <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the Riemanian SR1 update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeH_k^mathrmSR1 the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nH^mathrmSR1_k+1 = widetildeH^mathrmSR1_k\n+ frac\n  (y_k - widetildeH^mathrmSR1_k s_k) (y_k - widetildeH^mathrmSR1_k s_k)^mathrmT\n\n(y_k - widetildeH^mathrmSR1_k s_k)^mathrmT s_k\n\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\noperatornamegradf(x_k+1) - T^S_x_k α_k η_k(operatornamegradf(x_k))  T_x_k+1 mathcalM\n\nrespectively.\n\nThis method can be stabilized by only performing the update if denominator is larger than rlVert s_krVert_x_k+1lVert y_k - widetildeH^mathrmSR1_k s_k rVert_x_k+1 for some r0. For more details, see Section 6.2 in [NocedalWright2006]\n\n[NocedalWright2006]: Nocedal, J., Wright, S.: Numerical Optimization, Second Edition, Springer, 2006. doi: 10.1007/978-0-387-40065-5\n\nConstructor\n\nSR1(r::Float64=-1.0)\n\nGenerate the SR1 update, which by default does not include the check (since the default sets t0`)\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.InverseBFGS","page":"Quasi-Newton","title":"Manopt.InverseBFGS","text":"InverseBFGS <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the inverse Riemanian BFGS update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeB_k^mathrmBFGS the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nB^mathrmBFGS_k+1  = Bigl(\n  mathrmid_T_x_k+1 mathcalM - fracs_k y^mathrmT_k s^mathrmT_k y_k\nBigr)\nwidetildeB^mathrmBFGS_k\nBigl(\n  mathrmid_T_x_k+1 mathcalM - fracy_k s^mathrmT_k s^mathrmT_k y_k\nBigr) + fracs_k s^mathrmT_ks^mathrmT_k y_k\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\noperatornamegradf(x_k+1) - T^S_x_k α_k η_k(operatornamegradf(x_k))  T_x_k+1 mathcalM\n\nrespectively.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.InverseDFP","page":"Quasi-Newton","title":"Manopt.InverseDFP","text":"InverseDFP <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the inverse Riemanian DFP update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeB_k^mathrmDFP the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nB^mathrmDFP_k+1 = widetildeB^mathrmDFP_k\n+ fracs_k s^mathrmT_ks^mathrmT_k y_k\n- fracwidetildeB^mathrmDFP_k y_k y^mathrmT_k widetildeB^mathrmDFP_ky^mathrmT_k widetildeB^mathrmDFP_k y_k\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\noperatornamegradf(x_k+1) - T^S_x_k α_k η_k(operatornamegradf(x_k))  T_x_k+1 mathcalM\n\nrespectively.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.InverseBroyden","page":"Quasi-Newton","title":"Manopt.InverseBroyden","text":"InverseBroyden <: AbstractQuasiNewtonUpdateRule\n\nIndicates in AbstractQuasiNewtonDirectionUpdate that the Riemanian Broyden update is used in the Riemannian quasi-Newton method, which is as a convex combination of InverseBFGS and InverseDFP.\n\nWe denote by widetildeH_k^mathrmBr the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nB^mathrmBr_k+1 = widetildeB^mathrmBr_k\n - fracwidetildeB^mathrmBr_k y_k y^mathrmT_k widetildeB^mathrmBr_ky^mathrmT_k widetildeB^mathrmBr_k y_k\n   + fracs_k s^mathrmT_ks^mathrmT_k y_k\n + φ_k y^mathrmT_k widetildeB^mathrmBr_k y_k\n Bigl(\n     fracs_ks^mathrmT_k y_k - fracwidetildeB^mathrmBr_k y_ky^mathrmT_k widetildeB^mathrmBr_k y_k\n    Bigr) Bigl(\n        fracs_ks^mathrmT_k y_k - fracwidetildeB^mathrmBr_k y_ky^mathrmT_k widetildeB^mathrmBr_k y_k\n Bigr)^mathrmT\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\noperatornamegradf(x_k+1) - T^S_x_k α_k η_k(operatornamegradf(x_k))  T_x_k+1 mathcalM\n\nrespectively, and φ_k is the Broyden factor which is :constant by default but can also be set to :Davidon.\n\nConstructor\n\nInverseBroyden(φ, update_rule::Symbol = :constant)\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Manopt.InverseSR1","page":"Quasi-Newton","title":"Manopt.InverseSR1","text":"InverseSR1 <: AbstractQuasiNewtonUpdateRule\n\nindicates in AbstractQuasiNewtonDirectionUpdate that the inverse Riemanian SR1 update is used in the Riemannian quasi-Newton method.\n\nWe denote by widetildeB_k^mathrmSR1 the operator concatenated with a vector transport and its inverse before and after to act on x_k+1 = R_x_k(α_k η_k). Then the update formula reads\n\nB^mathrmSR1_k+1 = widetildeB^mathrmSR1_k\n+ frac\n  (s_k - widetildeB^mathrmSR1_k y_k) (s_k - widetildeB^mathrmSR1_k y_k)^mathrmT\n\n  (s_k - widetildeB^mathrmSR1_k y_k)^mathrmT y_k\n\n\nwhere s_k and y_k are the coordinate vectors with respect to the current basis (from QuasiNewtonOptions) of\n\nT^S_x_k α_k η_k(α_k η_k) quadtextandquad\noperatornamegradf(x_k+1) - T^S_x_k α_k η_k(operatornamegradf(x_k))  T_x_k+1 mathcalM\n\nrespectively.\n\nThis method can be stabilized by only performing the update if denominator is larger than rlVert y_krVert_x_k+1lVert s_k - widetildeH^mathrmSR1_k y_k rVert_x_k+1 for some r0. For more details, see Section 6.2 in [NocedalWright2006].\n\nConstructor\n\nInverseSR1(r::Float64=-1.0)\n\nGenerate the InverseSR1 update, which by default does not include the check, since the default sets t0`.\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Options-1","page":"Quasi-Newton","title":"Options","text":"","category":"section"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"The quasi Newton algorithm is based on a GradientProblem.","category":"page"},{"location":"solvers/quasi_Newton.html#","page":"Quasi-Newton","title":"Quasi-Newton","text":"QuasiNewtonOptions","category":"page"},{"location":"solvers/quasi_Newton.html#Manopt.QuasiNewtonOptions","page":"Quasi-Newton","title":"Manopt.QuasiNewtonOptions","text":"QuasiNewtonOptions <: Options\n\nThese Quasi Newton Options represent any quasi-Newton based method and can be used with any update rule for the direction.\n\nFields\n\nx – the current iterate, a point on a manifold\ngradient – the current gradient\nsk – the current step\nyk the current gradient difference\ndirection_update - an AbstractQuasiNewtonDirectionUpdate rule.\nretraction_method – an AbstractRetractionMethod\nstop – a StoppingCriterion\n\nSee also\n\nGradientProblem\n\n\n\n\n\n","category":"type"},{"location":"solvers/quasi_Newton.html#Literature-1","page":"Quasi-Newton","title":"Literature","text":"","category":"section"},{"location":"list.html#Table-of-Contents,-Types-and-Functions-1","page":"Function Index","title":"Table of Contents, Types and Functions","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"This page lists all pages of this documentations, all available types and functions.","category":"page"},{"location":"list.html#Complete-List-of-Contents-1","page":"Function Index","title":"Complete List of Contents","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Depth = 3","category":"page"},{"location":"list.html#Available-Types-1","page":"Function Index","title":"Available Types","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Modules = [Manopt]\nOrder   = [:type]","category":"page"},{"location":"list.html#Solver-Functions-1","page":"Function Index","title":"Solver Functions","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Modules = [Manopt]\nPages = [\"plans/index.md\", \"solvers/index.md\"]","category":"page"},{"location":"list.html#Functions-1","page":"Function Index","title":"Functions","text":"","category":"section"},{"location":"list.html#","page":"Function Index","title":"Function Index","text":"Modules = [Manopt]\nPages = [\"functions/adjointDifferentials.md\", \"functions/costs.md\", \"functions/differentials.md\", \"functions/gradients.md\", \"functions/jacobiFields.md\", \"functions/proximalMaps.md\"]","category":"page"},{"location":"solvers/trust_regions.html#trust_regions-1","page":"Trust-Regions Solver","title":"The Riemannian Trust-Regions Solver","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"The aim is to solve an optimization problem on a manifold","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"operatorname*min_x    mathcalM F(x)","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"by using the Riemannian trust-regions solver. It is number one choice for smooth optimization. This trust-region method uses the Steihaug-Toint truncated conjugate-gradient method truncated_conjugate_gradient_descent to solve the inner minimization problem called the trust-regions subproblem. This inner solve can be preconditioned by providing a preconditioner (symmetric and positive deﬁnite, an approximation of the inverse of the Hessian of F). If no Hessian of the cost function F is provided, a standard approximation of the Hessian based on the gradient operatornamegradF with ApproxHessianFiniteDifference will be computed.","category":"page"},{"location":"solvers/trust_regions.html#Initialization-1","page":"Trust-Regions Solver","title":"Initialization","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"Initialize x_0 = x with an initial point x on the manifold. It can be given by the caller or set randomly. Set the initial trust-region radius Delta =frac18 barDelta where barDelta is the maximum radius the trust-region can have. Usually one uses the root of the manifold dimension operatornamedim(mathcalM). For accepting the next iterate and evaluating the new trust-region radius one needs an accept/reject threshold rho    0frac14), which is rho = 01 on default. Set k=0.","category":"page"},{"location":"solvers/trust_regions.html#Iteration-1","page":"Trust-Regions Solver","title":"Iteration","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"Repeat until a convergence criterion is reached","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"Set η as a random tangent vector if using randomized approach. Else  set η as the zero vector in the tangential space T_x_kmathcalM.\nSet η^*  as the solution of the trust-region subproblem, computed by  the tcg-method with η as initial vector.\nIf using randomized approach compare η^*  with the Cauchy point  η_c^*  = -tau_c fracDeltaoperatornamenorm(operatornameGradf (x_k)) operatornameGradF (x_k) by the model function m_x_k(). If the  model decrease is larger by using the Cauchy point, set  η^*  = η_c^* .\nSet x^*  = operatornameRetr_x_k(η^* ).\nSet rho = fracF(x_k)-F(x^* )m_x_k(η)-m_x_k(η^* ), where  m_x_k() describes the quadratic model function.\nUpdate the trust-region radius:  Delta = begincases frac14 Delta  rho  frac14   textor  m_x_k(η)-m_x_k(η^* ) leq 0  textor    rho = pm   fty   operatornamemin(2 Delta barDelta)   rho  frac34  textand the tcg-method stopped because of negative  curvature or exceeding the trust-region  Delta   textotherwise  endcases\nIf m_x_k(η)-m_x_k(η^* ) geq 0 and rho  rho set  x_k = x^* .\nSet k = k+1.","category":"page"},{"location":"solvers/trust_regions.html#Result-1","page":"Trust-Regions Solver","title":"Result","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"The result is given by the last computed x_k.","category":"page"},{"location":"solvers/trust_regions.html#Remarks-1","page":"Trust-Regions Solver","title":"Remarks","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To the Initialization: A random point on the manifold.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 1: Using randomized approach means using a random tangent vector as initial vector for the approximal solve of the trust-regions subproblem. If this is the case, keep in mind that the vector must be in the trust-region radius. This is achieved by multiplying η by sqrt(4,eps(Float64)) as long as its norm is greater than the current trust-region radius Delta. For not using randomized approach, one can get the zero tangent vector.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 2: Obtain η^*  by (approximately) solving the trust-regions subproblem","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"operatorname*argmin_η    T_x_kmathcalM m_x_k(η) = F(x_k) +\nlangle operatornamegradF(x_k) η rangle_x_k + frac12 langle\noperatornameHessF(η)_ x_k η rangle_x_k","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"textst  langle η η rangle_x_k leq Delta^2","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"with the Steihaug-Toint truncated conjugate-gradient (tcg) method. The problem as well as the solution method is described in the truncated_conjugate_gradient_descent.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 3: If using a random tangent vector as an initial vector, compare the result of the tcg-method with the Cauchy point. Convergence proofs assume that one achieves at least (a fraction of) the reduction of the Cauchy point. The idea is to go in the direction of the gradient to an optimal point. This can be on the edge, but also before. The parameter tau_c for the optimal length is defined by","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"tau_c = begincases 1  langle operatornameGradF (x_k) \noperatornameHessF (η_k)_ x_krangle_x_k leq 0  \noperatornamemin(fracoperatornamenorm(operatornameGradF (x_k))^3\nDelta langle operatornameGradF (x_k) \noperatornameHessF (η_k)_ x_krangle_x_k 1)   textotherwise\nendcases","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To check the model decrease one compares m_x_k(η_c^* ) = F(x_k) + langle η_c^*  operatornameGradF (x_k)rangle_x_k + frac12langle η_c^*  operatornameHessF (η_c^* )_ x_krangle_x_k with m_x_k(η^* ) = F(x_k) + langle η^*  operatornameGradF (x_k)rangle_x_k + frac12langle η^*  operatornameHessF (η^* )_ x_krangle_x_k. If m_x_k(η_c^* )  m_x_k(η^* ) then is m_x_k(η_c^* ) the better choice.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 4: operatornameRetr_x_k() denotes the retraction, a mapping operatornameRetr_x_kT_x_kmathcalM rightarrow mathcalM wich approximates the exponential map. In some cases it is cheaper to use this instead of the exponential.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 6: One knows that the truncated_conjugate_gradient_descent algorithm stopped for these reasons when the stopping criteria StopWhenCurvatureIsNegative, StopWhenTrustRegionIsExceeded are activated.","category":"page"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"To step number 7: The last step is to decide if the new point x^*  is accepted.","category":"page"},{"location":"solvers/trust_regions.html#Interface-1","page":"Trust-Regions Solver","title":"Interface","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"trust_regions\ntrust_regions!","category":"page"},{"location":"solvers/trust_regions.html#Manopt.trust_regions","page":"Trust-Regions Solver","title":"Manopt.trust_regions","text":"trust_regions(M, F, gradF, hessF, x)\n\nevaluate the Riemannian trust-regions solver for optimization on manifolds. It will attempt to minimize the cost function F on the Manifold M. If no Hessian H is provided, a standard approximation of the Hessian based on the gradient gradF will be computed. For solving the the inner trust-region subproblem of finding an update-vector, it uses the Steihaug-Toint truncated conjugate-gradient method. For a description of the algorithm and more details see\n\nP.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: 10.1007/s10208-005-0179-9\nA. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: 10.1137/1.9780898719857\n\nInput\n\nM – a manifold mathcal M\nF – a cost function F  mathcal M  ℝ to minimize\ngradF- the gradient operatornamegradF  mathcal M  T mathcal M of F\nx – an initial value x    mathcal M\nHessF – the hessian operatornameHessF(x) T_xmathcal M  T_xmathcal M, X  operatonameHessF(x)X = _ξoperatornamegradf(x)\n\nOptional\n\nevaluation – (AllocatingEvaluation) specify whether the gradient and hessian work by  allocation (default) or MutatingEvaluation in place\nmax_trust_region_radius – the maximum trust-region radius\npreconditioner – a preconditioner (a symmetric, positive definite operator that should approximate the inverse of the Hessian)\nrandomize – set to true if the trust-region solve is to be initiated with a random tangent vector. If set to true, no preconditioner will be used. This option is set to true in some scenarios to escape saddle points, but is otherwise seldom activated.\nproject_vector! : (copyto!) specify a projection operation for tangent vectors within the TCG   for numerical stability. A function (M, Y, p, X) -> ... working in place of Y.   per default, no projection is perfomed, set it to project! to activate projection.\nretraction – (default_retraction_method(M)) approximation of the exponential map\nstopping_criterion – (StopWhenAny(StopAfterIteration(1000), StopWhenGradientNormLess(10^(-6))) a functor inheriting from StoppingCriterion indicating when to stop.\ntrust_region_radius - the initial trust-region radius\nρ_prime – Accept/reject threshold: if ρ (the performance ratio for the iterate) is at least ρ', the outer iteration is accepted. Otherwise, it is rejected. In case it is rejected, the trust-region radius will have been decreased. To ensure this, ρ' >= 0 must be strictly smaller than 1/4. If ρ_prime is negative, the algorithm is not guaranteed to produce monotonically decreasing cost values. It is strongly recommended to set ρ' > 0, to aid convergence.\nρ_regularization – Close to convergence, evaluating the performance ratio ρ is numerically challenging. Meanwhile, close to convergence, the quadratic model should be a good fit and the steps should be accepted. Regularization lets ρ go to 1 as the model decrease and the actual decrease go to zero. Set this option to zero to disable regularization (not recommended). When this is not zero, it may happen that the iterates produced are not monotonically improving the cost when very close to convergence. This is because the corrected cost improvement could change sign if it is negative but very small.\nreturn_options – (false) – if activated, the extended result, i.e. the complete Options are returned. This can be used to access recorded values. If set to false (default) just the optimal value x_opt is returned\n\nOutput\n\nx – the last reached point on the manifold\n\nsee also\n\ntruncated_conjugate_gradient_descent\n\n\n\n\n\n","category":"function"},{"location":"solvers/trust_regions.html#Manopt.trust_regions!","page":"Trust-Regions Solver","title":"Manopt.trust_regions!","text":"trust_regions!(M, F, gradF, hessF, x; kwargs...)\n\nevaluate the Riemannian trust-regions solver for optimization on manifolds in place of x.\n\nInput\n\nM – a manifold mathcal M\nF – a cost function F mathcal M  ℝ to minimize\ngradF- the gradient operatornamegradF mathcal M  T mathcal M of F\nx – an initial value x    mathcal M\nH – the hessian H( mathcal M x ξ) of F\n\nfor more details and all options, see trust_regions\n\n\n\n\n\n","category":"function"},{"location":"solvers/trust_regions.html#Options-1","page":"Trust-Regions Solver","title":"Options","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"AbstractHessianOptions\nTrustRegionsOptions","category":"page"},{"location":"solvers/trust_regions.html#Manopt.AbstractHessianOptions","page":"Trust-Regions Solver","title":"Manopt.AbstractHessianOptions","text":"AbstractHessianOptions <: Options\n\nAn Options type to represent algorithms that employ the Hessian. These options are assumed to have a field (gradient) to store the current gradient operatornamegradf(x)\n\n\n\n\n\n","category":"type"},{"location":"solvers/trust_regions.html#Manopt.TrustRegionsOptions","page":"Trust-Regions Solver","title":"Manopt.TrustRegionsOptions","text":"TrustRegionsOptions <: AbstractHessianOptions\n\ndescribe the trust-regions solver, with\n\nFields\n\na default value is given in brackets if a parameter can be left out in initialization.\n\nx : a point as starting point\nstop : a function s,r = @(o,iter) returning a stop   indicator and a reason based on an iteration number and the gradient\ntrust_region_radius : the (initial) trust-region radius\nmax_trust_region_radius : the maximum trust-region radius\nrandomize : indicates if the trust-region solve is to be initiated with a       random tangent vector. If set to true, no preconditioner will be       used. This option is set to true in some scenarios to escape saddle       points, but is otherwise seldom activated.\nproject_vector! : (copyto!) specify a projection operation for tangent vectors   for numerical stability. A function (M, Y, p, X) -> ... working in place of Y.   per default, no projection is perfomed, set it to project! to activate projection.\nρ_prime : a lower bound of the performance ratio for the iterate that       decides if the iteration will be accepted or not. If not, the       trust-region radius will have been decreased. To ensure this,       ρ'>= 0 must be strictly smaller than 1/4. If ρ' is negative,       the algorithm is not guaranteed to produce monotonically decreasing       cost values. It is strongly recommended to set ρ' > 0, to aid       convergence.\nρ_regularization : Close to convergence, evaluating the performance ratio ρ       is numerically challenging. Meanwhile, close to convergence, the       quadratic model should be a good fit and the steps should be       accepted. Regularization lets ρ go to 1 as the model decrease and       the actual decrease go to zero. Set this option to zero to disable       regularization (not recommended). When this is not zero, it may happen       that the iterates produced are not monotonically improving the cost       when very close to convergence. This is because the corrected cost       improvement could change sign if it is negative but very small.\n\nConstructor\n\nTrustRegionsOptions(x, stop, delta, delta_bar, uR, rho_prime, rho_reg)\n\nconstruct a trust-regions Option with the fields as above.\n\nSee also\n\ntrust_regions\n\n\n\n\n\n","category":"type"},{"location":"solvers/trust_regions.html#Approximation-of-the-Hessian-1","page":"Trust-Regions Solver","title":"Approximation of the Hessian","text":"","category":"section"},{"location":"solvers/trust_regions.html#","page":"Trust-Regions Solver","title":"Trust-Regions Solver","text":"ApproxHessianFiniteDifference","category":"page"},{"location":"solvers/trust_regions.html#Manopt.ApproxHessianFiniteDifference","page":"Trust-Regions Solver","title":"Manopt.ApproxHessianFiniteDifference","text":"approxHessianFiniteDifference{T, mT, P, G}\n\nA functor to approximate the Hessian by a finite difference of gradient evaluations\n\n\n\n\n\n","category":"type"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/BezierCurves.jl\"","category":"page"},{"location":"tutorials/BezierCurves.html#BezierCurvesTutorial-1","page":"work with Bézier curves","title":"Bezier curves and their acceleration","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"This tutorial illustrates how Bézier curves are generalized to manifolds and how to minimize their acceleration, i.e. how to get a curve that is as straight or as geodesic while fulfilling constraints","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"This example also illustrates how to apply the minimization on the corresponding PowerManifold manifold using a gradient_descent with ArmijoLinesearch.","category":"page"},{"location":"tutorials/BezierCurves.html#Table-of-contents-1","page":"work with Bézier curves","title":"Table of contents","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"Setup\nde Casteljau algorithm on manifolds\nComposite Bézire curves\nMinimizing the acceleration of a Bézier curve\nLiterature","category":"page"},{"location":"tutorials/BezierCurves.html#SetupTB-1","page":"work with Bézier curves","title":"Setup","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"We first initialize the necessary packages","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"using Manopt, Manifolds","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"using Colors\nblack = RGBA{Float64}(colorant\"#000000\")\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\")\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\")\nTolVibrantMagenta = RGBA{Float64}(colorant\"#EE3377\")\nTolVibrantCyan = RGBA{Float64}(colorant\"#33BBEE\")\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\")\ngeo_pts = collect(range(0.0, 1.0; length=101)) #hide\nbezier_pts = collect(range(0.0, 3.0; length=201)) #hide\ncamera_position = (-1.0, -0.7, 0.3) #hide\nnothing #hide","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"Then we load our data, see artificial_S2_composite_bezier_curve, a composite Bezier curve consisting of 3 segments on the Sphere","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"B = artificial_S2_composite_bezier_curve();\nnothing #hide","category":"page"},{"location":"tutorials/BezierCurves.html#Casteljau-1","page":"work with Bézier curves","title":"De Casteljau algorithm on manifolds","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"This curve can be evaluated using de Casteljau's algorithm[Casteljau1959][Casteljau1963] named after Paul de Casteljau(*1930). To simplify the idea and understand this algorithm, we will first only look at the points of the first segment","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"M = Sphere(2)\nb = B[2].pts","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"On Euclidean spaces Bézier curves of these n=4 so called control points like this segment yield polynomials of degree 3. The resulting curve gamma 01  ℝ^m is called Bezier curve or Bézier spline and is named after Piérre Bezier (1910–1999). They can be evaluated by the de Casteljau algorithm by evaluating line segments between points. While it is not easy to evaluate polynomials on a manifold, evaluating line segments generalizes to the evaluation of shortest_geodesics","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"We will illustrate this using these points b=(b_1b_2b_3b_4) on the Sphere mathbb S^2. Let's evaliuate this at the point t=frac1401. We first compute","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"t = 0.66\npts1 = shortest_geodesic.(Ref(M), b[1:3], b[2:4], Ref(t))","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"We obtain 3 points on the geodesics connecting the control points. Repeating this again twice","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"pts2 = shortest_geodesic.(Ref(M), pts1[1:2], pts1[2:3], Ref(t))\np = shortest_geodesic(M, pts2[1], pts2[2], t)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"we obtain the point on the Bézier curve c(t). This procedure is illustrated in the following image:","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Illustration of de Casteljau's algorithm on the Sphere.)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"From the control points (blue) and their geodesics, ont evaluation per geodesic yields three interims points (cyan), their two successive geodeics another two points (teal) and at its geodesic at t=066 we obtain the point on the curve.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"In Manopt.jl, to evaluate a Bézier curve knowing its BezierSegment, use de_casteljau.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"There are a few nice observations to make, that hold also for these Bézier curves on manifolds:","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"The curve starts in the first controlpoint b_0 and ends in the last controlpoint b_3\nThe tangent vector to the curve at the start dot c(0) is equal to log_b_0b_1 = dotgamma_b_0b_0(0), where gamma_ab denotes the shortest geodesic.\nThe tangent vector to the curve at the end dot c(1) is equal to -log_b_3b_2 = -dotgamma_b_3b_2(0) = dotgamma_b_2b_3(1).\nthe curve is differentiable.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"For more details on these properties, see for example [PopielNoakes2007].","category":"page"},{"location":"tutorials/BezierCurves.html#CompositeBezier-1","page":"work with Bézier curves","title":"Composite Bézier curves","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"With the properties of a single Bézier curve, also called Bézier segment, we can “stitch” curves together. Let a_0a_n and b_0b_m be two sets of controlpoints for the Bézier segments c(t) and d(t), respectively. We define the composite Bézier curve by B(t) = begincases c(t)  text if  0leq t  1  d(t-1)  text if  1leq t leq 2endcases where t02. This can of course be generalised straight forward to more than two cases. With the properties from the previous section we can now state that","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"the curve B(t) is continuous if c(1)=d(0) or in other words a_n=b_0\nthe curve B(t) is differentiable if additionally dot c(1)=dot d(0) or in other words -log_a_na_n-1 = log_b_0b_1. This is equivalent to a_n=b_0 = gamma_a_n-1b_1(tfrac12).","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"One nice interpretation of the last characterization is, that the tangents log_a_na_n-1 and log_b_0b_1 point into opposite directions. For a continuous curve, the first point of every segment (except for the first segment) can be ommitted, for a differentiable curve the first two points (except for the first segment) can be ommitted. You can reduce storage by calling get_bezier_points, though for econstruciton with get_bezier_segments you also need get_bezier_degrees. The reduced storage is represented as an array of points, i.e. an element of the corresponding PowerManifold.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"For the three segment example from the beginning this looks as follows[1]","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Illustration of a differentiable composite Bézier curve with 3 segments.)","category":"page"},{"location":"tutorials/BezierCurves.html#MinAccBezier-1","page":"work with Bézier curves","title":"Minimizing the acceleration of a composite Bézier curve","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"The motivation to minimize the acceleration of the composite Bézier curve is, that the curve should get “straighter” or more geodesic like. If we discretize the curve B(t) with its control points denoted by b_ij for the jth note in the ith segment, the discretized model for equispaced t_i, i=0N in the domain of B reads[BergmannGousenbourger2018]","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"A(b) eqqsum_i=1^N-1fracmathrmd^2_2 bigl B(t_i-1) B(t_i) B(t_i+1) bigrDelta_t^3","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"where mathrmd_2 denotes the second order finite difference using the mid point approach, see costTV2[BacakBergmannSteidlWeinmann2016],","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"d_2(xyz) = min_c  mathcal C_xz d_mathcal M(cy)qquad xyzmathcal M","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"Another model is based on logarithmic maps, see [BoumalAbsil2011], but that is not considered here. An advantage of the model considered here is, that it only consist of the evaluation of geodesics. This yields a gradient of A(b) with respect to b adjoint_Jacobi_fields. The following image shows the negative gradient (scaled)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Illustration of the gradient of the acceleration with respect to the control points.)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"In the following we consider two cases: Interpolation, which fixes the junction and end points of B(t) and approximation, where a weight and a dataterm are additionally introduced.","category":"page"},{"location":"tutorials/BezierCurves.html#Interpolation-1","page":"work with Bézier curves","title":"Interpolation","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"For interpolation, the junction points are fixed and their gradient entries are hence set to zero. After transferring to the already mentioned PowerManifold, we can then perform a gradient_descent as follows","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"curve_samples = collect(range(0.0, 3.0; length=151)) #exactness of approximating d^2\npB = get_bezier_points(M, B, :differentiable)\nN = PowerManifold(M, NestedPowerRepresentation(), length(pB))\nfunction F(M, pB)\n    return cost_acceleration_bezier(\n        M.manifold, pB, get_bezier_degrees(M.manifold, B), curve_samples\n    )\nend\nfunction gradF(M, pB)\n    return grad_acceleration_bezier(\n        M.manifold, pB, get_bezier_degrees(M.manifold, B), curve_samples\n    )\nend\nx0 = pB\npB_opt_ip = gradient_descent(\n    N,\n    F,\n    gradF,\n    x0;\n    stepsize=ArmijoLinesearch(1.0, ExponentialRetraction(), 0.5, 0.0001),\n    stopping_criterion=StopWhenChangeLess(5 * 10.0^(-7)),\n    debug=[\n        :Iteration,\n        \" | \",\n        :Cost,\n        \" | \",\n        DebugGradientNorm(),\n        \" | \",\n        DebugStepsize(),\n        \" | \",\n        :Change,\n        \"\\n\",\n        :Stop,\n        10,\n    ],\n)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"and the result looks like","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Interpolation Min Acc)","category":"page"},{"location":"tutorials/BezierCurves.html#Approximation-1","page":"work with Bézier curves","title":"Approximation","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"Similarly if we introduce the junction points as data fixed given d_i and set (for simplicity) p_i=b_i0 and p_n+1=b_n4 and set λ=3 in","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"fracλ2sum_k=0^3 d_mathcal M(d_ip_i)^2 + A(b)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"then λ models how important closeness to the data d_i is.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"λ = 3.0\nd = get_bezier_junctions(M, B)\nF(pB) = cost_L2_acceleration_bezier(M, pB, get_bezier_degrees(M, B), curve_samples, λ, d)\nfunction gradF(M, pB)\n    return grad_L2_acceleration_bezier(\n        M.manifold, pB, get_bezier_degrees(M.manifold, B), curve_samples, λ, d\n    )\nend\nx0 = pB\npB_opt_appr = gradient_descent(\n    N,\n    F,\n    gradF,\n    x0;\n    stepsize=ArmijoLinesearch(1.0, ExponentialRetraction(), 0.5, 0.001),\n    stopping_criterion=StopWhenChangeLess(10.0^(-5)),\n    debug=[\n        :Iteration,\n        \" | \",\n        :Cost,\n        \" | \",\n        DebugGradientNorm(),\n        \" | \",\n        DebugStepsize(),\n        \" | \",\n        :Change,\n        \"\\n\",\n        :Stop,\n        50,\n    ],\n)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"and the result looks like","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Approximation min Acc)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"The role of λ can be interpreted as follows: for large values of λ, the minimizer, i.e. the resulting curve, is closer to the original Bézier junction points. For small λ the resting curve is closer to a geodesic and the control points are closer to the curve. For λ=0 any (not necessarily shortest) geodesic is a solution and the problem is ill-posed. To illustrate the effect of λ, the following image contains 1000 runs for λ=10 in dark currant to λ=001 in bright yellow.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Approximation min Acc)","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"The effect of the data term can also be seen in the following video, which starts a little slow and takes about 40 seconds.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"(Image: Video of the effect of lambda, the weight of the dataterm)","category":"page"},{"location":"tutorials/BezierCurves.html#LiteratureBT-1","page":"work with Bézier curves","title":"Literature","text":"","category":"section"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[BacakBergmannSteidlWeinmann2016]: Bačák, M., Bergmann, R., Steidl, G. and Weinmann, A.: A second order nonsmooth variational model for restoring manifold-valued images, SIAM Journal on Scientific Computations, Volume 38, Number 1, pp. A567–597, doi: 10.1137/15M101988X, arXiv: 1506.02409","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics, 2018. doi: 10.3389/fams.2018.00059, arXiv: 1807.10090","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[BoumalAbsil2011]: Boumal, N. and Absil, P.-A.: A discrete regression method on manifolds and its application to data on SO(n). In: IFAC Proceedings Volumes (IFAC-PapersOnline). Vol. 18. Milano (2011). p. 2284–89. doi: 10.3182/20110828-6-IT-1002.00542, web: www","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[Casteljau1959]: de Casteljau, P.: Outillage methodes calcul, Enveloppe Soleau 40.040 (1959), Institute National de la Propriété Industrielle, Paris.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[Casteljau1963]: de Casteljau, P.: Courbes et surfaces à pôles, Microfiche P 4147-1, André Citroën Automobile SA, Paris, (1963).","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[PopielNoakes2007]: Popiel, T. and Noakes, L.: Bézier curves and C^2 interpolation in Riemannian manifolds. Journal of Approximation Theory (2007), 148(2), pp. 111–127.- doi: 10.1016/j.jat.2007.03.002.","category":"page"},{"location":"tutorials/BezierCurves.html#","page":"work with Bézier curves","title":"work with Bézier curves","text":"[1]: The images are rendered using asymptote_export_S2_signals. For code examples, see Get started: Optimize!.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"EditURL = \"https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/GradientOfSecondOrderDifference.jl\"","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#secondOrderDifferenceGrad-1","page":"see the gradient of d_2","title":"Illustration of the Gradient of a Second Order Difference","text":"","category":"section"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"This example explains how to compute the gradient of the second order difference mid point model using adjoint_Jacobi_fields.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"This example also illustrates the PowerManifold manifold as well as ArmijoLinesearch.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"We first initialize the manifold","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"using Manopt, Manifolds","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"and we define some colors from Paul Tol","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"using Colors\nblack = RGBA{Float64}(colorant\"#000000\")\nTolVibrantBlue = RGBA{Float64}(colorant\"#0077BB\") # points\nTolVibrantOrange = RGBA{Float64}(colorant\"#EE7733\") # results\nTolVibrantCyan = RGBA{Float64}(colorant\"#33BBEE\") # vectors\nTolVibrantTeal = RGBA{Float64}(colorant\"#009988\") # geo\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"Assume we have two points xy on the equator of the Sphere mathcal M = mathbb S^2 and a point y near the north pole","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"M = Sphere(2)\np = [1.0, 0.0, 0.0]\nq = [0.0, 1.0, 0.0]\nc = mid_point(M, p, q)\nr = shortest_geodesic(M, [0.0, 0.0, 1.0], c, 0.1)\n[c, r]","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"Now the second order absolute difference can be stated as (see [Bačák, Bergmann, Steidl, Weinmann, 2016])","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"d_2(xyz) = min_c  mathcal C_xz d_mathcal M(cy)qquad xyzmathcal M","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"where mathcal C_xz is the set of all mid points g(frac12xz), where g is a (not necessarily minimizing) geodesic connecting x and z.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"For illustration we further define the point opposite of","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"c2 = -c","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"and draw the geodesic connecting y and the nearest mid point c, namely","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"T = [0:0.1:1.0...]\ngeoPts_yc = shortest_geodesic(M, r, c, T)\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"looks as follows using the asymptote_export_S2_signals export","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"asymptote_export_S2_signals(\"secondOrderData.asy\";\n    render = asyResolution,\n    curves = [ geoPts_yc ],\n    points = [ [x,y,z], [c,c2] ],\n    colors=Dict(:curves => [TolVibrantTeal], :points => [black, TolVibrantBlue]),\n    dot_size = 3.5, line_width = 0.75, camera_position = (1.2,1.,.5)\n)\nrender_asymptote(\"SecondOrderData.asy\"; render=2)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Image: Three points ``p,r,q`` and the midpoint ``c=c(p,q)`` (blue))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"Since we moved r 10% along the geodesic from the north pole to c, the distance to c is frac9pi20approx 14137, and this is also what","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"costTV2(M, (p, r, q))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"returns, see costTV2 for reference. But also its gradient can be easily computed since it is just a distance with respect to y and a concatenation of a geodesic, where the start or end point is the argument, respectively, with a distance. Hence the adjoint differentials adjoint_differential_geodesic_startpoint and adjoint_differential_geodesic_endpoint can be employed, see grad_TV2 for details. we obtain","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Xp, Xr, Xq) = grad_TV2(M, (p, r, q))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"When we aim to minimize this, we look at the negative gradient, i.e. we can draw this as","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"asymptote_export_S2_signals(\"SecondOrderGradient.asy\";\n   points = [ [x,y,z], [c,c2] ],\n   colors=Dict(:tvectors => [TolVibrantCyan], :points => [black, TolVibrantBlue]),\n   dot_size = 3.5, line_width = 0.75, camera_position = (1.2,1.,.5)\n)\nrender_asymptote(\"SecondOrderGradient.asy\"; render=2)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Image: Three points ``x,y,z`` and the negative gradient of the second order absolute difference)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"If we now perform a gradient step, we obtain the three points","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"pn, rn, qn = exp.(Ref(M), [p, r, q], [-Xp, -Xr, -Xq])","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"as well we the new mid point","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"cn = mid_point(M, pn, qn)\ngeoPts_yncn = shortest_geodesic(M, rn, cn, T)\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"and obtain the new situation","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"asymptote_export_S2_signals(\"SecondOrderMin1.asy\";\n    points = [ [x,y,z], [c,c2,cn], [xn,yn,zn] ],\n    curves = [ geoPts_yncn ] ,\n    tangent_vectors = [Tuple.([ [p, -Xp], [r, -Xr], [q, -Xq] ])],\n    colors=Dict(:tvectors => [TolVibrantCyan],\n        :points => [black, TolVibrantBlue, TolVibrantOrange],\n        :curves => [TolVibrantTeal]\n    ),\n    dot_size = 3.5, line_width = 0.75, camera_position = (1.2,1.,.5)\n)\nrender_asymptote(\"SecondOrderMin1.asy\"; render=2)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Image: A gradient Step)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"One can see, that this step slightly “overshoots”, i.e. r is now even below c. and the cost function is still at","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"costTV2(M, (pn, rn, qn))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"But we can also search for the best step size using linesearch_backtrack on the PowerManifold manifold mathcal N = mathcal M^3 = (mathbb S^2)^3","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"x = [p, r, q]\nN = PowerManifold(M, NestedPowerRepresentation(), 3)\ns = linesearch_backtrack(N, x -> costTV2(N, x), x, grad_TV2(N, x), 1.0, 0.96, 0.999)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"and for the new points","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"pm, rm, qm = exp.(Ref(M), [p, r, q], s * [-Xp, -Xr, -Xq])\ncm = mid_point(M, pm, qm)\ngeoPts_xmzm = shortest_geodesic(M, pm, qm, T)\nnothing #hide","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"we obtain again with","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"asymptote_export_S2_signals(\"SecondOrderMin2.asy\";\n    points = [ [x,y,z], [c,c2,cm], [xm,ym,zm] ],\n    curves = [ geoPts_xmzm ] ,\n    tangent_vectors = [Tuple.( [-ξx, -ξy, -ξz], [x, y, z] )],\n    colors=Dict(:tvectors => [TolVibrantCyan],\n                :points => [black, TolVibrantBlue, TolVibrantOrange],\n                :curves => [TolVibrantTeal]\n                ),\n    dot_size = 3.5, line_width = 0.75, camera_position = (1.2,1.,.5)\n)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"(Image: A gradient Step)","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"Here, the cost function yields","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"costTV2(M, (pm, rm, qm))","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"which is nearly zero, as one can also see, since the new center c and r are quite close.","category":"page"},{"location":"tutorials/GradientOfSecondOrderDifference.html#Literature-1","page":"see the gradient of d_2","title":"Literature","text":"","category":"section"},{"location":"tutorials/GradientOfSecondOrderDifference.html#","page":"see the gradient of d_2","title":"see the gradient of d_2","text":"<ul>\n<li id=\"BačákBergmannSteidlWeinmann2016\">[<a>Bačák, Bergmann, Steidl, Weinmann, 2016</a>]\n  Bačák, M; Bergmann, R.; Steidl, G; Weinmann, A.: <emph>A second order nonsmooth\n  variational model for restoring manifold-valued images.</emph>,\n  SIAM Journal on Scientific Computations, Volume 38, Number 1, pp. A567–597,\n  doi: <a href=\"https://doi.org/10.1137/15M101988X\">10.1137/15M101988X</a></li>\n</ul>","category":"page"},{"location":"index.html#Welcome-to-Manopt.jl-1","page":"Home","title":"Welcome to Manopt.jl","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"CurrentModule = Manopt","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"Manopt.Manopt","category":"page"},{"location":"index.html#Manopt.Manopt","page":"Home","title":"Manopt.Manopt","text":"Manopt.jl – Optimization on Manifolds in Julia.\n\n\n\n\n\n","category":"module"},{"location":"index.html#","page":"Home","title":"Home","text":"For a function fmathcal M  ℝ defined on a Riemannian manifold mathcal M we aim to solve","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"operatorname*argmin_x  mathcal M f(x)","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"or in other words: find the point x on the manifold, where f reaches its minimal function value.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"Manopt.jl provides a framework for optimization on manifolds. Based on Manopt and MVIRT, both implemented in Matlab, this toolbox provide an easy access to optimization methods on manifolds for Julia, including example data and visualization methods.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"If you want to delve right into Manopt.jl check out the Get started: Optimize! tutorial.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"Manopt.jl makes it easy to use an algorithm for your favorite manifold as well as a manifold for your favorite algorithm. It already provides many manifolds and algorithms, which can easily be enhanced, for example to record certain data or display information throughout iterations.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"If you use Manopt.jlin your work, please cite the following","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"@article{Bergmann2022,\n    Author    = {Ronny Bergmann},\n    Doi       = {10.21105/joss.03866},\n    Journal   = {Journal of Open Source Software},\n    Number    = {70},\n    Pages     = {3866},\n    Publisher = {The Open Journal},\n    Title     = {Manopt.jl: Optimization on Manifolds in {J}ulia},\n    Volume    = {7},\n    Year      = {2022},\n}","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"To refer to a certain version or the source code in general we recommend to cite for example","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"@software{manoptjl-zenodo-mostrecent,\n    Author = {Ronny Bergmann},\n    Copyright = {MIT License},\n    Doi = {10.5281/zenodo.4290905},\n    Publisher = {Zenodo},\n    Title = {Manopt.jl},\n    Year = {2022},\n}","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"for the most recent version or a corresponding version specific DOI, see the list of all versions. Note that both citations are in BibLaTeX format.","category":"page"},{"location":"index.html#Main-Features-1","page":"Home","title":"Main Features","text":"","category":"section"},{"location":"index.html#Functions-on-Manifolds-1","page":"Home","title":"Functions on Manifolds","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"Several functions are available, implemented on an arbitrary manifold, cost functions, differentials, and gradients as well as proximal maps, but also several jacobi Fields and their adjoints.","category":"page"},{"location":"index.html#Optimization-Algorithms-(Solvers)-1","page":"Home","title":"Optimization Algorithms (Solvers)","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"For every optimization algorithm, a solver is implemented based on a Problem that describes the problem to solve and its Options that set up the solver, store interims values. Together they form a plan.","category":"page"},{"location":"index.html#Visualization-1","page":"Home","title":"Visualization","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"To visualize and interpret results, Manopt.jl aims to provide both easy plot functions as well as exports. Furthermore a system to get debug during the iterations of an algorithms as well as record capabilities, i.e. to record a specified tuple of values per iteration, most prominently RecordCost and RecordIterate. Take a look at the Get started: Optimize! tutorial how to easily activate this.","category":"page"},{"location":"index.html#Manifolds-1","page":"Home","title":"Manifolds","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"This project is build upon ManifoldsBase.jl, a generic interface to implement manifolds. Certain functions are extended for specific manifolds from Manifolds.jl, but all other manifolds from that package can be used here, too.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"The notation in the documentation aims to follow the same notation from these packages.","category":"page"},{"location":"index.html#Literature-1","page":"Home","title":"Literature","text":"","category":"section"},{"location":"index.html#","page":"Home","title":"Home","text":"If you want to get started with manifolds, one book is [do Carmo, 1992], and if you want do directly dive into optimization on manifolds, my favourite reference is [Absil, Mahony, Sepulchre, 2008], which is also available online for free.","category":"page"},{"location":"index.html#","page":"Home","title":"Home","text":"<ul>\n<li id=\"AbsilMahonySepulchre2008\">\n    [<a>Absil, Mahony, Sepulchre, 2008</a>]\n    P.-A. Absil, R. Mahony and R. Sepulchre,\n    <emph>Optimization Algorithms on Matrix Manifolds</emph>,\n    Princeton University Press, 2008,\n    doi: <a href=\"https://doi.org/10.1515/9781400830244\">10.1515/9781400830244</a>,\n    <a href=\"http://press.princeton.edu/chapters/absil/\">open access</a>.\n</li>\n<li id=\"doCarmo1992\">\n    [<a>doCarmo, 1992</a>]\n    M. P. do Carmo,\n    <emph>Riemannian Geometry</emph>,\n    Birkhäuser Boston, 1992,\n    ISBN: 0-8176-3490-8.\n</li>\n</ul>","category":"page"},{"location":"functions/gradients.html#GradientFunctions-1","page":"Gradients","title":"Gradients","text":"","category":"section"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"For a function fmathcal Mℝ the Riemannian gradient operatornamegradf(x) at xmathcal M is given by the unique tangent vector fulfilling","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"langle operatornamegradf(x) ξrangle_x = D_xfξquad\nforall ξ  T_xmathcal M","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"where D_xfξ denotes the differential of f at x with respect to the tangent direction (vector) ξ or in other words the directional derivative.","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"This page collects the available gradients.","category":"page"},{"location":"functions/gradients.html#","page":"Gradients","title":"Gradients","text":"Modules = [Manopt]\nPages   = [\"gradients.jl\"]","category":"page"},{"location":"functions/gradients.html#Manopt.forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{𝔽}, Tuple{PowerManifold{𝔽, TM, TSize, TPR}, Any}} where {𝔽, TM, TSize, TPR}","page":"Gradients","title":"Manopt.forward_logs","text":"Y = forward_logs(M,x)\nforward_logs!(M, Y, x)\n\ncompute the forward logs F (generalizing forward differences) occurring, in the power manifold array, the function\n\nF_i(x) = sum_j  mathcal I_i log_x_i x_jquad i    mathcal G\n\nwhere mathcal G is the set of indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i. This can also be done in place of ξ.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOuput\n\nY – resulting tangent vector in T_xmathcal M representing the logs, where mathcal N is thw power manifold with the number of dimensions added to size(x). The computation can be done in place of Y.\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.grad_L2_acceleration_bezier-Union{Tuple{P}, Tuple{AbstractManifold, AbstractVector{P}, AbstractVector{var\"#s53\"} where var\"#s53\"<:Integer, AbstractVector{T} where T, Any, AbstractVector{P}}} where P","page":"Gradients","title":"Manopt.grad_L2_acceleration_bezier","text":"grad_L2_acceleration_bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector,\n    λ,\n    d::AbstractVector{P}\n) where {P}\n\ncompute the gradient of the discretized acceleration of a composite Bézier curve on the Manifold M with respect to its control points B together with a data term that relates the junction points p_i to the data d with a weight λ compared to the acceleration. The curve is evaluated at the points given in pts (elementwise in 0N), where N is the number of segments of the Bézier curve. The summands are grad_distance for the data term and grad_acceleration_bezier for the acceleration with interpolation constrains. Here the get_bezier_junctions are included in the optimization, i.e. setting λ=0 yields the unconstrained acceleration minimization. Note that this is ill-posed, since any Bézier curve identical to a geodesic is a minimizer.\n\nNote that the Beziér-curve is given in reduces form as a point on a PowerManifold, together with the degrees of the segments and assuming a differentiable curve, the segments can internally be reconstructed.\n\nSee also\n\ngrad_acceleration_bezier, cost_L2_acceleration_bezier, cost_acceleration_bezier.\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.grad_TV","page":"Gradients","title":"Manopt.grad_TV","text":"X = grad_TV(M, λ, x[, p=1])\ngrad_TV!(M, X, λ, x[, p=1])\n\nCompute the (sub)gradient partial F of all forward differences occurring, in the power manifold array, i.e. of the function\n\nF(x) = sum_isum_j  mathcal I_i d^p(x_ix_j)\n\nwhere i runs over all indices of the PowerManifold manifold M and mathcal I_i denotes the forward neighbors of i.\n\nInput\n\nM – a PowerManifold manifold\nx – a point.\n\nOuput\n\nX – resulting tangent vector in T_xmathcal M. The computation can also be done in place.\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.grad_TV-Union{Tuple{T}, Tuple{AbstractManifold, Tuple{T, T}}, Tuple{AbstractManifold, Tuple{T, T}, Any}} where T","page":"Gradients","title":"Manopt.grad_TV","text":"X = grad_TV(M, (x,y)[, p=1])\ngrad_TV!(M, X, (x,y)[, p=1])\n\ncompute the (sub) gradient of frac1pd^p_mathcal M(xy) with respect to both x and y (in place of X and Y).\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.grad_TV2","page":"Gradients","title":"Manopt.grad_TV2","text":"Y = grad_TV2(M, q[, p=1])\ngrad_TV2!(M, Y, q[, p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(q_1 q_2 q_3) with respect to all three components of qmathcal M^3, where d_2 denotes the second order absolute difference using the mid point model, i.e. let\n\nmathcal C = bigl c  mathcal M   g(tfrac12q_1q_3) text for some geodesic gbigr\n\ndenote the mid points between q_1 and q_3 on the manifold mathcal M. Then the absolute second order difference is defined as\n\nd_2(q_1q_2q_3) = min_c  mathcal C_q_1q_3 d(c q_2)\n\nWhile the (sub)gradient with respect to q_2 is easy, the other two require the evaluation of an adjoint_Jacobi_field. See Illustration of the Gradient of a Second Order Difference for its derivation.\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.grad_TV2","page":"Gradients","title":"Manopt.grad_TV2","text":"grad_TV2(M::PowerManifold, q[, p=1])\n\ncomputes the (sub) gradient of frac1pd_2^p(q_1q_2q_3) with respect to all q_1q_2q_3 occurring along any array dimension in the point q, where M is the corresponding PowerManifold.\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.grad_acceleration_bezier-Tuple{AbstractManifold, AbstractVector{T} where T, AbstractVector{var\"#s53\"} where var\"#s53\"<:Integer, AbstractVector{T} where T}","page":"Gradients","title":"Manopt.grad_acceleration_bezier","text":"grad_acceleration_bezier(\n    M::AbstractManifold,\n    B::AbstractVector,\n    degrees::AbstractVector{<:Integer}\n    T::AbstractVector\n)\n\ncompute the gradient of the discretized acceleration of a (composite) Bézier curve c_B(t) on the Manifold M with respect to its control points B given as a point on the PowerManifold assuming C1 conditions and known degrees. The curve is evaluated at the points given in T (elementwise in 0N, where N is the number of segments of the Bézier curve). The get_bezier_junctions are fixed for this gradient (interpolation constraint). For the unconstrained gradient, see grad_L2_acceleration_bezier and set λ=0 therein. This gradient is computed using adjoint_Jacobi_fields. For details, see [BergmannGousenbourger2018]. See de_casteljau for more details on the curve.\n\nSee also\n\ncost_acceleration_bezier,  grad_L2_acceleration_bezier, cost_L2_acceleration_bezier.\n\n[BergmannGousenbourger2018]: Bergmann, R. and Gousenbourger, P.-Y.: A variational model for data fitting on manifolds by minimizing the acceleration of a Bézier curve. Frontiers in Applied Mathematics and Statistics (2018). doi 10.3389/fams.2018.00059, arXiv: 1807.10090\n\n\n\n\n\n","category":"method"},{"location":"functions/gradients.html#Manopt.grad_distance","page":"Gradients","title":"Manopt.grad_distance","text":"grad_distance(M,y,x[, p=2])\ngrad_distance!(M,X,y,x[, p=2])\n\ncompute the (sub)gradient of the distance (squared), in place of X.\n\nf(x) = frac1p d^p_mathcal M(xy)\n\nto a fixed point y on the manifold M and p is an integer. The gradient reads\n\n  operatornamegradf(x) = -d_mathcal M^p-2(xy)log_xy\n\nfor pneq 1 or xneq  y. Note that for the remaining case p=1, x=y the function is not differentiable. In this case, the function returns the corresponding zero tangent vector, since this is an element of the subdifferential.\n\nOptional\n\np – (2) the exponent of the distance,  i.e. the default is the squared distance\n\n\n\n\n\n","category":"function"},{"location":"functions/gradients.html#Manopt.grad_intrinsic_infimal_convolution_TV12-Tuple{AbstractManifold, Any, Any, Any, Any, Any}","page":"Gradients","title":"Manopt.grad_intrinsic_infimal_convolution_TV12","text":"grad_u,⁠ grad_v = grad_intrinsic_infimal_convolution_TV12(M, f, u, v, α, β)\n\ncompute (sub)gradient of the intrinsic infimal convolution model using the mid point model of second order differences, see costTV2, i.e. for some f  mathcal M on a PowerManifold manifold mathcal M this function computes the (sub)gradient of\n\nE(uv) =\nfrac12sum_i  mathcal G d_mathcal M(g(frac12v_iw_i)f_i)\n+ alpha\nbigl(\nβmathrmTV(v) + (1-β)mathrmTV_2(w)\nbigr)\n\nwhere both total variations refer to the intrinsic ones, grad_TV and grad_TV2, respectively.\n\n\n\n\n\n","category":"method"},{"location":"notation.html#Notation-1","page":"Notation","title":"Notation","text":"","category":"section"},{"location":"notation.html#","page":"Notation","title":"Notation","text":"In this package, we follow the notation introduced in Manifolds.jl – Notation","category":"page"},{"location":"notation.html#","page":"Notation","title":"Notation","text":"with the following additional or slightly changed notation","category":"page"},{"location":"notation.html#","page":"Notation","title":"Notation","text":"Symbol Description Also used Comment\n The Levi-Cevita connection  \noperatornamegradf The Riemannian gradient f due to possible confusion with the connection, we try to avoid f\noperatornameHessf The Riemannian Hessian  \npq points on a manifold  when definiting functions\nx_k y_k points on a manifold  iterates of an algorithm","category":"page"}]
}
