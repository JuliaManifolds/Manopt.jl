<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Use AD in Manopt ¬∑ Manopt.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../Optimize!/">Get started: Optimize!</a></li><li class="is-active"><a class="tocitem" href>Use AD in Manopt</a><ul class="internal"><li><a class="tocitem" href="#.-Conversion-of-an-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-an-(not-necessarily-isometrically)-embedded-Manifold"><span>2. Conversion of an Euclidean Gradient in the Embedding to a Riemannian Gradient of an (not necessarily isometrically) embedded Manifold</span></a></li></ul></li><li><a class="tocitem" href="../HowToRecord/">Record values</a></li><li><a class="tocitem" href="../GeodesicRegression/">Do Geodesic regression</a></li><li><a class="tocitem" href="../Bezier/">Use Bezier Curves</a></li><li><a class="tocitem" href="../SecondOrderDifference/">Compute a second order difference</a></li><li><a class="tocitem" href="../StochasticGradientDescent/">Do stochastic gradient descent</a></li><li><a class="tocitem" href="../Benchmark/">speed up! using <code>gradF!</code></a></li><li><a class="tocitem" href="../JacobiFields/">Illustrate Jacobi Fields</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../../solvers/">Introduction</a></li><li><a class="tocitem" href="../../solvers/alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../../solvers/conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../../solvers/cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../../solvers/DouglasRachford/">Douglas‚ÄìRachford</a></li><li><a class="tocitem" href="../../solvers/FrankWolfe/">Frank-Wolfe</a></li><li><a class="tocitem" href="../../solvers/gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/NelderMead/">Nelder‚ÄìMead</a></li><li><a class="tocitem" href="../../solvers/particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../../solvers/primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../../solvers/quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../../solvers/stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../../solvers/truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../../solvers/trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/robustPCA/">Robust PCA</a></li><li><a class="tocitem" href="../../examples/smallestEigenvalue/">Rayleigh quotient</a></li><li><a class="tocitem" href="../../examples/FrankWolfeSPDMean/">Frank Wolfe for Riemannian Center of Mass</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/options/">Options</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../../functions/">Introduction</a></li><li><a class="tocitem" href="../../functions/bezier/">B√©zier curves</a></li><li><a class="tocitem" href="../../functions/costs/">Cost functions</a></li><li><a class="tocitem" href="../../functions/differentials/">Differentials</a></li><li><a class="tocitem" href="../../functions/adjointdifferentials/">Adjoint Differentials</a></li><li><a class="tocitem" href="../../functions/gradients/">Gradients</a></li><li><a class="tocitem" href="../../functions/Jacobi_fields/">Jacobi Fields</a></li><li><a class="tocitem" href="../../functions/proximal_maps/">Proximal Maps</a></li><li><a class="tocitem" href="../../functions/manifold/">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/data/">Data</a></li><li><a class="tocitem" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../list/">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">How to...</a></li><li class="is-active"><a href>Use AD in Manopt</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Use AD in Manopt</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/tutorials/AutomaticDifferentiation.jl" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><style>
    table {
        display: table !important;
        margin: 2rem auto !important;
        border-top: 2pt solid rgba(0,0,0,0.2);
        border-bottom: 2pt solid rgba(0,0,0,0.2);
    }

    pre, div {
        margin-top: 1.4rem !important;
        margin-bottom: 1.4rem !important;
    }

    .code-output {
        padding: 0.7rem 0.5rem !important;
    }

    .admonition-body {
        padding: 0em 1.25em !important;
    }
</style>

<!-- PlutoStaticHTML.Begin -->
<!--
    # This information is used for caching.
    [PlutoStaticHTML.State]
    input_sha = "34c46022203c42e6fc2b0df073c20523454fc93aa1d2306c84a08686923669d0"
    julia_version = "1.8.2"
-->

<!DOCTYPE ><HTML><head></head><body><div class="markdown"><h1>Using (Euclidean) AD in Manopt.jl</h1></div></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>Since <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/">Manifolds.jl</a> 0.7 the support of automatic differentiation support has been extended.</p><p>This tutorial explains how to use Euclidean tools to derive a gradient for a real-valued function <span class="tex">$F\colon \mathcal M ‚Üí ‚Ñù$</span>. We will consider two methods: an intrinsic variant and a variant employing the embedding. These gradients can then be used within any gradient based optimisation algorithm in <a href="https://manoptjl.org">Manopt.jl</a>.</p><p>While by default we use <a href="https://juliadiff.org/FiniteDifferences.jl/latest/">FiniteDifferences.jl</a>, you can also use <a href="https://github.com/JuliaDiff/FiniteDiff.jl">FiniteDiff.jl</a>, <a href="https://juliadiff.org/ForwardDiff.jl/stable/">ForwardDiff.jl</a>, <a href="https://juliadiff.org/ReverseDiff.jl/">ReverseDiff.jl</a>, or  <a href="https://fluxml.ai/Zygote.jl/">Zygote.jl</a>.</p></div></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>In this Notebook we will take a look at a few possibilities to approximate or derive the gradient of a function <span class="tex">$f:\mathcal M \to ‚Ñù$</span> on a Riemannian manifold, without computing it yourself. There is mainly two different philosophies:</p><ol><li><p>Working <em>instrinsically</em>, i.e. stay on the manifold and in the tangent spaces. Here, we will consider approximating the gradient by forward differences.</p></li><li><p>Working in an embedding ‚Äì¬†there we can use all tools from functions on Euclidean spaces ‚Äì¬†finite differences or automatic differenciation ‚Äì and then compute the corresponding Riemannian gradient from there.</p></li></ol><p>Let's first load all packages we need.</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>using Manifolds, Manopt, Random, LinearAlgebra</code></pre>
<!DOCTYPE ><HTML><head></head><body></body></HTML>

<pre class='language-julia'><code class='language-julia'>using FiniteDifferences</code></pre>
<!DOCTYPE ><HTML><head></head><body></body></HTML>


<!DOCTYPE ><HTML><head></head><body>```
## 1. (Intrinsic) Forward Differences<p>@raw html &lt;div class=&quot;markdown&quot;&gt; &lt;p&gt;A first idea is to generalise (multivariate) finite differences to Riemannian manifolds. Let &lt;span class=&quot;tex&quot;&gt;<span>$X_1,\ldots,X_d ‚àà T_p\mathcal M$</span>&lt;/span&gt; denote an orthonormal basis of the tangent space &lt;span class=&quot;tex&quot;&gt;<span>$T_p\mathcal M$</span>&lt;/span&gt; at the point &lt;span class=&quot;tex&quot;&gt;<span>$p‚àà\mathcal M$</span>&lt;/span&gt; on the Riemannian manifold.&lt;/p&gt;&lt;p&gt;We can generalise the notion of a directional derivative, i.e. for the ‚Äúdirection‚Äù &lt;span class=&quot;tex&quot;&gt;<span>$Y‚ààT_p\mathcal M$</span>&lt;/span&gt; let &lt;span class=&quot;tex&quot;&gt;<span>$c\colon [-Œµ,Œµ]$</span>&lt;/span&gt;, &lt;span class=&quot;tex&quot;&gt;<span>$Œµ&amp;gt;0$</span>&lt;/span&gt;, be a curve with &lt;span class=&quot;tex&quot;&gt;<span>$c(0) = p$</span>&lt;/span&gt;, &lt;span class=&quot;tex&quot;&gt;<span>$\dot c(0) = Y$</span>&lt;/span&gt; and we obtain&lt;/p&gt;&lt;p class=&quot;tex&quot;&gt;$	Df(p)[Y] = \frac{\mathrm{d}}{\mathrm{d}t} f(c(t)) = \lim<em>{h \to 0} \frac{1}{h}(f(\exp</em>p(hY))-f(p))<span>$&lt;/p&gt;&lt;p&gt;We can approximate &lt;span class=&quot;tex&quot;&gt;$Df(p)[X]$&lt;/span&gt; by a finite difference scheme for an &lt;span class=&quot;tex&quot;&gt;$h&amp;gt;0$&lt;/span&gt; as&lt;/p&gt;&lt;p class=&quot;tex&quot;&gt;$</span>DF(p)[Y] ‚âà G<em>h(Y) := \frac{1}{h}(f(\exp</em>p(hY))-f(p))<span>$&lt;/p&gt;&lt;p&gt;Furthermore the gradient &lt;span class=&quot;tex&quot;&gt;$\operatorname{grad}f$&lt;/span&gt; is the Riesz representer of the differential, ie.&lt;/p&gt;&lt;p class=&quot;tex&quot;&gt;$</span>	Df(p)[Y] = g<em>p(\operatorname{grad}f(p), Y),\qquad \text{ for all } Y ‚àà T</em>p\mathcal M<span>$&lt;/p&gt;&lt;p&gt;and since it is a tangent vector, we can write it in terms of a basis as&lt;/p&gt;&lt;p class=&quot;tex&quot;&gt;$</span>	\operatorname{grad}f(p) = \sum<em>{i=1}^{d} g</em>p(\operatorname{grad}f(p),X<em>i)X</em>i 	= \sum<em>{i=1}^{d} Df(p)[X</em>i]X<em>i<span>$&lt;/p&gt;&lt;p&gt;and perform the approximation from above to obtain&lt;/p&gt;&lt;p class=&quot;tex&quot;&gt;$</span>	\operatorname{grad}f(p) ‚âà \sum</em>{i=1}^{d} G<em>h(X</em>i)X_i$&lt;/p&gt;&lt;p&gt;for some suitable step size &lt;span class=&quot;tex&quot;&gt;<span>$h$</span>&lt;/span&gt;.This comes at the cost of &lt;span class=&quot;tex&quot;&gt;<span>$d+1$</span>&lt;/span&gt; function evaluations and &lt;span class=&quot;tex&quot;&gt;<span>$d$</span>&lt;/span&gt; exponential maps.&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;This is the first variant we can use. An advantage is, that it is &lt;em&gt;intrinsic&lt;/em&gt; in the sense that it does not require any embedding of the manifold.&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;markdown&quot;&gt;&lt;h3&gt;An Example: The Rayleigh Quotient&lt;/h3&gt;&lt;p&gt;The Rayleigh quotient is concerned with finding Eigenvalues (and Eigenvectors) of a symmetric matrix &lt;span class=&quot;tex&quot;&gt;<span>$A\in ‚Ñù^{(n+1)√ó(n+1)}$</span>&lt;/span&gt;. The optimisation problem reads&lt;/p&gt;&lt;p class=&quot;tex&quot;&gt;<span>$F\colon ‚Ñù^{n+1} \to ‚Ñù,\quad F(\mathbf x) = \frac{\mathbf x^\mathrm{T}A\mathbf x}{\mathbf x^\mathrm{T}\mathbf x}$</span>&lt;/p&gt;&lt;p&gt;Minimizing this function yields the smallest eigenvalue &lt;span class=&quot;tex&quot;&gt;<span>$\lambda_1$</span>&lt;/span&gt; as a value and the corresponding minimizer &lt;span class=&quot;tex&quot;&gt;<span>$\mathbf x^*$</span>&lt;/span&gt; is a corresponding eigenvector.&lt;/p&gt;&lt;p&gt;Since the length of an eigenvector is irrelevant, there is an ambiguity in the cost function. It can be better phrased on the sphere &lt;span class=&quot;tex&quot;&gt;<span>$ùïä^n$</span>&lt;/span&gt; of unit vectors in &lt;span class=&quot;tex&quot;&gt;<span>$\mathbb R^{n+1}$</span>&lt;/span&gt;, i.e.&lt;/p&gt;&lt;p class=&quot;tex&quot;&gt;<span>$\operatorname*{arg\,min}_{p \in ùïä^n} f(p) = \operatorname*{arg\,min}_{p \in ùïä^n} p^\mathrm{T}Ap$</span>&lt;/p&gt;&lt;p&gt;We can compute the Riemannian gradient exactly as&lt;/p&gt;&lt;p class=&quot;tex&quot;&gt;<span>$\operatorname{grad} f(p) = 2(Ap - pp^\mathrm{T}Ap)$</span>&lt;/p&gt;&lt;p&gt;so we can compare it to the approximation by finite differences.&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;pre class=&#39;language-julia&#39;&gt;&lt;code class=&#39;language-julia&#39;&gt;begin     Random.seed!(42)     n = 200     A = randn(n + 1, n + 1)     A = Symmetric(A)     M = Sphere(n)     nothing end&lt;/code&gt;&lt;/pre&gt; &lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;pre class=&#39;language-julia&#39;&gt;&lt;code class=&#39;language-julia&#39;&gt;f1(p) = p&#39; * A&#39;p&lt;/code&gt;&lt;/pre&gt; &lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;pre class=&quot;code-output documenter-example-output&quot; id=&quot;var-f1&quot;&gt;f1 (generic function with 1 method)&lt;/pre&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;pre class=&#39;language-julia&#39;&gt;&lt;code class=&#39;language-julia&#39;&gt;gradf1(p) = 2 * (A * p - p * p&#39; * A * p)&lt;/code&gt;&lt;/pre&gt; &lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;pre class=&quot;code-output documenter-example-output&quot; id=&quot;var-gradf1&quot;&gt;gradf1 (generic function with 1 method)&lt;/pre&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;Manifolds provides a finite difference scheme in Tangent spaces, that you can introduce to use an existing framework (if the wrapper is implemented) form Euclidean space. Here we use &lt;code&gt;FiniteDiff.jl&lt;/code&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;pre class=&#39;language-julia&#39;&gt;&lt;code class=&#39;language-julia&#39;&gt;r<em>backend = Manifolds.TangentDiffBackend(Manifolds.FiniteDifferencesBackend())&lt;/code&gt;&lt;/pre&gt; &lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;pre class=&quot;code-output documenter-example-output&quot; id=&quot;var-r</em>backend&quot;&gt;Manifolds.TangentDiffBackend{Manifolds.FiniteDifferencesBackend{FiniteDifferences.AdaptedFiniteDifferenceMethod{5, 1, FiniteDifferences.UnadaptedFiniteDifferenceMethod{7, 5}}}, ExponentialRetraction, LogarithmicInverseRetraction, DefaultOrthonormalBasis{‚Ñù, ManifoldsBase.TangentSpaceType}, DefaultOrthonormalBasis{‚Ñù, ManifoldsBase.TangentSpaceType}}(Manifolds.FiniteDifferencesBackend{FiniteDifferences.AdaptedFiniteDifferenceMethod{5, 1, FiniteDifferences.UnadaptedFiniteDifferenceMethod{7, 5}}}(FiniteDifferences.AdaptedFiniteDifferenceMethod{5, 1, FiniteDifferences.UnadaptedFiniteDifferenceMethod{7, 5}}([-2, -1, 0, 1, 2], [0.08333333333333333, -0.6666666666666666, 0.0, 0.6666666666666666, -0.08333333333333333], ([-0.08333333333333333, 0.5, -1.5, 0.8333333333333334, 0.25], [0.08333333333333333, -0.6666666666666666, 0.0, 0.6666666666666666, -0.08333333333333333], [-0.25, -0.8333333333333334, 1.5, -0.5, 0.08333333333333333]), 10.0, 1.0, Inf, 0.05555555555555555, 1.4999999999999998, FiniteDifferences.UnadaptedFiniteDifferenceMethod{7, 5}([-3, -2, -1, 0, 1, 2, 3], [-0.5, 2.0, -2.5, 0.0, 2.5, -2.0, 0.5], ([0.5, -4.0, 12.5, -20.0, 17.5, -8.0, 1.5], [-0.5, 2.0, -2.5, 0.0, 2.5, -2.0, 0.5], [-1.5, 8.0, -17.5, 20.0, -12.5, 4.0, -0.5]), 10.0, 1.0, Inf, 0.5365079365079365, 10.0))), ExponentialRetraction(), LogarithmicInverseRetraction(), DefaultOrthonormalBasis(‚Ñù), DefaultOrthonormalBasis(‚Ñù))&lt;/pre&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;pre class=&#39;language-julia&#39;&gt;&lt;code class=&#39;language-julia&#39;&gt;gradf1<em>FD(p) = Manifolds.gradient(M, f1, p, r</em>backend)&lt;/code&gt;&lt;/pre&gt; &lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;pre class=&quot;code-output documenter-example-output&quot; id=&quot;var-gradf1<em>FD&quot;&gt;gradf1</em>FD (generic function with 1 method)&lt;/pre&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;pre class=&#39;language-julia&#39;&gt;&lt;code class=&#39;language-julia&#39;&gt;begin     p = zeros(n + 1)     p[1] = 1.0     X1 = gradf1(p)     X2 = gradf1_FD(p)     norm(M, p, X1 - X2) end&lt;/code&gt;&lt;/pre&gt; &lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;pre class=&quot;code-output documenter-example-output&quot; id=&quot;var-p&quot;&gt;1.016587429282545e-12&lt;/pre&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;We obtain quite a good approximation of the gradient.&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;!DOCTYPE &gt;&lt;HTML&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;```</p><h2 id=".-Conversion-of-an-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-an-(not-necessarily-isometrically)-embedded-Manifold"><a class="docs-heading-anchor" href="#.-Conversion-of-an-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-an-(not-necessarily-isometrically)-embedded-Manifold">2. Conversion of an Euclidean Gradient in the Embedding to a Riemannian Gradient of an (not necessarily isometrically) embedded Manifold</a><a id=".-Conversion-of-an-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-an-(not-necessarily-isometrically)-embedded-Manifold-1"></a><a class="docs-heading-anchor-permalink" href="#.-Conversion-of-an-Euclidean-Gradient-in-the-Embedding-to-a-Riemannian-Gradient-of-an-(not-necessarily-isometrically)-embedded-Manifold" title="Permalink"></a></h2><div class="markdown">
<p>Let <span class="tex">$\tilde f\colon\mathbb R^m \to \mathbb R$</span> be a function in the embedding of an <span class="tex">$n$</span>-dimensional manifold <span class="tex">$\mathcal M \subset \mathbb R^m$</span> and <span class="tex">$f\colon \mathcal M \to \mathbb R$</span> denote the restriction of <span class="tex">$\tilde f$</span> to the manifold <span class="tex">$\mathcal M$</span>.</p><p>Since we can use the push forward of the embedding to also embed the tangent space <span class="tex">$T_p\mathcal M$</span>, <span class="tex">$p\in \mathcal M$</span>, we can similarly obtain the differential <span class="tex">$Df(p)\colon T_p\mathcal M \to \mathbb R$</span> by restricting the differential <span class="tex">$D\tilde f(p)$</span> to the tangent space.</p><p>If both <span class="tex">$T_p\mathcal M$</span> and <span class="tex">$T_p\mathcal R^m$</span> have the same inner product, or in other words the manifold is isometrically embedded in <span class="tex">$R^m$</span> (like for example the sphere <span class="tex">$\mathbb S^n\subset\mathbb R^{m+1}$</span> then this restriction of the differential directly translates to a projection of the gradient, i.e.</p><p class="tex">$$\operatorname{grad}f(p) = \operatorname{Proj}_{T_p\mathcal M}(\operatorname{grad} \tilde f(p))$$</p><p>More generally we might have to take a change of the metric into account, i.e.</p><p class="tex">$$\langle  \operatorname{Proj}_{T_p\mathcal M}(\operatorname{grad} \tilde f(p)), X \rangle
= Df(p)[X] = g_p(\operatorname{grad}f(p), X)$$</p><p>or in words: we have to change the Riesz representer of the (restricted/projected) differential of <span class="tex">$f$</span> (<span class="tex">$\tilde f$</span>) to the one with respect to the Riemannian metric. This is done using <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/metric.html#Manifolds.change_representer-Tuple{AbstractManifold,%20AbstractMetric,%20Any,%20Any}"><code>change_representer</code></a>.</p></div></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><h3>A continued Example</h3><p>We continue with the Rayleigh Quotient from before, now just starting with the defintion of the Euclidean case in the embedding, the function <span class="tex">$F$</span>.</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>F(x) = x' * A * x / (x' * x);</code></pre>
<!DOCTYPE ><HTML><head></head><body></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>The cost function is the same by restriction</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>f2(M, p) = F(p);</code></pre>
<!DOCTYPE ><HTML><head></head><body></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>The gradient is now computed combining our gradient scheme with FiniteDifferences.</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>function grad_f2_AD(M, p)
    return Manifolds.gradient(
        M, F, p, Manifolds.RiemannianProjectionBackend(Manifolds.FiniteDifferencesBackend())
    )
end</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-grad_f2_AD">grad_f2_AD (generic function with 1 method)</pre></body></HTML>

<pre class='language-julia'><code class='language-julia'>X3 = grad_f2_AD(M, p)</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-X3">201-element Vector{Float64}:
  0.0
  0.14038510230588766
  1.2081562751116681
 -1.8650092022183193
 -1.3296034534632897
  0.8233596435551424
  0.013307949254910802
  ‚ãÆ
 -1.0895103349140873
  2.423453243905215
  2.349106449107357
  0.5799736335284804
 -0.07423232665910076
  2.0859147085739482</pre></body></HTML>

<pre class='language-julia'><code class='language-julia'>norm(M, p, X1 - X3)</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-hash230331">1.721066272238026e-12</pre></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><h3>An Example for a nonisometrically embedded Manifold</h3><p>on the manifold <span class="tex">$\mathcal P(3)$</span> of symmetric positive definite matrices.</p></div></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>The following function computes (half) the distance squared (with respect to the linear affine metric) on the manifold <span class="tex">$\mathcal P(3)$</span> to the identity, i.e. <span class="tex">$I_3$</span>. denoting the unit matrix we consider the function</p><p class="tex">$$	G(q) = \frac{1}{2}d^2_{\mathcal P(3)}(q,I_3) = \lVert \operatorname{Log}(q) \rVert_F^2,$$</p><p>where <span class="tex">$\operatorname{Log}$</span> denotes the matrix logarithm and <span class="tex">$\lVert \cdot \rVert_F$</span> is the Frobenius norm. This can be computed for symmetric positive definite matrices by summing the squares of the <span class="tex">$\log$</span>arithms of the eigenvalues of <span class="tex">$q$</span> and divide by two:</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>G(q) = sum(log.(eigvals(Symmetric(q))) .^ 2) / 2</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-G">G (generic function with 1 method)</pre></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>We can also interpret this as a function on the space of matrices and apply the Euclidean finite differences machinery; in this way we can easily derive the Euclidean gradient. But when computing the Riemannian gradient, we have to change the representer (see again <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/metric.html#Manifolds.change_representer-Tuple{AbstractManifold,%20AbstractMetric,%20Any,%20Any}"><code>change_representer</code></a>) after projecting onto the tangent space <span class="tex">$T_p\mathcal P(n)$</span> at <span class="tex">$p$</span>.</p><p>Let's first define a point and the manifold <span class="tex">$N=\mathcal P(3)$</span>.</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>rotM(Œ±) = [1.0 0.0 0.0; 0.0 cos(Œ±) sin(Œ±); 0.0 -sin(Œ±) cos(Œ±)]</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-rotM">rotM (generic function with 1 method)</pre></body></HTML>

<pre class='language-julia'><code class='language-julia'>q = rotM(œÄ / 6) * [1.0 0.0 0.0; 0.0 2.0 0.0; 0.0 0.0 3.0] * transpose(rotM(œÄ / 6))</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-q">3√ó3 Matrix{Float64}:
 1.0  0.0       0.0
 0.0  2.25      0.433013
 0.0  0.433013  2.75</pre></body></HTML>

<pre class='language-julia'><code class='language-julia'>N = SymmetricPositiveDefinite(3)</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-N">SymmetricPositiveDefinite(3)</pre></body></HTML>

<pre class='language-julia'><code class='language-julia'>is_point(N, q)</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-hash141872">true</pre></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>We could first just compute the gradient using <code>FiniteDifferences.jl</code>, but this yields the Euclidean gradient:</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>FiniteDifferences.grad(central_fdm(5, 1), G, q)</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-hash184002">([3.240417492806275e-14 -2.3531899864903462e-14 0.0; 0.0 0.3514812167654708 0.017000516835452926; 0.0 0.0 0.36129646973723023],)</pre></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>Instead, we use the <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/features/differentiation.html#Manifolds.RiemannianProjectionBackend"><code>RiemannianProjectedBackend</code></a> of <code>Manifolds.jl</code>, which in this case internally uses <code>FiniteDifferences.jl</code> to compute a Euclidean gradient but then uses the conversion explained above to derive the Riemannian gradient.</p><p>We define this here again as a function <code>grad_G_FD</code> that could be used in the <code>Manopt.jl</code> framework within a gradient based optimisation.</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>function grad_G_FD(N, q)
    return Manifolds.gradient(
        N, G, q, Manifolds.RiemannianProjectionBackend(Manifolds.FiniteDifferencesBackend())
    )
end</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-grad_G_FD">grad_G_FD (generic function with 1 method)</pre></body></HTML>

<pre class='language-julia'><code class='language-julia'>G1 = grad_G_FD(N, q)</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-G1">3√ó3 Matrix{Float64}:
  3.24042e-14  -2.64734e-14  -5.09481e-15
 -2.64734e-14   1.86368       0.826856
 -5.09481e-15   0.826856      2.81845</pre></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>Now, we can agaon compare this to the (known) solution of the gradient, namely the gradient of (a half) the distance suqared, i.e. <span class="tex">$G(q) = \frac{1}{2}d^2_{\mathcal P(3)}(q,I_3)$</span> is given by <span class="tex">$\operatorname{grad} G(q) = -\operatorname{log}_q I_3$</span>, where <span class="tex">$\operatorname{log}$</span> is the <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/symmetricpositivedefinite.html#Base.log-Tuple{SymmetricPositiveDefinite,%20Vararg{Any,%20N}%20where%20N}">logarithmic map</a> on the manifold.</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>G2 = -log(N, q, Matrix{Float64}(I, 3, 3))</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-G2">3√ó3 Matrix{Float64}:
 -0.0  -0.0       -0.0
 -0.0   1.86368    0.826856
 -0.0   0.826856   2.81845</pre></body></HTML>


<!DOCTYPE ><HTML><head></head><body><div class="markdown"><p>Both terms agree up to <span class="tex">$1.8√ó10^{-12}$</span>:</p></div></body></HTML>

<pre class='language-julia'><code class='language-julia'>norm(G1 - G2)</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-hash135981">1.776388869742036e-12</pre></body></HTML>

<pre class='language-julia'><code class='language-julia'>isapprox(M, q, G1, G2; atol=2 * 1e-12)</code></pre>
<!DOCTYPE ><HTML><head></head><body><pre class="code-output documenter-example-output" id="var-hash251789">true</pre></body></HTML>


<!DOCTYPE ><HTML><head></head><body>```
## Summary<p>@raw html &lt;div class=&quot;markdown&quot;&gt; &lt;p&gt;This tutorial illustrates how to use tools from Euclidean spaces, finite differences or automatic differentiation, to compute gradients on Riemannian manifolds. The scheme allows to use &lt;em&gt;any&lt;/em&gt; differentiation framework within the embedding to derive a Riemannian gradient.&lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/HTML&gt;</p><p>&lt;!‚Äì PlutoStaticHTML.End ‚Äì&gt;</p><pre><code class="nohighlight hljs"></code></pre><p>@meta EditURL = &quot;https://github.com/JuliaManifolds/Manopt.jl/blob/main/nothing&quot; ```</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Optimize!/">¬´ Get started: Optimize!</a><a class="docs-footer-nextpage" href="../HowToRecord/">Record values ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 19 October 2022 08:11">Wednesday 19 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
