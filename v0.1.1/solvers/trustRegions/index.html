<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Riemannian Trust-Regions Solver · Manopt.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="Manopt.jl logo"/></a><h1>Manopt.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../about/">About</a></li><li><span class="toctext">Manifolds</span><ul><li><a class="toctext" href="../../manifolds/">Introduction</a></li><li><a class="toctext" href="../../manifolds/combined/">Combinations of Manifolds</a></li><li><a class="toctext" href="../../manifolds/circle/">The Circle <span>$\mathbb S^1$</span></a></li><li><a class="toctext" href="../../manifolds/euclidean/">The Euclidean Space <span>$\mathbb R^n$</span></a></li><li><a class="toctext" href="../../manifolds/grassmannian/">The Grassmannian Manifold <span>$\mathrm{Gr}(k,n)$</span></a></li><li><a class="toctext" href="../../manifolds/hyperbolic/">The Hyperbolic Space <span>$\mathbb H^n$</span></a></li><li><a class="toctext" href="../../manifolds/rotations/">The Special Orthogonal Group <span>$\mathrm{SO}(n)$</span></a></li><li><a class="toctext" href="../../manifolds/sphere/">The Sphere <span>$\mathbb S^n$</span></a></li><li><a class="toctext" href="../../manifolds/stiefel/">The Stiefel Manifold <span>$\mathrm{St}(k,n)$</span></a></li><li><a class="toctext" href="../../manifolds/symmetric/">The Symmetric Matrices <span>$\mathrm{Sym}(n)$</span></a></li><li><a class="toctext" href="../../manifolds/symmetricpositivedefinite/">The Symmetric Positive Definite Matrices <span>$\mathcal P(n)$</span></a></li></ul></li><li><a class="toctext" href="../../plans/">Plans</a></li><li><span class="toctext">Solvers</span><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../cyclicProximalPoint/">Cyclic Proximal Point</a></li><li><a class="toctext" href="../DouglasRachford/">Douglas–Rachford</a></li><li><a class="toctext" href="../gradientDescent/">Gradient Descent</a></li><li><a class="toctext" href="../NelderMead/">Nelder–Mead</a></li><li><a class="toctext" href="../subGradientMethod/">Subgradient Method</a></li><li><a class="toctext" href="../truncatedConjugateGradient/">Steihaug-Toint TCG Method</a></li><li class="current"><a class="toctext" href>Riemannian Trust-Regions Solver</a><ul class="internal"><li><a class="toctext" href="#Initialization-1">Initialization</a></li><li><a class="toctext" href="#Iteration-1">Iteration</a></li><li><a class="toctext" href="#Result-1">Result</a></li><li><a class="toctext" href="#Remarks-1">Remarks</a></li><li><a class="toctext" href="#Interface-1">Interface</a></li><li><a class="toctext" href="#Options-1">Options</a></li><li><a class="toctext" href="#Approximation-of-the-Hessian-1">Approximation of the Hessian</a></li></ul></li></ul></li><li><span class="toctext">Functions</span><ul><li><a class="toctext" href="../../functions/">Introduction</a></li><li><a class="toctext" href="../../functions/costFunctions/">cost functions</a></li><li><a class="toctext" href="../../functions/differentials/">Differentials</a></li><li><a class="toctext" href="../../functions/adjointDifferentials/">Adjoint Differentials</a></li><li><a class="toctext" href="../../functions/gradients/">Gradients</a></li><li><a class="toctext" href="../../functions/jacobiFields/">JacobiFields</a></li><li><a class="toctext" href="../../functions/proximalMaps/">Proximal Maps</a></li></ul></li><li><span class="toctext">Helpers</span><ul><li><a class="toctext" href="../../helpers/data/">Data</a></li><li><a class="toctext" href="../../helpers/errorMeasures/">Error Measures</a></li><li><a class="toctext" href="../../helpers/exports/">Exports</a></li></ul></li><li><span class="toctext">Tutorials</span><ul><li><a class="toctext" href="../../tutorials/MeanAndMedian/">Getting Started: Optimize!</a></li><li><a class="toctext" href="../../tutorials/GradientOfSecondOrderDifference/">Gradient of <span>$d_2$</span></a></li><li><a class="toctext" href="../../tutorials/JacobiFields/">Jacobi Fields</a></li></ul></li><li><a class="toctext" href="../../list/">Function Index</a></li></ul></nav><article id="docs"><header><nav><ul><li>Solvers</li><li><a href>Riemannian Trust-Regions Solver</a></li></ul><a class="edit-page" href="https://github.com/kellertuer/Manopt.jl/blob/master/docs/src/solvers/trustRegions.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Riemannian Trust-Regions Solver</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="trustRegions-1" href="#trustRegions-1">The Riemannian Trust-Regions Solver</a></h1><p>The aim is to solve an optimization problem on a manifold</p><div>\[\operatorname*{min}_{x \in \mathcal{M}} F(x)\]</div><p>by using the Riemannian trust-regions solver. It is number one choice for smooth optimization. This trust-region method uses the Steihaug-Toint truncated conjugate-gradient method <a href="../truncatedConjugateGradient/#Manopt.truncatedConjugateGradient"><code>truncatedConjugateGradient</code></a> to solve the inner minimization problem called the trust-regions subproblem. This inner solve can be preconditioned by providing a preconditioner (symmetric and positive deﬁnite, an approximation of the inverse of the Hessian of <span>$F$</span>). If no Hessian of the cost function <span>$F$</span> is provided, a standard approximation of the Hessian based on the gradient <span>$\nabla F$</span> with <a href="#Manopt.approxHessianFD"><code>approxHessianFD</code></a> will be computed.</p><h2><a class="nav-anchor" id="Initialization-1" href="#Initialization-1">Initialization</a></h2><p>Initialize <span>$x_0 = x$</span> with an initial point <span>$x$</span> on the manifold. It can be given by the caller or set randomly. Set the initial trust-region radius <span>$\Delta =\frac{1}{8} \bar{\Delta}$</span> where <span>$\bar{\Delta}$</span> is the maximum radius the trust-region can have. Usually one uses the root of the manifold dimension <span>$\operatorname{dim}(\mathcal{M})$</span>. For accepting the next iterate and evaluating the new trust-region radius one needs an accept/reject threshold <span>$\rho&#39; \in [0,\frac{1}{4})$</span>, which is <span>$\rho&#39; = 0.1$</span> on default. Set <span>$k=0$</span>.</p><h2><a class="nav-anchor" id="Iteration-1" href="#Iteration-1">Iteration</a></h2><p>Repeat until a convergence criterion is reached</p><ol><li>Set <span>$\eta$</span> as a random tangent vector if using randomized approach. Else  set <span>$\eta$</span> as the zero vector in the tangential space <span>$T_{x_k}\mathcal{M}$</span>.</li><li>Set <span>$\eta^{* }$</span> as the solution of the trust-region subproblem, computed by  the tcg-method with <span>$\eta$</span> as initial vector.</li><li>If using randomized approach compare <span>$\eta^{* }$</span> with the Cauchy point  <span>$\eta_{c}^{* } = -\tau_{c} \frac{\Delta}{\operatorname{norm}(\operatorname{Grad}[f] (x_k))} \operatorname{Grad}[F] (x_k)$</span> by the model function <span>$m_{x_k}(\cdot)$</span>. If the  model decrease is larger by using the Cauchy point, set  <span>$\eta^{* } = \eta_{c}^{* }$</span>.</li><li>Set <span>${x}^{* } = \operatorname{Retr}_{x_k}(\eta^{* })$</span>.</li><li>Set <span>$\rho = \frac{F(x_k)-F({x}^{* })}{m_{x_k}(\eta)-m_{x_k}(\eta^{* })}$</span>, where  <span>$m_{x_k}(\cdot)$</span> describes the quadratic model function.</li><li>Update the trust-region radius:  <span>$\Delta = \begin{cases} \frac{1}{4} \Delta &amp; \rho &lt; \frac{1}{4} \,  \text{or} \, m_{x_k}(\eta)-m_{x_k}(\eta^{* }) \leq 0 \, \text{or}  \,  \rho = \pm \infty , \\ \operatorname{min}(2 \Delta, \bar{\Delta}) &amp;  \rho &gt; \frac{3}{4} \, \text{and the tcg-method stopped because of negative  curvature or exceeding the trust-region}, \\ \Delta &amp; \, \text{otherwise.}  \end{cases}$</span></li><li>If <span>$m_{x_k}(\eta)-m_{x_k}(\eta^{* }) \geq 0$</span> and <span>$\rho &gt; \rho&#39;$</span> set  <span>$x_k = {x}^{* }$</span>.</li><li>Set <span>$k = k+1$</span>.</li></ol><h2><a class="nav-anchor" id="Result-1" href="#Result-1">Result</a></h2><p>The result is given by the last computed <span>$x_k$</span>.</p><h2><a class="nav-anchor" id="Remarks-1" href="#Remarks-1">Remarks</a></h2><p>To the Initialization: A random point on the manifold can be generated with <a href="../../manifolds/combined/#Manopt.randomMPoint-Tuple{TangentBundle}"><code>randomMPoint</code></a><code>(M)</code>.</p><p>To step number 1: Using randomized approach means using a random tangent vector as initial vector for the approximal solve of the trust-regions subproblem. If this is the case, keep in mind that the vector must be in the trust-region radius. This is achieved by multiplying <code>η =</code><a href="../../manifolds/combined/#Manopt.randomTVector-Tuple{TangentBundle,TBPoint}"><code>randomTVector</code></a><code>(M,x)</code> by <code>sqrt(4,eps(Float64))</code> as long as its norm is greater than the current trust-region radius <span>$\Delta$</span>. For not using randomized approach, one can get the zero  tangent vector with <code>η =</code><a href="../../manifolds/combined/#Manopt.zeroTVector-Union{Tuple{P}, Tuple{Mt}, Tuple{TangentBundle{Mt},TBPoint{P,T} where T&lt;:TVector}} where P&lt;:MPoint where Mt&lt;:Manifold"><code>zeroTVector</code></a><code>(M,x)</code>.</p><p>To step number 2: Obtain <span>$\eta^{* }$</span> by (approximately) solving the trust-regions subproblem</p><div>\[\operatorname*{arg\,min}_{\eta \in T_{x_k}\mathcal{M}} m_{x_k}(\eta) = F(x_k) +
\langle \nabla F(x_k), \eta \rangle_{x_k} + \frac{1}{2} \langle
\operatorname{Hess}[F](\eta)_ {x_k}, \eta \rangle_{x_k}\]</div><div>\[\text{s.t.} \; \langle \eta, \eta \rangle_{x_k} \leq {\Delta}^2\]</div><p>with the Steihaug-Toint truncated conjugate-gradient (tcg) method. The problem as well as the solution method is described in the <a href="../truncatedConjugateGradient/#Manopt.truncatedConjugateGradient"><code>truncatedConjugateGradient</code></a>.</p><p>To step number 3: If using a random tangent vector as an initial vector, compare the result of the tcg-method with the Cauchy point. Convergence proofs assume that one achieves at least (a fraction of) the reduction of the Cauchy point. The idea is to go in the direction of the gradient to an optimal point. This can be on the edge, but also before. The parameter <span>$\tau_{c}$</span> for the optimal length is defined by</p><div>\[\tau_{c} = \begin{cases} 1 &amp; \langle \operatorname{Grad}[F] (x_k), \,
\operatorname{Hess}[F] (\eta_k)_ {x_k}\rangle_{x_k} \leq 0 , \\
\operatorname{min}(\frac{{\operatorname{norm}(\operatorname{Grad}[F] (x_k))}^3}
{\Delta \langle \operatorname{Grad}[F] (x_k), \,
\operatorname{Hess}[F] (\eta_k)_ {x_k}\rangle_{x_k}}, 1) &amp; \, \text{otherwise.}
\end{cases}\]</div><p>To check the model decrease one compares <span>$m_{x_k}(\eta_{c}^{* }) = F(x_k) + \langle \eta_{c}^{* }, \operatorname{Grad}[F] (x_k)\rangle_{x_k} + \frac{1}{2}\langle \eta_{c}^{* }, \operatorname{Hess}[F] (\eta_{c}^{* })_ {x_k}\rangle_{x_k}$</span> with <span>$m_{x_k}(\eta^{* }) = F(x_k) + \langle \eta^{* }, \operatorname{Grad}[F] (x_k)\rangle_{x_k} + \frac{1}{2}\langle \eta^{* }, \operatorname{Hess}[F] (\eta^{* })_ {x_k}\rangle_{x_k}$</span>. If <span>$m_{x_k}(\eta_{c}^{* }) &lt; m_{x_k}(\eta^{* })$</span> then is <span>$m_{x_k}(\eta_{c}^{* })$</span> the better choice.</p><p>To step number 4: <span>$\operatorname{Retr}_{x_k}(\cdot)$</span> denotes the retraction, a mapping <span>$\operatorname{Retr}_{x_k}:T_{x_k}\mathcal{M} \rightarrow \mathcal{M}$</span> wich approximates the exponential map. In some cases it is cheaper to use this instead of the exponential.</p><p>To step number 6: One knows that the <a href="../truncatedConjugateGradient/#Manopt.truncatedConjugateGradient"><code>truncatedConjugateGradient</code></a> algorithm stopped for these reasons when the stopping criteria <a href="../truncatedConjugateGradient/#Manopt.stopWhenCurvatureIsNegative"><code>stopWhenCurvatureIsNegative</code></a>, <a href="../truncatedConjugateGradient/#Manopt.stopWhenTrustRegionIsExceeded"><code>stopWhenTrustRegionIsExceeded</code></a> are activated.</p><p>To step number 7: The last step is to decide if the new point <span>${x}^{* }$</span> is accepted.</p><h2><a class="nav-anchor" id="Interface-1" href="#Interface-1">Interface</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Manopt.trustRegions" href="#Manopt.trustRegions"><code>Manopt.trustRegions</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-julia">trustRegions(M, F, ∇F, x, H)</code></pre><p>evaluate the Riemannian trust-regions solver for optimization on manifolds. It will attempt to minimize the cost function F on the Manifold M. If no Hessian H is provided, a standard approximation of the Hessian based on the gradient ∇F will be computed. For solving the the inner trust-region subproblem of finding an update-vector, it uses the Steihaug-Toint truncated conjugate-gradient method. For a description of the algorithm and more details see</p><ul><li>P.-A. Absil, C.G. Baker, K.A. Gallivan,   Trust-region methods on Riemannian manifolds, FoCM, 2007.   doi: <a href="https://doi.org/10.1007/s10208-005-0179-9">10.1007/s10208-005-0179-9</a></li><li>A. R. Conn, N. I. M. Gould, P. L. Toint, Trust-region methods, SIAM,   MPS, 2000. doi: <a href="https://doi.org/10.1137/1.9780898719857">10.1137/1.9780898719857</a></li></ul><p><strong>Input</strong></p><ul><li><code>M</code> – a manifold <span>$\mathcal M$</span></li><li><code>F</code> – a cost function <span>$F \colon \mathcal M \to \mathbb R$</span> to minimize</li><li><code>∇F</code>- the gradient <span>$\nabla F \colon \mathcal M \to T \mathcal M$</span> of <span>$F$</span></li><li><code>x</code> – an initial value <span>$x \in \mathcal M$</span></li><li><code>H</code> – the hessian <span>$H( \mathcal M, x, \xi)$</span> of <span>$F$</span></li></ul><p><strong>Optional</strong></p><ul><li><code>retraction</code> – approximation of the exponential map <a href="../../manifolds/combined/#Base.exp"><code>exp</code></a></li><li><code>preconditioner</code> – a preconditioner (a symmetric, positive definite operator that should approximate the inverse of the Hessian)</li><li><code>stoppingCriterion</code> – (<a href="../#Manopt.stopWhenAny"><code>stopWhenAny</code></a>(<a href="../#Manopt.stopAfterIteration"><code>stopAfterIteration</code></a><code>(1000)</code>, <a href="../#Manopt.stopWhenGradientNormLess"><code>stopWhenGradientNormLess</code></a><code>(10^(-6))</code>) a functor inheriting from <a href="../#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> indicating when to stop.</li><li><code>Δ_bar</code> – the maximum trust-region radius</li><li><code>Δ</code> - the (initial) trust-region radius</li><li><code>useRandom</code> – set to true if the trust-region solve is to be initiated with a random tangent vector. If set to true, no preconditioner will be used. This option is set to true in some scenarios to escape saddle points, but is otherwise seldom activated.</li><li><code>ρ_prime</code> – Accept/reject threshold: if ρ (the performance ratio for the iterate) is at least ρ&#39;, the outer iteration is accepted. Otherwise, it is rejected. In case it is rejected, the trust-region radius will have been decreased. To ensure this, ρ&#39; &gt;= 0 must be strictly smaller than 1/4. If ρ_prime is negative, the algorithm is not guaranteed to produce monotonically decreasing cost values. It is strongly recommended to set ρ&#39; &gt; 0, to aid convergence.</li><li><code>ρ_regularization</code> – Close to convergence, evaluating the performance ratio ρ is numerically challenging. Meanwhile, close to convergence, the quadratic model should be a good fit and the steps should be accepted. Regularization lets ρ go to 1 as the model decrease and the actual decrease go to zero. Set this option to zero to disable regularization (not recommended). When this is not zero, it may happen that the iterates produced are not monotonically improving the cost when very close to convergence. This is because the corrected cost improvement could change sign if it is negative but very small.</li><li><code>returnOptions</code> – (<code>false</code>) – if actiavated, the extended result, i.e. the complete <a href="../../plans/#Manopt.Options"><code>Options</code></a> are returned. This can be used to access recorded values. If set to false (default) just the optimal value <code>xOpt</code> is returned</li></ul><p><strong>Output</strong></p><ul><li><code>x</code> – the last reached point on the manifold</li></ul><p><strong>see also</strong></p><p><a href="../truncatedConjugateGradient/#Manopt.truncatedConjugateGradient"><code>truncatedConjugateGradient</code></a></p></div><a class="source-link" target="_blank" href="https://github.com/kellertuer/Manopt.jl/blob/b6fdd47b484ab565a9f6945ba878c403700a116b/src/solvers/trustRegions.jl#L8">source</a></section><h2><a class="nav-anchor" id="Options-1" href="#Options-1">Options</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Manopt.TrustRegionsOptions" href="#Manopt.TrustRegionsOptions"><code>Manopt.TrustRegionsOptions</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-julia">TrustRegionsOptions &lt;: HessianOptions</code></pre><p>describe the trust-regions solver, with</p><p><strong>Fields</strong></p><p>a default value is given in brackets if a parameter can be left out in initialization.</p><ul><li><code>x</code> : a <a href="../../manifolds/#Manopt.MPoint"><code>MPoint</code></a> as starting point</li><li><code>stop</code> : a function s,r = @(o,iter) returning a stop   indicator and a reason based on an iteration number and the gradient</li><li><code>Δ</code> : the (initial) trust-region radius</li><li><code>Δ_bar</code> : the maximum trust-region radius</li><li><code>useRand</code> : indicates if the trust-region solve is to be initiated with a       random tangent vector. If set to true, no preconditioner will be       used. This option is set to true in some scenarios to escape saddle       points, but is otherwise seldom activated.</li><li><code>ρ_prime</code> : a lower bound of the performance ratio for the iterate that       decides if the iteration will be accepted or not. If not, the       trust-region radius will have been decreased. To ensure this,       ρ&#39;&gt;= 0 must be strictly smaller than 1/4. If ρ&#39; is negative,       the algorithm is not guaranteed to produce monotonically decreasing       cost values. It is strongly recommended to set ρ&#39; &gt; 0, to aid       convergence.</li><li><code>ρ_regularization</code> : Close to convergence, evaluating the performance ratio ρ       is numerically challenging. Meanwhile, close to convergence, the       quadratic model should be a good fit and the steps should be       accepted. Regularization lets ρ go to 1 as the model decrease and       the actual decrease go to zero. Set this option to zero to disable       regularization (not recommended). When this is not zero, it may happen       that the iterates produced are not monotonically improving the cost       when very close to convergence. This is because the corrected cost       improvement could change sign if it is negative but very small.</li></ul><p><strong>Constructor</strong></p><pre><code class="language-none">TrustRegionsOptions(x, stop, delta, delta_bar, uR, rho_prime, rho_reg)</code></pre><p>construct a trust-regions Option with the fields as above.</p><p><strong>See also</strong></p><p><a href="#Manopt.trustRegions"><code>trustRegions</code></a></p></div><a class="source-link" target="_blank" href="https://github.com/kellertuer/Manopt.jl/blob/b6fdd47b484ab565a9f6945ba878c403700a116b/src/plans/hessianPlan.jl#L83">source</a></section><h2><a class="nav-anchor" id="Approximation-of-the-Hessian-1" href="#Approximation-of-the-Hessian-1">Approximation of the Hessian</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Manopt.approxHessianFD" href="#Manopt.approxHessianFD"><code>Manopt.approxHessianFD</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-julia">approxHessianFD(p,x,ξ,[stepsize=2.0^(-14)])</code></pre><p>return an approximated solution of the Hessian of the cost function applied to a <a href="../../manifolds/#Manopt.TVector"><code>TVector</code></a> <code>ξ</code> by using a generic finite difference approximation based on computations of the gradient.</p><p>Input</p><ul><li><code>p</code> – a Manopt problem structure (already containing the manifold and enough       information to compute the cost gradient)</li><li><code>x</code> – a <a href="../../manifolds/#Manopt.MPoint"><code>MPoint</code></a> where the Hessian is ​​to be approximated</li><li><code>ξ</code> – a <a href="../../manifolds/#Manopt.TVector"><code>TVector</code></a> on which the approximated Hessian is ​​to be applied</li></ul><p><strong>Optional</strong></p><ul><li><code>stepsize</code> – the length of the step with which the method should work</li></ul><p><strong>Output</strong></p><ul><li>a <a href="../../manifolds/#Manopt.TVector"><code>TVector</code></a> generated by applying the approximated Hessian to the   <a href="../../manifolds/#Manopt.TVector"><code>TVector</code></a> ξ</li></ul></div><a class="source-link" target="_blank" href="https://github.com/kellertuer/Manopt.jl/blob/b6fdd47b484ab565a9f6945ba878c403700a116b/src/plans/hessianPlan.jl#L163">source</a></section><footer><hr/><a class="previous" href="../truncatedConjugateGradient/"><span class="direction">Previous</span><span class="title">Steihaug-Toint TCG Method</span></a><a class="next" href="../../functions/"><span class="direction">Next</span><span class="title">Introduction</span></a></footer></article></body></html>
