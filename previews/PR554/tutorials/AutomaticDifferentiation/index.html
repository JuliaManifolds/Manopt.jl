<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Use automatic differentiation ¬∑ Manopt.jl</title><meta name="title" content="Use automatic differentiation ¬∑ Manopt.jl"/><meta property="og:title" content="Use automatic differentiation ¬∑ Manopt.jl"/><meta property="twitter:title" content="Use automatic differentiation ¬∑ Manopt.jl"/><meta name="description" content="Documentation for Manopt.jl."/><meta property="og:description" content="Documentation for Manopt.jl."/><meta property="twitter:description" content="Documentation for Manopt.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/citations.css" rel="stylesheet" type="text/css"/><link href="../../assets/link-icons.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../getstarted/">üèîÔ∏è Get started with Manopt.jl</a></li><li><a class="tocitem" href="../InplaceGradient/">Speedup using in-place computations</a></li><li class="is-active"><a class="tocitem" href>Use automatic differentiation</a><ul class="internal"><li><a class="tocitem" href="#1.-(Intrinsic)-forward-differences"><span>1. (Intrinsic) forward differences</span></a></li><li><a class="tocitem" href="#EmbeddedGradient"><span>2. Conversion of a Euclidean gradient in the embedding to a Riemannian Gradient of a (not Necessarily Isometrically) embedded manifold</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Technical-details"><span>Technical details</span></a></li></ul></li><li><a class="tocitem" href="../EmbeddingObjectives/">Define objectives in the embedding</a></li><li><a class="tocitem" href="../CountAndCache/">Count and use a cache</a></li><li><a class="tocitem" href="../HowToDebug/">Print debug output</a></li><li><a class="tocitem" href="../HowToRecord/">Record values</a></li><li><a class="tocitem" href="../ImplementASolver/">Implement a solver</a></li><li><a class="tocitem" href="../ImplementOwnManifold/">Optimize on your own manifold</a></li><li><a class="tocitem" href="../ConstrainedOptimization/">Do constrained optimization</a></li><li><a class="tocitem" href="../BoxDomain/">Do optimization with bounds</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../../solvers/">List of Solvers</a></li><li><a class="tocitem" href="../../solvers/adaptive-regularization-with-cubics/">Adaptive Regularization with Cubics</a></li><li><a class="tocitem" href="../../solvers/alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/augmented_Lagrangian_method/">Augmented Lagrangian Method</a></li><li><a class="tocitem" href="../../solvers/ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../../solvers/cma_es/">CMA-ES</a></li><li><a class="tocitem" href="../../solvers/conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../../solvers/conjugate_residual/">Conjugate Residual</a></li><li><a class="tocitem" href="../../solvers/convex_bundle_method/">Convex bundle method</a></li><li><a class="tocitem" href="../../solvers/cyclic_proximal_point/">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../../solvers/difference_of_convex/">Difference of Convex</a></li><li><a class="tocitem" href="../../solvers/DouglasRachford/">Douglas‚ÄîRachford</a></li><li><a class="tocitem" href="../../solvers/exact_penalty_method/">Exact Penalty Method</a></li><li><a class="tocitem" href="../../solvers/FrankWolfe/">Frank-Wolfe</a></li><li><a class="tocitem" href="../../solvers/generalized_cauchy_direction_subsolver/">Generalized Cauchy direction subsolver</a></li><li><a class="tocitem" href="../../solvers/gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/interior_point_Newton/">Interior Point Newton</a></li><li><a class="tocitem" href="../../solvers/LevenbergMarquardt/">Levenberg‚ÄìMarquardt</a></li><li><a class="tocitem" href="../../solvers/mesh_adaptive_direct_search/">Mesh Adaptive Direct Search</a></li><li><a class="tocitem" href="../../solvers/NelderMead/">Nelder‚ÄìMead</a></li><li><a class="tocitem" href="../../solvers/particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../../solvers/primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../../solvers/projected_gradient_method/">Projected Gradient Method</a></li><li><a class="tocitem" href="../../solvers/proximal_bundle_method/">Proximal bundle method</a></li><li><a class="tocitem" href="../../solvers/proximal_gradient_method/">Proximal Gradient Method</a></li><li><a class="tocitem" href="../../solvers/quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../../solvers/stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../../solvers/subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../../solvers/truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../../solvers/trust_regions/">Trust-Regions Solver</a></li><li><a class="tocitem" href="../../solvers/vectorbundle_newton/">Vector Bundle Newton Method</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/objective/">Objective</a></li><li><a class="tocitem" href="../../plans/state/">Solver State</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li><li><a class="tocitem" href="../../helpers/test/">Test</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../extensions/">Extensions</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../changelog/">Changelog</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">How to...</a></li><li class="is-active"><a href>Use automatic differentiation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Use automatic differentiation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/Manopt.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/tutorials/AutomaticDifferentiation.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Using-automatic-differentiation-in-Manopt.jl"><a class="docs-heading-anchor" href="#Using-automatic-differentiation-in-Manopt.jl">Using automatic differentiation in Manopt.jl</a><a id="Using-automatic-differentiation-in-Manopt.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Using-automatic-differentiation-in-Manopt.jl" title="Permalink"></a></h1><p>Since <a href="https://juliamanifolds.github.io/Manifolds.jl/latest/">Manifolds.jl</a> 0.7, the support of automatic differentiation support has been extended.</p><p>This tutorial explains how to use Euclidean tools to derive a gradient for a real-valued function <span>$f:  \mathcal M ‚Üí ‚Ñù$</span>. Two methods are considered: an intrinsic variant and a variant employing the embedding. These gradients can then be used within any gradient based optimization algorithm in <a href="https://manoptjl.org">Manopt.jl</a>.</p><p>While by default <a href="https://juliadiff.org/FiniteDifferences.jl/latest/">FiniteDifferences.jl</a>are used, one can also use <a href="https://github.com/JuliaDiff/FiniteDiff.jl">FiniteDiff.jl</a>, <a href="https://juliadiff.org/ForwardDiff.jl/stable/">ForwardDiff.jl</a>, <a href="https://juliadiff.org/ReverseDiff.jl/">ReverseDiff.jl</a>, or <a href="https://fluxml.ai/Zygote.jl/">Zygote.jl</a>.</p><p>This tutorial looks at a few possibilities to approximate or derive the gradient of a function <span>$f:\mathcal M ‚Üí ‚Ñù$</span> on a Riemannian manifold, without computing it yourself. There are mainly two different philosophies:</p><ol><li>Working <em>intrinsically</em>, that is staying on the manifold and in the tangent spaces, considering to approximate the gradient by forward differences.</li><li>Working in an embedding where all tools from functions on Euclidean spaces can be used, like finite differences or automatic differentiation, and then compute the corresponding Riemannian gradient from there.</li></ol><p>First, load all necessary packages</p><pre><code class="language-julia hljs">using Manopt, Manifolds, Random, LinearAlgebra
using FiniteDifferences, ManifoldDiff, ADTypes
Random.seed!(42);</code></pre><h2 id="1.-(Intrinsic)-forward-differences"><a class="docs-heading-anchor" href="#1.-(Intrinsic)-forward-differences">1. (Intrinsic) forward differences</a><a id="1.-(Intrinsic)-forward-differences-1"></a><a class="docs-heading-anchor-permalink" href="#1.-(Intrinsic)-forward-differences" title="Permalink"></a></h2><p>A first idea is to generalize (multivariate) finite differences to Riemannian manifolds. Let <span>$X_1,\ldots,X_d ‚àà T_p\mathcal M$</span> denote an orthonormal basis of the tangent space <span>$T_p\mathcal M$</span> at the point <span>$p‚àà\mathcal M$</span> on the Riemannian manifold.</p><p>The notion of a directional derivative is generalized to a ‚Äúdirection‚Äù <span>$Y‚ààT_p\mathcal M$</span>. Let <span>$c:  [-Œµ,Œµ]$</span>, <span>$Œµ&gt;0$</span>, be a curve with <span>$c(0) = p$</span>, <span>$\dot c(0) = Y$</span>, for example <span>$c(t)= \exp_p(tY)$</span>. This yields</p><p class="math-container">\[    Df(p)[Y] = \left. \frac{d}{dt} \right|_{t=0} f(c(t)) = \lim_{t ‚Üí 0} \frac{1}{t}(f(\exp_p(tY))-f(p))\]</p><p>The differential <span>$Df(p)[X]$</span> is approximated by a finite difference scheme for an <span>$h&gt;0$</span> as</p><p class="math-container">\[DF(p)[Y] ‚âà G_h(Y) := \frac{1}{h}(f(\exp_p(hY))-f(p))\]</p><p>Furthermore the gradient <span>$\operatorname{grad}f$</span> is the Riesz representer of the differential:</p><p class="math-container">\[    Df(p)[Y] = g_p(\operatorname{grad}f(p), Y),\qquad \text{ for all } Y ‚àà T_p\mathcal M\]</p><p>and since it is a tangent vector, we can write it in terms of a basis as</p><p class="math-container">\[    \operatorname{grad}f(p) = \sum_{i=1}^{d} g_p(\operatorname{grad}f(p),X_i)X_i
    = \sum_{i=1}^{d} Df(p)[X_i]X_i\]</p><p>and perform the approximation from before to obtain</p><p class="math-container">\[    \operatorname{grad}f(p) ‚âà \sum_{i=1}^{d} G_h(X_i)X_i\]</p><p>for some suitable step size <span>$h$</span>. This comes at the cost of <span>$d+1$</span> function evaluations and <span>$d$</span> exponential maps.</p><p>This is the first variant we can use. An advantage is that it is <em>intrinsic</em> in the sense that it does not require any embedding of the manifold.</p><h3 id="An-example:-the-Rayleigh-quotient"><a class="docs-heading-anchor" href="#An-example:-the-Rayleigh-quotient">An example: the Rayleigh quotient</a><a id="An-example:-the-Rayleigh-quotient-1"></a><a class="docs-heading-anchor-permalink" href="#An-example:-the-Rayleigh-quotient" title="Permalink"></a></h3><p>The Rayleigh quotient is concerned with finding eigenvalues (and eigenvectors) of a symmetric matrix <span>$A ‚àà ‚Ñù^{(n+1)√ó(n+1)}$</span>. The optimization problem reads</p><p class="math-container">\[F:  ‚Ñù^{n+1} ‚Üí ‚Ñù,\quad F(\mathbf x) = \frac{\mathbf x^\mathrm{T}A\mathbf x}{\mathbf x^\mathrm{T}\mathbf x}\]</p><p>Minimizing this function yields the smallest eigenvalue <span>$\lambda_1$</span> as a value and the corresponding minimizer <span>$\mathbf x^*$</span> is a corresponding eigenvector.</p><p>Since the length of an eigenvector is irrelevant, there is an ambiguity in the cost function. It can be better phrased on the sphere <span>$ùïä^n$</span> of unit vectors in <span>$‚Ñù^{n+1}$</span>,</p><p class="math-container">\[\operatorname*{arg\,min}_{p ‚àà ùïä^n}\ f(p) = \operatorname*{arg\,min}_{\ p ‚àà ùïä^n} p^\mathrm{T}Ap\]</p><p>We can compute the Riemannian gradient exactly as</p><p class="math-container">\[\operatorname{grad} f(p) = 2(Ap - pp^\mathrm{T}Ap)\]</p><p>so we can compare it to the approximation by finite differences.</p><pre><code class="language-julia hljs">n = 200
A = randn(n + 1, n + 1)
A = Symmetric(A)
M = Sphere(n);

f1(p) = p&#39; * A&#39;p
gradf1(p) = 2 * (A * p - p * p&#39; * A * p)</code></pre><pre><code class="nohighlight hljs">gradf1 (generic function with 1 method)</code></pre><p>Manifolds provides a finite difference scheme in tangent spaces, that you can introduce to use an existing framework (if the wrapper is implemented) form Euclidean space. Here we use <code>FiniteDiff.jl</code>.</p><pre><code class="language-julia hljs">r_backend = ManifoldDiff.TangentDiffBackend(
    AutoFiniteDifferences(central_fdm(5, 1))
)
gradf1_FD(p) = ManifoldDiff.gradient(M, f1, p, r_backend)

p = zeros(n + 1)
p[1] = 1.0
X1 = gradf1(p)
X2 = gradf1_FD(p)
norm(M, p, X1 - X2)</code></pre><pre><code class="nohighlight hljs">1.0156376260445835e-12</code></pre><p>We obtain quite a good approximation of the gradient.</p><h2 id="EmbeddedGradient"><a class="docs-heading-anchor" href="#EmbeddedGradient">2. Conversion of a Euclidean gradient in the embedding to a Riemannian Gradient of a (not Necessarily Isometrically) embedded manifold</a><a id="EmbeddedGradient-1"></a><a class="docs-heading-anchor-permalink" href="#EmbeddedGradient" title="Permalink"></a></h2><p>Let <span>$\tilde f: ‚Ñù^m ‚Üí ‚Ñù$</span> be a function on the embedding of an <span>$n$</span>-dimensional manifold <span>$\mathcal M \subset ‚Ñù^m$</span>and let <span>$f:  \mathcal M ‚Üí ‚Ñù$</span> denote the restriction of <span>$\tilde f$</span> to the manifold <span>$\mathcal M$</span>.</p><p>Since we can use the pushforward of the embedding to also embed the tangent space <span>$T_p\mathcal M$</span>, <span>$p‚àà\mathcal M$</span>, we can similarly obtain the differential <span>$Df(p):  T_p\mathcal M ‚Üí ‚Ñù$</span> by restricting the differential <span>$D\tilde f(p)$</span> to the tangent space.</p><p>If both <span>$T_p\mathcal M$</span> and <span>$T_p‚Ñù^m$</span> have the same inner product, or in other words the manifold is isometrically embedded in <span>$‚Ñù^m$</span> (like for example the sphere <span>$\mathbb S^n\subset‚Ñù^{m+1}$</span>), then this restriction of the differential directly translates to a projection of the gradient</p><p class="math-container">\[\operatorname{grad}f(p) = \operatorname{Proj}_{T_p\mathcal M}(\operatorname{grad} \tilde f(p))\]</p><p>More generally take a change of the metric into account as</p><p class="math-container">\[\langle  \operatorname{Proj}_{T_p\mathcal M}(\operatorname{grad} \tilde f(p)), X \rangle
= Df(p)[X] = g_p(\operatorname{grad}f(p), X)\]</p><p>or in words: we have to change the Riesz representer of the (restricted/projected) differential of <span>$f$</span> (<span>$\tilde f$</span>) to the one with respect to the Riemannian metric. This is done using <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/manifolds/#ManifoldsBase.change_representer-Tuple%7BAbstractManifold%2C%20AbstractMetric%2C%20Any%2C%20Any%7D"><code>change_representer</code></a>.</p><h3 id="A-continued-example"><a class="docs-heading-anchor" href="#A-continued-example">A continued example</a><a id="A-continued-example-1"></a><a class="docs-heading-anchor-permalink" href="#A-continued-example" title="Permalink"></a></h3><p>We continue with the Rayleigh Quotient from before, now just starting with the definition of the Euclidean case in the embedding, the function <span>$F$</span>.</p><pre><code class="language-julia hljs">F(x) = x&#39; * A * x / (x&#39; * x);</code></pre><p>The cost function is the same by restriction</p><pre><code class="language-julia hljs">f2(M, p) = F(p);</code></pre><p>The gradient is now computed combining our gradient scheme with FiniteDifferences.</p><pre><code class="language-julia hljs">function grad_f2_AD(M, p)
    b = Manifolds.RiemannianProjectionBackend(AutoFiniteDifferences(central_fdm(5, 1)))
    return Manifolds.gradient(M, F, p, b)
end
X3 = grad_f2_AD(M, p)
norm(M, p, X1 - X3)</code></pre><pre><code class="nohighlight hljs">1.7224975655660473e-12</code></pre><h3 id="An-example-for-a-non-isometrically-embedded-manifold"><a class="docs-heading-anchor" href="#An-example-for-a-non-isometrically-embedded-manifold">An example for a non-isometrically embedded manifold</a><a id="An-example-for-a-non-isometrically-embedded-manifold-1"></a><a class="docs-heading-anchor-permalink" href="#An-example-for-a-non-isometrically-embedded-manifold" title="Permalink"></a></h3><p>on the manifold <span>$\mathcal P(3)$</span> of symmetric positive definite matrices.</p><p>The following function computes (half) the distance squared (with respect to the linear affine metric) on the manifold <span>$\mathcal P(3)$</span> to the identity matrix <span>$I_3$</span>. Denoting the unit matrix we consider the function</p><p class="math-container">\[    G(q)
    = \frac{1}{2}d^2_{\mathcal P(3)}(q,I_3)
    = \lVert \operatorname{Log}(q) \rVert_F^2,\]</p><p>where <span>$\operatorname{Log}$</span> denotes the matrix logarithm and <span>$\lVert \cdot \rVert_F$</span> is the Frobenius norm. This can be computed for symmetric positive definite matrices by summing the squares of the logarithms of the eigenvalues of <span>$q$</span> and dividing by two:</p><pre><code class="language-julia hljs">G(q) = sum(log.(eigvals(Symmetric(q))) .^ 2) / 2</code></pre><pre><code class="nohighlight hljs">G (generic function with 1 method)</code></pre><p>We can also interpret this as a function on the space of matrices and apply the Euclidean finite differences machinery; in this way we can easily derive the Euclidean gradient. But when computing the Riemannian gradient, we have to change the representer (see again <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/manifolds/#ManifoldsBase.change_representer-Tuple%7BAbstractManifold%2C%20AbstractMetric%2C%20Any%2C%20Any%7D"><code>change_representer</code></a>) after projecting onto the tangent space <span>$T_p\mathcal P(n)$</span> at <span>$p$</span>.</p><p>Let‚Äôs first define a point and the manifold <span>$N=\mathcal P(3)$</span>.</p><pre><code class="language-julia hljs">rotM(Œ±) = [1.0 0.0 0.0; 0.0 cos(Œ±) sin(Œ±); 0.0 -sin(Œ±) cos(Œ±)]
q = rotM(œÄ / 6) * [1.0 0.0 0.0; 0.0 2.0 0.0; 0.0 0.0 3.0] * transpose(rotM(œÄ / 6))
N = SymmetricPositiveDefinite(3)
is_point(N, q)</code></pre><pre><code class="nohighlight hljs">true</code></pre><p>We could first just compute the gradient using <code>FiniteDifferences.jl</code>, but this yields the Euclidean gradient:</p><pre><code class="language-julia hljs">FiniteDifferences.grad(central_fdm(5, 1), G, q)</code></pre><pre><code class="nohighlight hljs">([3.240417492806275e-14 -2.3531899864903462e-14 0.0; 0.0 0.3514812167654708 0.017000516835452926; 0.0 0.0 0.36129646973723023],)</code></pre><p>Instead, we use the <a href="https://juliamanifolds.github.io/ManifoldDiff.jl/stable/library/#ManifoldDiff.RiemannianProjectionBackend"><code>RiemannianProjectedBackend</code></a> of <a href="https://juliamanifolds.github.io/ManifoldDiff.jl/stable/"><code>ManifoldDiff.jl</code></a>, which in this case internally uses <code>FiniteDifferences.jl</code> to compute a Euclidean gradient but then uses the conversion explained before to derive the Riemannian gradient.</p><p>We define this here again as a function <code>grad_G_FD</code> that could be used in the <code>Manopt.jl</code> framework within a gradient based optimization.</p><pre><code class="language-julia hljs">function grad_G_FD(N, q)
    return Manifolds.gradient(
        N,
        G,
        q,
        ManifoldDiff.RiemannianProjectionBackend(AutoFiniteDifferences(central_fdm(5, 1))),
    )
end
G1 = grad_G_FD(N, q)</code></pre><pre><code class="nohighlight hljs">3√ó3 Matrix{Float64}:
  3.24042e-14  -2.64734e-14  -5.09481e-15
 -2.64734e-14   1.86368       0.826856
 -5.09481e-15   0.826856      2.81845</code></pre><p>Now, we can again compare this to the (known) solution of the gradient, namely the gradient of (half of) the distance squared <span>$G(q) = \frac{1}{2}d^2_{\mathcal P(3)}(q,I_3)$</span> is given by <span>$\operatorname{grad} G(q) = -\operatorname{log}_q I_3$</span>, where <span>$\operatorname{log}$</span> is th <a href="https://juliamanifolds.github.io/Manifolds.jl/stable/manifolds/symmetricpositivedefinite/#Base.log-Tuple%7BSymmetricPositiveDefinite%2C%20Vararg%7BAny%7D%7D">logarithmic map</a> on the manifold.</p><pre><code class="language-julia hljs">G2 = -log(N, q, Matrix{Float64}(I, 3, 3))</code></pre><pre><code class="nohighlight hljs">3√ó3 Matrix{Float64}:
 -0.0  -0.0       -0.0
 -0.0   1.86368    0.826856
 -0.0   0.826856   2.81845</code></pre><p>Both terms agree up to <span>$1.8√ó10^{-12}$</span>:</p><pre><code class="language-julia hljs">norm(G1 - G2)
isapprox(M, q, G1, G2; atol=2 * 1e-12)</code></pre><pre><code class="nohighlight hljs">true</code></pre><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>This tutorial illustrates how to use tools from Euclidean spaces, finite differences or automatic differentiation, to compute gradients on Riemannian manifolds. The scheme allows to use <em>any</em> differentiation framework within the embedding to derive a Riemannian gradient.</p><h2 id="Technical-details"><a class="docs-heading-anchor" href="#Technical-details">Technical details</a><a id="Technical-details-1"></a><a class="docs-heading-anchor-permalink" href="#Technical-details" title="Permalink"></a></h2><p>This tutorial is cached. It was last run on the following package versions.</p><pre><code class="nohighlight hljs">Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`
  [47edcb42] ADTypes v1.21.0
  [6e4b80f9] BenchmarkTools v1.6.3
  [5ae59095] Colors v0.13.1
  [31c24e10] Distributions v0.25.123
  [26cc04aa] FiniteDifferences v0.12.33
  [7073ff75] IJulia v1.34.2
  [8ac3fa9e] LRUCache v1.6.2
  [af67fdf4] ManifoldDiff v0.4.5
  [1cead3c2] Manifolds v0.11.12
  [3362f125] ManifoldsBase v2.3.0
  [0fc0a36d] Manopt v0.5.32 `.`
  [91a5bcdd] Plots v1.41.5
  [731186ca] RecursiveArrayTools v3.47.0
  [37e2e46d] LinearAlgebra v1.12.0
  [9a3f8284] Random v1.11.0</code></pre><p>This tutorial was last rendered February 11, 2026, 10:29:17.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../InplaceGradient/">¬´ Speedup using in-place computations</a><a class="docs-footer-nextpage" href="../EmbeddingObjectives/">Define objectives in the embedding ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 18 February 2026 18:03">Wednesday 18 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
