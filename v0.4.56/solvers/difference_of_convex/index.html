<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Difference of Convex ¬∑ Manopt.jl</title><meta name="title" content="Difference of Convex ¬∑ Manopt.jl"/><meta property="og:title" content="Difference of Convex ¬∑ Manopt.jl"/><meta property="twitter:title" content="Difference of Convex ¬∑ Manopt.jl"/><meta name="description" content="Documentation for Manopt.jl."/><meta property="og:description" content="Documentation for Manopt.jl."/><meta property="twitter:description" content="Documentation for Manopt.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/citations.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Manopt.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../about/">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../../tutorials/Optimize/">üèîÔ∏è Get started: optimize.</a></li><li><a class="tocitem" href="../../tutorials/InplaceGradient/">Speedup using in-place computations</a></li><li><a class="tocitem" href="../../tutorials/AutomaticDifferentiation/">Use automatic differentiation</a></li><li><a class="tocitem" href="../../tutorials/EmbeddingObjectives/">Define objectives in the embedding</a></li><li><a class="tocitem" href="../../tutorials/CountAndCache/">Count and use a cache</a></li><li><a class="tocitem" href="../../tutorials/HowToDebug/">Print debug output</a></li><li><a class="tocitem" href="../../tutorials/HowToRecord/">Record values</a></li><li><a class="tocitem" href="../../tutorials/ImplementASolver/">Implement a solver</a></li><li><a class="tocitem" href="../../tutorials/ImplementOwnManifold/">Optimize on your own manifold</a></li><li><a class="tocitem" href="../../tutorials/ConstrainedOptimization/">Do constrained optimization</a></li><li><a class="tocitem" href="../../tutorials/GeodesicRegression/">Do geodesic regression</a></li></ul></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../adaptive-regularization-with-cubics/">Adaptive Regularization with Cubics</a></li><li><a class="tocitem" href="../alternating_gradient_descent/">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../augmented_Lagrangian_method/">Augmented Lagrangian Method</a></li><li><a class="tocitem" href="../ChambollePock/">Chambolle-Pock</a></li><li><a class="tocitem" href="../conjugate_gradient_descent/">Conjugate gradient descent</a></li><li><a class="tocitem" href="../convex_bundle_method/">Convex bundle method</a></li><li><a class="tocitem" href="../cyclic_proximal_point/">Cyclic Proximal Point</a></li><li class="is-active"><a class="tocitem" href>Difference of Convex</a><ul class="internal"><li><a class="tocitem" href="#solver-difference-of-convex"><span>Difference of convex algorithm</span></a></li><li><a class="tocitem" href="#solver-difference-of-convex-proximal-point"><span>Difference of convex proximal point</span></a></li><li><a class="tocitem" href="#Solver-states"><span>Solver states</span></a></li><li><a class="tocitem" href="#The-difference-of-convex-objective"><span>The difference of convex objective</span></a></li><li><a class="tocitem" href="#Helper-functions"><span>Helper functions</span></a></li><li><a class="tocitem" href="#sec-cp-technical-details"><span>Technical details</span></a></li><li><a class="tocitem" href="#Literature"><span>Literature</span></a></li></ul></li><li><a class="tocitem" href="../DouglasRachford/">Douglas‚ÄîRachford</a></li><li><a class="tocitem" href="../exact_penalty_method/">Exact Penalty Method</a></li><li><a class="tocitem" href="../FrankWolfe/">Frank-Wolfe</a></li><li><a class="tocitem" href="../gradient_descent/">Gradient Descent</a></li><li><a class="tocitem" href="../LevenbergMarquardt/">Levenberg‚ÄìMarquardt</a></li><li><a class="tocitem" href="../NelderMead/">Nelder‚ÄìMead</a></li><li><a class="tocitem" href="../particle_swarm/">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../primal_dual_semismooth_Newton/">Primal-dual Riemannian semismooth Newton</a></li><li><a class="tocitem" href="../proximal_bundle_method/">Proximal bundle method</a></li><li><a class="tocitem" href="../quasi_Newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../stochastic_gradient_descent/">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../subgradient/">Subgradient method</a></li><li><a class="tocitem" href="../truncated_conjugate_gradient_descent/">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../trust_regions/">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Plans</span><ul><li><a class="tocitem" href="../../plans/">Specify a Solver</a></li><li><a class="tocitem" href="../../plans/problem/">Problem</a></li><li><a class="tocitem" href="../../plans/objective/">Objective</a></li><li><a class="tocitem" href="../../plans/state/">Solver State</a></li><li><a class="tocitem" href="../../plans/stepsize/">Stepsize</a></li><li><a class="tocitem" href="../../plans/stopping_criteria/">Stopping Criteria</a></li><li><a class="tocitem" href="../../plans/debug/">Debug Output</a></li><li><a class="tocitem" href="../../plans/record/">Recording values</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../../helpers/checks/">Checks</a></li><li><a class="tocitem" href="../../helpers/exports/">Exports</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../../extensions/">Extensions</a></li><li><a class="tocitem" href="../../notation/">Notation</a></li><li><a class="tocitem" href="../../changelog/">Changelog</a></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solvers</a></li><li class="is-active"><a href>Difference of Convex</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Difference of Convex</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/Manopt.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/solvers/difference_of_convex.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Difference-of-convex"><a class="docs-heading-anchor" href="#Difference-of-convex">Difference of convex</a><a id="Difference-of-convex-1"></a><a class="docs-heading-anchor-permalink" href="#Difference-of-convex" title="Permalink"></a></h1><h2 id="solver-difference-of-convex"><a class="docs-heading-anchor" href="#solver-difference-of-convex">Difference of convex algorithm</a><a id="solver-difference-of-convex-1"></a><a class="docs-heading-anchor-permalink" href="#solver-difference-of-convex" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.difference_of_convex_algorithm" href="#Manopt.difference_of_convex_algorithm"><code>Manopt.difference_of_convex_algorithm</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">difference_of_convex_algorithm(M, f, g, ‚àÇh, p=rand(M); kwargs...)
difference_of_convex_algorithm(M, mdco, p; kwargs...)</code></pre><p>Compute the difference of convex algorithm [<a href="../../references/#BergmannFerreiraSantosSouza:2023">BFSS23</a>] to minimize</p><p class="math-container">\[    \operatorname*{arg\,min}_{p‚àà\mathcal M}\  g(p) - h(p)\]</p><p>where you need to provide <span>$f(p) = g(p) - h(p)$</span>, <span>$g$</span> and the subdifferential <span>$‚àÇh$</span> of <span>$h$</span>.</p><p>This algorithm performs the following steps given a start point <code>p</code>= <span>$p^{(0)}$</span>. Then repeat for <span>$k=0,1,\ldots$</span></p><ol><li>Take <span>$X^{(k)}  ‚àà ‚àÇh(p^{(k)})$</span></li><li>Set the next iterate to the solution of the subproblem</li></ol><p class="math-container">\[  p^{(k+1)} ‚àà \operatorname*{argmin}_{q ‚àà \mathcal M} g(q) - ‚ü®X^{(k)}, \log_{p^{(k)}}q‚ü©\]</p><p>until the <code>stopping_criterion</code> is fulfilled.</p><p><strong>Optional parameters</strong></p><ul><li><code>evaluation</code>          (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient works by allocation (default) form <code>grad_f(M, p)</code> or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> form <code>grad_f!(M, X, x)</code></li><li><code>gradient</code>            (<code>nothing</code>) specify <span>$\operatorname{grad} f$</span>, for debug / analysis or enhancing <code>stopping_criterion=</code></li><li><code>grad_g</code>              (<code>nothing</code>) specify the gradient of <code>g</code>. If specified, a subsolver is automatically set up.</li><li><code>initial_vector</code>      (<code>zero_vector(M, p)</code>) initialise the inner tangent vector to store the subgradient result.</li><li><code>stopping_criterion</code>  (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(200) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenChangeLess"><code>StopWhenChangeLess</code></a><code>(1e-8)</code>) a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> for the algorithm. This includes a <a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(1e-8)</code>, when a <code>gradient</code> is provided.</li></ul><p>if you specify the <a href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>ManifoldDifferenceOfConvexObjective</code></a> <code>mdco</code>, additionally</p><ul><li><code>g</code>                   - (<code>nothing</code>) specify the function <code>g</code> If specified, a subsolver is automatically set up.</li></ul><p>While there are several parameters for a sub solver, the easiest is to provide the function <code>grad_g=</code>, such that together with the mandatory function <code>g</code> a default cost and gradient can be generated and passed to a default subsolver. Hence the easiest example call looks like</p><pre><code class="nohighlight hljs">difference_of_convex_algorithm(M, f, g, grad_h, p; grad_g=grad_g)</code></pre><p><strong>Optional parameters for the sub problem</strong></p><ul><li><code>sub_cost</code>              (<a href="#Manopt.LinearizedDCCost"><code>LinearizedDCCost</code></a><code>(g, p, initial_vector)</code>) a cost to be used within the default <code>sub_problem</code> Use this if you have a more efficient version than the default that is built using <code>g</code> from before.</li><li><code>sub_grad</code>              (<a href="#Manopt.LinearizedDCGrad"><code>LinearizedDCGrad</code></a><code>(grad_g, p, initial_vector; evaluation=evaluation)</code> gradient to be used within the default <code>sub_problem</code>. This is generated by default when <code>grad_g</code> is provided. You can specify your own by overwriting this keyword.</li><li><code>sub_hess</code>              (a finite difference approximation by default) specify a Hessian  of the subproblem, which the default solver, see <code>sub_state</code> needs</li><li><code>sub_kwargs</code>            (<code>(;)</code>) pass keyword arguments to the <code>sub_state</code>, in form of a <code>Dict(:kwname=&gt;value)</code>, unless you set the <code>sub_state</code> directly.</li><li><code>sub_objective</code>         (a gradient or Hessian objective based on the last 3 keywords) provide the objective used within <code>sub_problem</code> (if that is not specified by the user)</li><li><code>sub_problem</code>           (<a href="../../plans/problem/#Manopt.DefaultManoptProblem"><code>DefaultManoptProblem</code></a><code>(M, sub_objective)</code> specify a manopt problem for the sub-solver runs. You can also provide a function for a closed form solution. Then <code>evaluation=</code> is taken into account for the form of this function.</li><li><code>sub_state</code>             (<a href="../trust_regions/#Manopt.TrustRegionsState"><code>TrustRegionsState</code></a> by default, requires <code>sub_hessian</code> to be provided; decorated with <code>sub_kwargs</code>). Choose the solver by specifying a solver state to solve the <code>sub_problem</code> if the <code>sub_problem</code> if a function (a closed form solution), this is set to <code>evaluation</code> and can be changed to the evaluation type of the closed form solution accordingly.</li><li><code>sub_stopping_criterion</code> (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(300) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenStepsizeLess"><code>StopWhenStepsizeLess</code></a><code>(1e-9) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(1e-9)</code>) a stopping criterion used withing the default <code>sub_state=</code></li><li><code>sub_stepsize</code>           (<a href="../../plans/stepsize/#Manopt.ArmijoLinesearch"><code>ArmijoLinesearch</code></a><code>(M)</code>) specify a step size used within the <code>sub_state</code>,</li></ul><p>all others are passed on to decorate the inner <a href="#Manopt.DifferenceOfConvexState"><code>DifferenceOfConvexState</code></a>.</p><p><strong>Output</strong></p><p>the obtained (approximate) minimizer <span>$p^*$</span>, see <a href="../#Manopt.get_solver_return"><code>get_solver_return</code></a> for details</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/solvers/difference_of_convex_algorithm.jl#L112-L187">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.difference_of_convex_algorithm!" href="#Manopt.difference_of_convex_algorithm!"><code>Manopt.difference_of_convex_algorithm!</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">difference_of_convex_algorithm!(M, f, g, ‚àÇh, p; kwargs...)
difference_of_convex_algorithm!(M, mdco, p; kwargs...)</code></pre><p>Run the difference of convex algorithm and perform the steps in place of <code>p</code>. See <a href="#Manopt.difference_of_convex_algorithm"><code>difference_of_convex_algorithm</code></a> for more details.</p><p>if you specify the <a href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>ManifoldDifferenceOfConvexObjective</code></a> <code>mdco</code>, the <code>g</code> is a keyword argument.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/solvers/difference_of_convex_algorithm.jl#L246-L255">source</a></section></article><h2 id="solver-difference-of-convex-proximal-point"><a class="docs-heading-anchor" href="#solver-difference-of-convex-proximal-point">Difference of convex proximal point</a><a id="solver-difference-of-convex-proximal-point-1"></a><a class="docs-heading-anchor-permalink" href="#solver-difference-of-convex-proximal-point" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.difference_of_convex_proximal_point" href="#Manopt.difference_of_convex_proximal_point"><code>Manopt.difference_of_convex_proximal_point</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">difference_of_convex_proximal_point(M, grad_h, p=rand(M); kwargs...)
difference_of_convex_proximal_point(M, mdcpo, p=rand(M); kwargs...)</code></pre><p>Compute the difference of convex proximal point algorithm [<a href="../../references/#SouzaOliveira:2015">SO15</a>] to minimize</p><p class="math-container">\[    \operatorname*{arg\,min}_{p‚àà\mathcal M} g(p) - h(p)\]</p><p>where you have to provide the (sub) gradient <span>$‚àÇh$</span> of <span>$h$</span> and either</p><ul><li>the proximal map <span>$\operatorname{prox}_{\lambda g}$</span> of <code>g</code> as a function <code>prox_g(M, Œª, p)</code> or  <code>prox_g(M, q, Œª, p)</code></li><li>the functions <code>g</code> and <code>grad_g</code> to compute the proximal map using a sub solver</li><li>your own sub-solver, see optional keywords below</li></ul><p>This algorithm performs the following steps given a start point <code>p</code>= <span>$p^{(0)}$</span>. Then repeat for <span>$k=0,1,\ldots$</span></p><ol><li><span>$X^{(k)}  ‚àà \operatorname{grad} h(p^{(k)})$</span></li><li><span>$q^{(k)} = \operatorname{retr}_{p^{(k)}}(Œª_kX^{(k)})$</span></li><li><span>$r^{(k)} = \operatorname{prox}_{Œª_kg}(q^{(k)})$</span></li><li><span>$X^{(k)} = \operatorname{retr}^{-1}_{p^{(k)}}(r^{(k)})$</span></li><li>Compute a stepsize <span>$s_k$</span> and</li><li>set <span>$p^{(k+1)} = \operatorname{retr}_{p^{(k)}}(s_kX^{(k)})$</span>.</li></ol><p>until the <code>stopping_criterion</code> is fulfilled. See [<a href="../../references/#AlmeidaNetoOliveiraSouza:2020">ACOO20</a>] for more details on the modified variant, where steps 4-6 are slightly changed, since here the classical proximal point method for DC functions is obtained for <span>$s_k = 1$</span> and one can hence employ usual line search method.</p><p><strong>Optional parameters</strong></p><ul><li><code>Œª</code>:                          ( <code>i -&gt; 1/2</code> ) a function returning the sequence of prox parameters Œªi</li><li><code>evaluation</code>:                 (<a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a>) specify whether the gradient works by allocation (default) form <code>gradF(M, x)</code> or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> in place of the form <code>gradF!(M, X, x)</code>.</li><li><code>cost</code>:                       (<code>nothing</code>) provide the cost <code>f</code>, for debug reasons / analysis the default <code>sub_problem</code>. Use this if you have a more efficient version than using <code>g</code> from before.</li><li><code>gradient</code>:                   (<code>nothing</code>) specify <span>$\operatorname{grad} f$</span>, for debug / analysis  or enhancing the <code>stopping_criterion</code></li><li><code>prox_g</code>:                     (<code>nothing</code>) specify a proximal map for the sub problem <em>or</em> both of the following</li><li><code>g</code>:                          (<code>nothing</code>) specify the function <code>g</code>.</li><li><code>grad_g</code>:                     (<code>nothing</code>) specify the gradient of <code>g</code>. If both <code>g</code>and <code>grad_g</code> are specified, a subsolver is automatically set up.</li><li><code>inverse_retraction_method</code>:  (<code>default_inverse_retraction_method(M)</code>) an inverse retraction method to use (see step 4).</li><li><code>retraction_method</code>:          (<code>default_retraction_method(M)</code>) a retraction to use (see step 2)</li><li><code>stepsize</code>:                   (<a href="../../plans/stepsize/#Manopt.ConstantStepsize"><code>ConstantStepsize</code></a><code>(M)</code>) specify a <a href="../../plans/stepsize/#Stepsize"><code>Stepsize</code></a> to run the modified algorithm (experimental.) functor.</li><li><code>stopping_criterion</code>:         (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(200) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenChangeLess"><code>StopWhenChangeLess</code></a><code>(1e-8)</code>) a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> for the algorithm, also includes a <a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(1e-8)</code>, when a <code>gradient</code> is provided.</li></ul><p>While there are several parameters for a sub solver, the easiest is to provide the function <code>g</code> and <code>grad_g</code>, such that together with the mandatory function <code>g</code> a default cost and gradient can be generated and passed to a default subsolver. Hence the easiest example call looks like</p><pre><code class="nohighlight hljs">difference_of_convex_proximal_point(M, grad_h, p0; g=g, grad_g=grad_g)</code></pre><p><strong>Optional parameters for the sub problem</strong></p><ul><li><code>sub_cost</code>:               (<a href="#Manopt.ProximalDCCost"><code>ProximalDCCost</code></a><code>(g, copy(M, p), Œª(1))</code>) cost to be used within the default <code>sub_problem</code> that is initialized as soon as <code>g</code> is provided.</li><li><code>sub_grad</code>:               (<a href="#Manopt.ProximalDCGrad"><code>ProximalDCGrad</code></a><code>(grad_g, copy(M, p), Œª(1); evaluation=evaluation)</code> gradient to be used within the default <code>sub_problem</code>, that is initialized as soon as <code>grad_g</code> is provided. This is generated by default when <code>grad_g</code> is provided. You can specify your own by overwriting this keyword.</li><li><code>sub_hess</code>:               (a finite difference approximation by default) specify a Hessian of the subproblem, which the default solver, see <code>sub_state</code> needs</li><li><code>sub_kwargs</code>:             (<code>(;)</code>) pass keyword arguments to the <code>sub_state</code>, in form of a <code>Dict(:kwname=&gt;value)</code>, unless you set the <code>sub_state</code> directly.</li><li><code>sub_objective</code>:          (a gradient or Hessian objective based on the last 3 keywords) provide the objective used within <code>sub_problem</code> (if that is not specified by the user)</li><li><code>sub_problem</code>:            (<a href="../../plans/problem/#Manopt.DefaultManoptProblem"><code>DefaultManoptProblem</code></a><code>(M, sub_objective)</code> specify a manopt problem for the sub-solver runs. You can also provide a function for a closed form solution. Then <code>evaluation=</code> is taken into account for the form of this function.</li><li><code>sub_state</code>:              (<a href="../trust_regions/#Manopt.TrustRegionsState"><code>TrustRegionsState</code></a>). requires the <code>sub_Hessian to be provided,  decorated with</code>sub<em>kwargs<code>) choose the solver by specifying a solver state to solve the</code>sub</em>problem`</li><li><code>sub_stopping_criterion</code>: (<a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(300) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenStepsizeLess"><code>StopWhenStepsizeLess</code></a><code>(1e-9) |</code><a href="../../plans/stopping_criteria/#Manopt.StopWhenGradientNormLess"><code>StopWhenGradientNormLess</code></a><code>(1e-9)</code>) a stopping criterion used withing the default <code>sub_state=</code></li></ul><p>all others are passed on to decorate the inner <a href="#Manopt.DifferenceOfConvexProximalState"><code>DifferenceOfConvexProximalState</code></a>.</p><p><strong>Output</strong></p><p>the obtained (approximate) minimizer <span>$p^*$</span>, see <a href="../#Manopt.get_solver_return"><code>get_solver_return</code></a> for details</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/solvers/difference-of-convex-proximal-point.jl#L133-L216">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.difference_of_convex_proximal_point!" href="#Manopt.difference_of_convex_proximal_point!"><code>Manopt.difference_of_convex_proximal_point!</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">difference_of_convex_proximal_point!(M, grad_h, p; cost=nothing, kwargs...)
difference_of_convex_proximal_point!(M, mdcpo, p; cost=nothing, kwargs...)
difference_of_convex_proximal_point!(M, mdcpo, prox_g, p; cost=nothing, kwargs...)</code></pre><p>Compute the difference of convex algorithm to minimize</p><p class="math-container">\[    \operatorname*{arg\,min}_{p‚àà\mathcal M} g(p) - h(p)\]</p><p>where you have to provide the proximal map of <code>g</code> and the gradient of <code>h</code>.</p><p>The computation is done in-place of <code>p</code>.</p><p>For all further details, especially the keyword arguments, see <a href="#Manopt.difference_of_convex_proximal_point"><code>difference_of_convex_proximal_point</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/solvers/difference-of-convex-proximal-point.jl#L281-L297">source</a></section></article><h2 id="Solver-states"><a class="docs-heading-anchor" href="#Solver-states">Solver states</a><a id="Solver-states-1"></a><a class="docs-heading-anchor-permalink" href="#Solver-states" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.DifferenceOfConvexState" href="#Manopt.DifferenceOfConvexState"><code>Manopt.DifferenceOfConvexState</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferenceOfConvexState{Pr,St,P,T,SC&lt;:StoppingCriterion} &lt;:
           AbstractManoptSolverState</code></pre><p>A struct to store the current state of the [<code>difference_of_convex_algorithm</code>])(@ref). It comes in two forms, depending on the realisation of the <code>subproblem</code>.</p><p><strong>Fields</strong></p><ul><li><code>p</code>           the current iterate, a point on the manifold</li><li><code>X</code>           the current subgradient, a tangent vector to <code>p</code>.</li><li><code>sub_problem</code> problem for the subsolver</li><li><code>sub_state</code>   state of the subproblem</li><li><code>stop</code>        a functor inheriting from <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> indicating when to stop.</li></ul><p>For the sub task, a method to solve</p><p class="math-container">\[    \operatorname*{argmin}_{q‚àà\mathcal M}\ g(p) - ‚ü®X, \log_p q‚ü©\]</p><p>is needed. Besides a problem and options, one can also provide a function and an <a href="../../plans/objective/#Manopt.AbstractEvaluationType"><code>AbstractEvaluationType</code></a>, respectively, to indicate a closed form solution for the sub task.</p><p><strong>Constructors</strong></p><pre><code class="nohighlight hljs">DifferenceOfConvexState(M, p, sub_problem, sub_state; kwargs...)
DifferenceOfConvexState(M, p, sub_solver; evaluation=InplaceEvaluation(), kwargs...)</code></pre><p>Generate the state either using a solver from Manopt, given by an <a href="../../plans/problem/#Manopt.AbstractManoptProblem"><code>AbstractManoptProblem</code></a> <code>sub_problem</code> and an <a href="../../plans/state/#Manopt.AbstractManoptSolverState"><code>AbstractManoptSolverState</code></a> <code>sub_state</code>, or a closed form solution <code>sub_solver</code> for the sub-problem the function expected to be of the form <code>(M, p, X) -&gt; q</code> or <code>(M, q, p, X) -&gt; q</code>, where by default its <a href="../../plans/objective/#Manopt.AbstractEvaluationType"><code>AbstractEvaluationType</code></a> <code>evaluation</code> is in-place of <code>q</code>. Here the elements passed are the current iterate <code>p</code> and the subgradient <code>X</code> of <code>h</code> can be passed to that function.</p><p><strong>further keyword arguments</strong></p><ul><li><code>initial_vector=zero_vector</code> (<code>zero_vectoir(M,p)</code>) how to initialize the inner gradient tangent vector</li><li><code>stopping_criterion</code>         a <a href="../../plans/stopping_criteria/#Manopt.StopAfterIteration"><code>StopAfterIteration</code></a><code>(200)</code> a stopping criterion</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/solvers/difference_of_convex_algorithm.jl#L2-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.DifferenceOfConvexProximalState" href="#Manopt.DifferenceOfConvexProximalState"><code>Manopt.DifferenceOfConvexProximalState</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferenceOfConvexProximalState{Type} &lt;: Options</code></pre><p>A struct to store the current state of the algorithm as well as the form. It comes in two forms, depending on the realisation of the <code>subproblem</code>.</p><p><strong>Fields</strong></p><ul><li><code>inverse_retraction_method</code>: (<code>default_inverse_retraction_method(M)</code>) an inverse retraction method to use within Frank Wolfe.</li><li><code>retraction_method</code>:         (<code>default_retraction_method(M)</code>) a type of retraction</li><li><code>p</code>, <code>q</code>, <code>r</code>:               the current iterate, the gradient step and the prox, respectively their type is set by initializing <code>p</code></li><li><code>stepsize</code>:                  (<a href="../../plans/stepsize/#Manopt.ConstantStepsize"><code>ConstantStepsize</code></a><code>(1.0)</code>) a <a href="../../plans/stepsize/#Stepsize"><code>Stepsize</code></a> function to run the modified algorithm (experimental)</li><li><code>stop</code>:                      (<a href="../../plans/stopping_criteria/#Manopt.StopWhenChangeLess"><code>StopWhenChangeLess</code></a><code>(1e-8)</code>) a <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a></li><li><code>X</code>, <code>Y</code>:                    (<code>zero_vector(M,p)</code>) the current gradient and descent direction, respectively their common type is set by the keyword <code>X</code></li></ul><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">DifferenceOfConvexProximalState(M, p; kwargs...)</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>X</code>, <code>retraction_method</code>, <code>inverse_retraction_method</code>, <code>stepsize</code> for the corresponding fields</li><li><code>stoppping_criterion</code> for the <a href="../../plans/stopping_criteria/#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/solvers/difference-of-convex-proximal-point.jl#L2-L26">source</a></section></article><h2 id="The-difference-of-convex-objective"><a class="docs-heading-anchor" href="#The-difference-of-convex-objective">The difference of convex objective</a><a id="The-difference-of-convex-objective-1"></a><a class="docs-heading-anchor-permalink" href="#The-difference-of-convex-objective" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.ManifoldDifferenceOfConvexObjective" href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>Manopt.ManifoldDifferenceOfConvexObjective</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ManifoldDifferenceOfConvexObjective{E} &lt;: AbstractManifoldCostObjective{E}</code></pre><p>Specify an objective for a <a href="#Manopt.difference_of_convex_algorithm"><code>difference_of_convex_algorithm</code></a>.</p><p>The objective <span>$f: \mathcal M ‚Üí ‚Ñù$</span> is given as</p><p class="math-container">\[    f(p) = g(p) - h(p)\]</p><p>where both <span>$g$</span> and <span>$h$</span> are convex, lower semicontinuous and proper. Furthermore the subdifferential <span>$‚àÇh$</span> of <span>$h$</span> is required.</p><p><strong>Fields</strong></p><ul><li><code>cost</code>: an implementation of <span>$f(p) = g(p)-h(p)$</span> as a function <code>f(M,p)</code>.</li><li><code>‚àÇh!!</code>: a deterministic version of <span>$‚àÇh: \mathcal M ‚Üí T\mathcal M$</span>, in the sense that calling <code>‚àÇh(M, p)</code> returns a subgradient of <span>$h$</span> at <code>p</code> and if there is more than one, it returns a deterministic choice.</li></ul><p>Note that the subdifferential might be given in two possible signatures</p><ul><li><code>‚àÇh(M,p)</code> which does an <a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a></li><li><code>‚àÇh!(M, X, p)</code> which does an <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> in place of <code>X</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/plans/difference_of_convex_plan.jl#L1-L26">source</a></section></article><p>as well as for the corresponding sub problem</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.LinearizedDCCost" href="#Manopt.LinearizedDCCost"><code>Manopt.LinearizedDCCost</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LinearizedDCCost</code></pre><p>A functor <code>(M,q) ‚Üí ‚Ñù</code> to represent the inner problem of a <a href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>ManifoldDifferenceOfConvexObjective</code></a>. This is a cost function of the form</p><p class="math-container">\[    F_{p_k,X_k}(p) = g(p) - ‚ü®X_k, \log_{p_k}p‚ü©\]</p><p>for a point <code>p_k</code> and a tangent vector <code>X_k</code> at <code>p_k</code> (for example outer iterates) that are stored within this functor as well.</p><p><strong>Fields</strong></p><ul><li><code>g</code> a function</li><li><code>pk</code> a point on a manifold</li><li><code>Xk</code> a tangent vector at <code>pk</code></li></ul><p>Both interim values can be set using <code>set_manopt_parameter!(::LinearizedDCCost, ::Val{:p}, p)</code> and <code>set_manopt_parameter!(::LinearizedDCCost, ::Val{:X}, X)</code>, respectively.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">LinearizedDCCost(g, p, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/plans/difference_of_convex_plan.jl#L96-L120">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.LinearizedDCGrad" href="#Manopt.LinearizedDCGrad"><code>Manopt.LinearizedDCGrad</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LinearizedDCGrad</code></pre><p>A functor <code>(M,X,p) ‚Üí ‚Ñù</code> to represent the gradient of the inner problem of a <a href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>ManifoldDifferenceOfConvexObjective</code></a>. This is a gradient function of the form</p><p class="math-container">\[    F_{p_k,X_k}(p) = g(p) - ‚ü®X_k, \log_{p_k}p‚ü©\]</p><p>its gradient is given by using <span>$F=F_1(F_2(p))$</span>, where <span>$F_1(X) = ‚ü®X_k,X‚ü©$</span> and <span>$F_2(p) = \log_{p_k}p$</span> and the chain rule as well as the adjoint differential of the logarithmic map with respect to its argument for <span>$D^*F_2(p)$</span></p><p class="math-container">\[    \operatorname{grad} F(q) = \operatorname{grad} f(q) - DF_2^*(q)[X]\]</p><p>for a point <code>pk</code> and a tangent vector <code>Xk</code> at <code>pk</code> (the outer iterates) that are stored within this functor as well</p><p><strong>Fields</strong></p><ul><li><code>grad_g!!</code> the gradient of <span>$g$</span> (see also <a href="#Manopt.LinearizedDCCost"><code>LinearizedDCCost</code></a>)</li><li><code>pk</code> a point on a manifold</li><li><code>Xk</code> a tangent vector at <code>pk</code></li></ul><p>Both interim values can be set using <code>set_manopt_parameter!(::LinearizedDCGrad, ::Val{:p}, p)</code> and <code>set_manopt_parameter!(::LinearizedDCGrad, ::Val{:X}, X)</code>, respectively.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">LinearizedDCGrad(grad_g, p, X; evaluation=AllocatingEvaluation())</code></pre><p>Where you specify whether <code>grad_g</code> is <a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a> or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a>, while this function still provides <em>both</em> signatures.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/plans/difference_of_convex_plan.jl#L137-L171">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.ManifoldDifferenceOfConvexProximalObjective" href="#Manopt.ManifoldDifferenceOfConvexProximalObjective"><code>Manopt.ManifoldDifferenceOfConvexProximalObjective</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ManifoldDifferenceOfConvexProximalObjective{E} &lt;: Problem</code></pre><p>Specify an objective <a href="#Manopt.difference_of_convex_proximal_point"><code>difference_of_convex_proximal_point</code></a> algorithm. The problem is of the form</p><p class="math-container">\[    \operatorname*{argmin}_{p‚àà\mathcal M} g(p) - h(p)\]</p><p>where both <span>$g$</span> and <span>$h$</span> are convex, lower semicontinuous and proper.</p><p><strong>Fields</strong></p><ul><li><code>cost</code>:     (<code>nothing</code>) implementation of <span>$f(p) = g(p)-h(p)$</span> (optional)</li><li><code>gradient</code>: the gradient of the cost</li><li><code>grad_h!!</code>: a function <span>$\operatorname{grad}h: \mathcal M ‚Üí T\mathcal M$</span>,</li></ul><p>Note that both the gradients might be given in two possible signatures as allocating or in-place.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">ManifoldDifferenceOfConvexProximalObjective(gradh; cost=nothing, gradient=nothing)</code></pre><p>an note that neither cost nor gradient are required for the algorithm, just for eventual debug or stopping criteria.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/plans/difference_of_convex_plan.jl#L214-L241">source</a></section></article><p>as well as for the corresponding sub problems</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.ProximalDCCost" href="#Manopt.ProximalDCCost"><code>Manopt.ProximalDCCost</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ProximalDCCost</code></pre><p>A functor <code>(M, p) ‚Üí ‚Ñù</code> to represent the inner cost function of a <a href="#Manopt.ManifoldDifferenceOfConvexProximalObjective"><code>ManifoldDifferenceOfConvexProximalObjective</code></a>. This is the cost function of the proximal map of <code>g</code>.</p><p class="math-container">\[    F_{p_k}(p) = \frac{1}{2Œª}d_{\mathcal M}(p_k,p)^2 + g(p)\]</p><p>for a point <code>pk</code> and a proximal parameter <span>$Œª$</span>.</p><p><strong>Fields</strong></p><ul><li><code>g</code>  - a function</li><li><code>pk</code> - a point on a manifold</li><li><code>Œª</code>  - the prox parameter</li></ul><p>Both interim values can be set using <code>set_manopt_parameter!(::ProximalDCCost, ::Val{:p}, p)</code> and <code>set_manopt_parameter!(::ProximalDCCost, ::Val{:Œª}, Œª)</code>, respectively.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">ProximalDCCost(g, p, Œª)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/plans/difference_of_convex_plan.jl#L303-L328">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.ProximalDCGrad" href="#Manopt.ProximalDCGrad"><code>Manopt.ProximalDCGrad</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ProximalDCGrad</code></pre><p>A functor <code>(M,X,p) ‚Üí ‚Ñù</code> to represent the gradient of the inner cost function of a <a href="#Manopt.ManifoldDifferenceOfConvexProximalObjective"><code>ManifoldDifferenceOfConvexProximalObjective</code></a>. This is the gradient function of the proximal map cost function of <code>g</code>. Based on</p><p class="math-container">\[    F_{p_k}(p) = \frac{1}{2Œª}d_{\mathcal M}(p_k,p)^2 + g(p)\]</p><p>it reads</p><p class="math-container">\[    \operatorname{grad} F_{p_k}(p) = \operatorname{grad} g(p) - \frac{1}{Œª}\log_p p_k\]</p><p>for a point <code>pk</code> and a proximal parameter <code>Œª</code>.</p><p><strong>Fields</strong></p><ul><li><code>grad_g</code>  - a gradient function</li><li><code>pk</code> - a point on a manifold</li><li><code>Œª</code>  - the prox parameter</li></ul><p>Both interim values can be set using <code>set_manopt_parameter!(::ProximalDCGrad, ::Val{:p}, p)</code> and <code>set_manopt_parameter!(::ProximalDCGrad, ::Val{:Œª}, Œª)</code>, respectively.</p><p><strong>Constructor</strong></p><pre><code class="nohighlight hljs">ProximalDCGrad(grad_g, pk, Œª; evaluation=AllocatingEvaluation())</code></pre><p>Where you specify whether <code>grad_g</code> is <a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a> or <a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a>, while this function still always provides <em>both</em> signatures.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/plans/difference_of_convex_plan.jl#L345-L379">source</a></section></article><h2 id="Helper-functions"><a class="docs-heading-anchor" href="#Helper-functions">Helper functions</a><a id="Helper-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Helper-functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Manopt.get_subtrahend_gradient" href="#Manopt.get_subtrahend_gradient"><code>Manopt.get_subtrahend_gradient</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">X = get_subtrahend_gradient(amp, q)
get_subtrahend_gradient!(amp, X, q)</code></pre><p>Evaluate the (sub)gradient of the subtrahend <code>h</code> from within a <a href="#Manopt.ManifoldDifferenceOfConvexObjective"><code>ManifoldDifferenceOfConvexObjective</code></a> <code>amp</code> at the point <code>q</code> (in place of <code>X</code>).</p><p>The evaluation is done in place of <code>X</code> for the <code>!</code>-variant. The <code>T=</code><a href="../../plans/objective/#Manopt.AllocatingEvaluation"><code>AllocatingEvaluation</code></a> problem might still allocate memory within. When the non-mutating variant is called with a <code>T=</code><a href="../../plans/objective/#Manopt.InplaceEvaluation"><code>InplaceEvaluation</code></a> memory for the result is allocated.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/plans/difference_of_convex_plan.jl#L39-L50">source</a></section><section><div><pre><code class="language-julia hljs">X = get_subtrahend_gradient(M::AbstractManifold, dcpo::ManifoldDifferenceOfConvexProximalObjective, p)
get_subtrahend_gradient!(M::AbstractManifold, X, dcpo::ManifoldDifferenceOfConvexProximalObjective, p)</code></pre><p>Evaluate the gradient of the subtrahend <span>$h$</span> from within a <a href="#Manopt.ManifoldDifferenceOfConvexProximalObjective"><code>ManifoldDifferenceOfConvexProximalObjective</code></a><code></code>P<code>at the point</code>p` (in place of X).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaManifolds/Manopt.jl/blob/a0cb692652c66978b801baab75cb1205297dcc00/src/plans/difference_of_convex_plan.jl#L258-L264">source</a></section></article><h2 id="sec-cp-technical-details"><a class="docs-heading-anchor" href="#sec-cp-technical-details">Technical details</a><a id="sec-cp-technical-details-1"></a><a class="docs-heading-anchor-permalink" href="#sec-cp-technical-details" title="Permalink"></a></h2><p>The <a href="#Manopt.difference_of_convex_algorithm"><code>difference_of_convex_algorithm</code></a> and <a href="#Manopt.difference_of_convex_proximal_point"><code>difference_of_convex_proximal_point</code></a> solver requires the following functions of a manifold to be available</p><ul><li>A <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/retractions/"><code>retract!</code></a><code>(M, q, p, X)</code>; it is recommended to set the <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/retractions/#ManifoldsBase.default_retraction_method-Tuple{AbstractManifold}"><code>default_retraction_method</code></a> to a favourite retraction. If this default is set, a <code>retraction_method=</code> or <code>retraction_method_dual=</code> (for <span>$\mathcal N$</span>) does not have to be specified.</li><li>An <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/retractions/"><code>inverse_retract!</code></a><code>(M, X, p, q)</code>; it is recommended to set the <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/retractions/#ManifoldsBase.default_inverse_retraction_method-Tuple{AbstractManifold}"><code>default_inverse_retraction_method</code></a> to a favourite retraction. If this default is set, a <code>inverse_retraction_method=</code> or <code>inverse_retraction_method_dual=</code> (for <span>$\mathcal N$</span>) does not have to be specified.</li></ul><p>By default, one of the stopping criteria is <a href="../../plans/stopping_criteria/#Manopt.StopWhenChangeLess"><code>StopWhenChangeLess</code></a>, which either requires</p><ul><li>A <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/retractions/"><code>retract!</code></a><code>(M, q, p, X)</code>; it is recommended to set the <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/retractions/#ManifoldsBase.default_retraction_method-Tuple{AbstractManifold}"><code>default_retraction_method</code></a> to a favourite retraction. If this default is set, a <code>retraction_method=</code> or <code>retraction_method_dual=</code> (for <span>$\mathcal N$</span>) does not have to be specified.</li><li>An <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/retractions/"><code>inverse_retract!</code></a><code>(M, X, p, q)</code>; it is recommended to set the <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/retractions/#ManifoldsBase.default_inverse_retraction_method-Tuple{AbstractManifold}"><code>default_inverse_retraction_method</code></a> to a favourite retraction. If this default is set, a <code>inverse_retraction_method=</code> or <code>inverse_retraction_method_dual=</code> (for <span>$\mathcal N$</span>) does not have to be specified or the <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/functions/#ManifoldsBase.distance-Tuple{AbstractManifold,%20Any,%20Any}"><code>distance</code></a><code>(M, p, q)</code> for said default inverse retraction.</li><li>A <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/functions/#Base.copyto!-Tuple{AbstractManifold,%20Any,%20Any}"><code>copyto!</code></a><code>(M, q, p)</code> and <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/functions/#Base.copy-Tuple{AbstractManifold,%20Any}"><code>copy</code></a><code>(M,p)</code> for points.</li><li>By default the tangent vector storing the gradient is initialized calling <a href="https://juliamanifolds.github.io/ManifoldsBase.jl/stable/functions/#ManifoldsBase.zero_vector-Tuple{AbstractManifold,%20Any}"><code>zero_vector</code></a><code>(M,p)</code>.</li><li>everything the subsolver requires, which by default is the <a href="../trust_regions/#Manopt.trust_regions"><code>trust_regions</code></a> or if you do not provide a Hessian <a href="../gradient_descent/#Manopt.gradient_descent"><code>gradient_descent</code></a>.</li></ul><h2 id="Literature"><a class="docs-heading-anchor" href="#Literature">Literature</a><a id="Literature-1"></a><a class="docs-heading-anchor-permalink" href="#Literature" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[ACOO20]</dt><dd><div>Y.¬†T.¬†Almeida, J.¬†X.¬†Cruz Neto, P.¬†R.¬†Oliveira and J.¬†C.¬†Oliveira Souza. <em>A modified proximal point method for DC functions on Hadamard manifolds</em>. <a href="https://doi.org/10.1007/s10589-020-00173-3">Computational¬†Optimization¬†and¬†Applications <strong>76</strong>, 649‚Äì673</a> (2020).</div></dd><dt>[BFSS23]</dt><dd><div>R.¬†Bergmann, O.¬†P.¬†Ferreira, E.¬†M.¬†Santos and J.¬†C.¬†Souza. <a href="https://arxiv.org/abs/2112.05250"><em>The difference of convex algorithm on Hadamard manifolds</em></a>, arXiv¬†preprint (2023).</div></dd><dt>[SO15]</dt><dd><div>J.¬†C.¬†Souza and P.¬†R.¬†Oliveira. <em>A proximal point algorithm for DC fuctions on Hadamard manifolds</em>. <a href="https://doi.org/10.1007/s10898-015-0282-7">Journal¬†of¬†Global¬†Optimization <strong>63</strong>, 797‚Äì810</a> (2015).</div></dd></dl></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../cyclic_proximal_point/">¬´ Cyclic Proximal Point</a><a class="docs-footer-nextpage" href="../DouglasRachford/">Douglas‚ÄîRachford ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Friday 8 March 2024 18:06">Friday 8 March 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
