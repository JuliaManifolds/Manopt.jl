<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>do stochastic gradient descent · Manopt.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../index.html"><img src="../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">Manopt.jl</span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Home</a></li><li><a class="tocitem" href="../about.html">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="MeanAndMedian.html">get Started: Optimize!</a></li><li class="is-active"><a class="tocitem" href="StochasticGradientDescent.html">do stochastic gradient descent</a></li><li><a class="tocitem" href="BezierCurves.html">work with Bézier curves</a></li><li><a class="tocitem" href="GradientOfSecondOrderDifference.html">see the gradient of <span>$d_2$</span></a></li><li><a class="tocitem" href="JacobiFields.html">use Jacobi Fields</a></li></ul></li><li><a class="tocitem" href="../plans/index.html">Plans</a></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../solvers/index.html">Introduction</a></li><li><a class="tocitem" href="../solvers/ChambollePock.html">Chambolle-Pock</a></li><li><a class="tocitem" href="../solvers/conjugate_gradient_descent.html">Conjugate gradient descent</a></li><li><a class="tocitem" href="../solvers/cyclic_proximal_point.html">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../solvers/DouglasRachford.html">Douglas–Rachford</a></li><li><a class="tocitem" href="../solvers/gradient_descent.html">Gradient Descent</a></li><li><a class="tocitem" href="../solvers/NelderMead.html">Nelder–Mead</a></li><li><a class="tocitem" href="../solvers/particle_swarm.html">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../solvers/stochastic_gradient_descent.html">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../solvers/subgradient.html">Subgradient method</a></li><li><a class="tocitem" href="../solvers/truncated_conjugate_gradient_descent.html">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../solvers/trust_regions.html">Riemannian Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../functions/index.html">Introduction</a></li><li><a class="tocitem" href="../functions/bezier.html">Bézier curves</a></li><li><a class="tocitem" href="../functions/costs.html">Cost functions</a></li><li><a class="tocitem" href="../functions/differentials.html">Differentials</a></li><li><a class="tocitem" href="../functions/adjointdifferentials.html">Adjoint Differentials</a></li><li><a class="tocitem" href="../functions/gradients.html">Gradients</a></li><li><a class="tocitem" href="../functions/Jacobi_fields.html">Jacobi Fields</a></li><li><a class="tocitem" href="../functions/proximal_maps.html">Proximal Maps</a></li><li><a class="tocitem" href="../functions/manifold.html">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../helpers/data.html">Data</a></li><li><a class="tocitem" href="../helpers/errorMeasures.html">Error Measures</a></li><li><a class="tocitem" href="../helpers/exports.html">Exports</a></li></ul></li><li><a class="tocitem" href="../list.html">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">How to...</a></li><li class="is-active"><a href="StochasticGradientDescent.html">do stochastic gradient descent</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="StochasticGradientDescent.html">do stochastic gradient descent</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/src/tutorials/StochasticGradientDescent.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="SGDTutorial-1"><a class="docs-heading-anchor" href="#SGDTutorial-1">Stochastic Gradient Descent</a><a class="docs-heading-anchor-permalink" href="#SGDTutorial-1" title="Permalink"></a></h1><p>This tutorial illustrates how to use the <a href="../solvers/stochastic_gradient_descent.html#Manopt.stochastic_gradient_descent"><code>stochastic_gradient_descent</code></a> solver and different <a href="../solvers/gradient_descent.html#Manopt.DirectionUpdateRule"><code>DirectionUpdateRule</code></a>s in order to introduce the average or momentum variant, see <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent</a>.</p><p>Computationally we look at a very simple but large scale problem, the Riemannian Center of Mass or <a href="https://en.wikipedia.org/wiki/Fréchet_mean">Fréchet mean</a>: For given points <span>$p_i \in \mathcal M$</span>, <span>$i=1,\ldots,N$</span> this optimization problem reads</p><div>\[\operatorname*{arg\,min}_{x\in\mathcal M} \frac{1}{2}\sum_{i=1}^{N}
  \operatorname{d}^2_{\mathcal M}(x,p_i),\]</div><p>which of course can be (and is) solved by a gradient descent, see the <a href="MeanAndMedian.html#Optimize-1">introductionary tutorial</a>. If <span>$N$</span> is very large it might be quite expensive to evaluate the complete gradient. A remedy is, to evaluate only one of the terms at a time and choose a random order for these.</p><p>We first initialize the manifold (see [])</p><pre><code class="language-julia">using Manopt, Manifolds, Random, Colors</code></pre><p>and we define some colors from <a href="https://personal.sron.nl/~pault/">Paul Tol</a></p><pre><code class="language-julia">black = RGBA{Float64}(colorant&quot;#000000&quot;)
TolVibrantOrange = RGBA{Float64}(colorant&quot;#EE7733&quot;) # Start
TolVibrantBlue = RGBA{Float64}(colorant&quot;#0077BB&quot;) # a path
TolVibrantTeal = RGBA{Float64}(colorant&quot;#009988&quot;) # points</code></pre><p>And optain a large data set</p><pre><code class="language-julia">n = 5000
σ = π / 12
M = Sphere(2)
x = 1 / sqrt(2) * [1.0, 0.0, 1.0]
Random.seed!(42)
data = [exp(M, x, random_tangent(M, x, Val(:Gaussian), σ)) for i in 1:n]</code></pre><p>which looks like</p><pre><code class="language-julia">asymptote_export_S2_signals(&quot;centerAndLargeData.asy&quot;;
    points = [ [x], data],
    colors=Dict(:points =&gt; [TolVibrantBlue, TolVibrantTeal]),
    dot_sizes = [2.5, 1.0], camera_position = (1.,.5,.5)
)
render_asymptote(&quot;centerAndLargeData.asy&quot;; render = 2)</code></pre><p><img src="../assets/images/tutorials/centerAndLargeData.png" alt="The data of noisy versions of \$x\$"/></p><p>Note that due to the construction of the points as zero mean tangent vectors, the mean should be very close to our initial point <code>x</code>.</p><p>In order to use the stochastic gradient, we now need a function that returns the vector of gradients. There are two ways to define it in <code>Manopt.jl</code>: as one function, that returns a vector or a vector of funtions.</p><p>The first variant is of course easier to define, but the second is more efficient when only evaluating one of the gradients. For the mean we have as a gradient</p><div>\[ ∇F(x) = \sum_{i=1}^N ∇f_i(x) \quad \text{where} ∇f_i(x) = -\log_x p_i\]</div><p>Which we define as</p><pre><code class="language-julia">F(x) = 1 / (2 * n) * sum(map(p -&gt; distance(M, x, p)^2, data))
∇F(x) = [∇distance(M, p, x) for p in data]
∇f = [x -&gt; ∇distance(M, p, x) for p in data];</code></pre><p>The calls are only slightly different, but notice that accessing the 2nd gradient element requires evaluating all logs in the first function. So while you can use both <code>∇F</code> and <code>∇f</code> in the following call, the second one is faster:</p><pre><code class="language-julia">@time x_opt1 = stochastic_gradient_descent(M, ∇F, x);</code></pre><pre><code class="language-none"> 18.525834 seconds (100.86 M allocations: 10.848 GiB, 15.17% gc time)</code></pre><p>versus</p><pre><code class="language-julia">@time x_opt2 = stochastic_gradient_descent(M, ∇f, x);</code></pre><pre><code class="language-none">  0.130007 seconds (262.47 k allocations: 14.572 MiB)</code></pre><p>This result is reasonably close. But we can improve it by using a <a href="../solvers/gradient_descent.html#Manopt.DirectionUpdateRule"><code>DirectionUpdateRule</code></a>, namely: On the one hand <a href="../solvers/gradient_descent.html#Manopt.MomentumGradient"><code>MomentumGradient</code></a>, which requires both the manifold and the initial value,    in order to keep track of the iterate and parallel transport the last direction to the current iterate.    you can also set a <code>vector_transport_method</code>, if <code>ParallelTransport()</code> is not    available on your manifold. Here we simply do</p><pre><code class="language-julia">@time x_opt3 = stochastic_gradient_descent(
    M, ∇f, x; direction=MomentumGradient(M, x, StochasticGradient())
);</code></pre><pre><code class="language-none">  0.224690 seconds (455.08 k allocations: 30.037 MiB, 6.48% gc time)</code></pre><p>And on the other hand the <a href="../solvers/gradient_descent.html#Manopt.AverageGradient"><code>AverageGradient</code></a> computes an average of the last <code>n</code> gradients, i.e.</p><pre><code class="language-julia">@time x_opt4 = stochastic_gradient_descent(
    M, ∇f, x; direction=AverageGradient(M, x, 10, StochasticGradient())
);</code></pre><pre><code class="language-none">  0.400347 seconds (1.00 M allocations: 64.311 MiB, 4.06% gc time)</code></pre><p>note that the default <a href="../solvers/index.html#Manopt.StoppingCriterion"><code>StoppingCriterion</code></a> is a fixed number of iterations.</p><p>Note that since you can apply both also in the <a href="../solvers/gradient_descent.html#Manopt.Gradient"><code>Gradient</code></a> case of <a href="../solvers/gradient_descent.html#Manopt.gradient_descent"><code>gradient_descent</code></a>, both constructors have to know that internally the default avaluation of the Stochastic gradient (choosing one gradient <span>$∇f_k$</span> at random) has to be specified.</p><p>For this small example you can of course also use a gradient descent with <a href="../plans/index.html#Manopt.ArmijoLinesearch"><code>ArmijoLinesearch</code></a>, but it will be a little slower usually</p><pre><code class="language-julia">@time x_opt5 = gradient_descent(M, F, x -&gt; sum(∇F(x)), x; stepsize=ArmijoLinesearch());</code></pre><pre><code class="language-none">  3.226290 seconds (26.48 M allocations: 1.571 GiB, 13.54% gc time)</code></pre><p>but it is for sure faster than the variant above that evaluates the full gradient on every iteration, since stochastic gradient descent takes more iterations.</p><p>Note that all 5 of couse yield the same result</p><pre><code class="language-julia">[distance(M, x, y) for y in [x_opt1, x_opt2, x_opt3, x_opt4, x_opt5]]</code></pre><pre><code class="language-none">5-element Array{Float64,1}:
 0.0
 0.0
 0.0
 0.0
 0.0</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="MeanAndMedian.html">« get Started: Optimize!</a><a class="docs-footer-nextpage" href="BezierCurves.html">work with Bézier curves »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 3 December 2020 18:35">Thursday 3 December 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
